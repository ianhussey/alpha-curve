
<!DOCTYPE html>
<html id="_htmlTag" lang="en">

<head><meta charset="utf-8" /><title>
	Seeing two sides at once: Effects of viewpoint and object structure on reco...: EBSCOhost
</title>
	
<link rel="icon" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/ehost/favicon.ico" type="image/x-icon" />
<link rel="shortcut icon" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/ehost/favicon.ico" type="image/x-icon" />

	<link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/css/ehost/master_bundle.css" media="All" />
<link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/css/_layout2/rtac.css" media="All" />
<link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/css/common/abody.css" media="All" />
<link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/css/_layout2/selecteddatabasescontrol.css" media="All" />
<link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/css/_layout2/page/detail.css" media="All" />
<link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/css/_layout2/carousel.css" media="All" />
<link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/css/_layout2/bookcarousel.css" media="All" />
<link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/css/_layout2/emailprintdialog.css" media="All" />
<link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/css/_layout2/print.css" media="Print" />
<!--[if lt IE 9]><link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/css/_layout2/ie8.css" media="All" /><![endif]-->
<!--[if lt IE 8]><link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/css/_layout2/ie7.css" media="All" /><![endif]-->
<!--[if lt IE 7]><link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/css/_layout2/ie6.css" media="All" /><![endif]-->
<!--##EPCSS##-->
	
	<script>
var ep = {"version":"16.1.0.155","baseImagePath":"http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/","brandingPath":"http://imageserver.ebscohost.com/branding/","interfaceId":"ehost","cssLayout":2,"messages":{"Close":"Close","Loading":"Loading","show_this_area":"Show this area","hide_this_area":"Hide this area","column1-closed":"Show Left Column","column1-open":"Hide Left Column","column2-closed":"Show Right Column","column2-open":"Hide Right Column","sh_more":"Show More","sh_less":"Show Less","enter_email_address":"Please enter your e-mail address.","email_invalid_error":"Please provide a valid email address.","field_required":"This field is required.","your_subject_may_not_contain_html_markup":"Your subject may not contain HTML markup.","your_comments_may_not_contain_html_markup":"Your comments may not contain HTML markup.","err_sending_email":"Error Sending Email","your_message_may_not_contain_html_markup":"Your message may not contain HTML markup."},"clientData":{"googleTagManagerId":"GTM-NCMJP5","usrNo":0,"currentRecord":{"Db":"pdh","Tag":"AN","Term":"2001-05318-014"},"rtacView":"detail","rtacTimeout":30,"addThis":{"widgetUrl":"http://s7.addthis.com/js/250/addthis_widget.js#username=ebscohost","bookmarkUrl":"http://www.addthis.com/bookmark.php?v=250\u0026username=ebscohost"},"hoverPreviewLabelData":"{\"Abstract\":\"Abstract\",\"Date\":\"Date\",\"Source\":\"Source\",\"Subjects\":\"Subjects\",\"Title\":\"Title\",\"Citation\":\"Detail\",\"FullCitation\":\"Detailed Record\",\"AddToFolder\":\"Add to folder\",\"RemoveFromFolder\":\"Remove from folder\",\"FolderItem\":\"Folder Item\",\"AddExternalRecToFolder\":\"Add citation to Other Contents Folder\",\"RemoveExternalRecFromFolder\":\"Remove citation from Other Content Sources Folder\",\"AddToFolderTitle\":\"Add result to folder\",\"RemoveFromFolderTitle\":\"Remove result from folder\",\"AddRemoveToFolder\":\"Add/Remove \",\"AddRemoveToFolderTitle\":\"Add or remove from folders\",\"PublicationType\":\"Publication Type\",\"Database\":\"Database\",\"Duration\":\"Length (hours:minutes)\"}","plink":"http://search.ebscohost.com/login.aspx?direct=true\u0026db=pdh\u0026AN=2001-05318-014\u0026site=ehost-live"},"templates":{},"pageScripts":["bundled/jqueryplusui.js","bundled/underscore.js","bundled/_layout2/master.js","bundled/ehost/page/detail.js","bundled/buzzloader.js","bundled/buzzsessionsync.js","ep/selectdb.js","ep/widgets/epeditor.js","ckeditor/ckeditor.js","ckeditor/adapters/jquery.js","bundled/notesmodal.js","jquery/plugins/jquery.ba-bbq.js","ep/controller/realtimeavailabilitycontroller.js","ep/jqueryplugins/scrollto.js","ep/controller/concurrentaccesscontroller.js","ep/ep_readspeaker.js","ep/common/menubar.js","ep/googleclassroom/gc-boot.js"],"relativeRequestPath":"detail/detail","sid":"9978e2e2-c03c-4987-8900-75d99d89b0d3@sessionmgr115","vid":"0","existingReturnUrl":"","newReturnUrl":"/ehost/detail/detail?sid=9978e2e2-c03c-4987-8900-75d99d89b0d3@sessionmgr115\u0026vid=0\u0026hid=105\u0026bdata=JnNpdGU9ZWhvc3QtbGl2ZQ==","locale":"en"}
</script>
<script src="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/javascript/bundled/ep_boot.js"></script>
<!--[if lt IE 9]>
<script src="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/javascript/html5shiv/html5.js"></script>
<![endif]-->
<script>

ep.boot(function() {
	ep.updateSearchMode();
	focusOnMainContent();
	
ep.util.url.updateHash("db=pdh&AN=2001-05318-014");

	ep.getInstance( { epId: 'ep.controller.page.CitationController' });
	ep.getScreenResolution();

},
null);
</script>
<!--##EPJS##-->
	
	
</head>
<body id="ctl00_ctl00__bodyTag" class="column1-open column2-open limited-scope no-skin detail ehost">
	
	

	<div id="epAjaxActive">Loading...</div>	
	<form method="post" action="detail?sid=9978e2e2-c03c-4987-8900-75d99d89b0d3%40sessionmgr115&amp;vid=0&amp;hid=105&amp;bdata=JnNpdGU9ZWhvc3QtbGl2ZQ%3d%3d" id="aspnetForm">
<input type="hidden" name="AddToFolderClientIDs" id="AddToFolderClientIDs" value="" />
<input type="hidden" name="RelRequestPath" id="RelRequestPath" value="detail/detail" />
<input type="hidden" name="__sid" id="__sid" value="9978e2e2-c03c-4987-8900-75d99d89b0d3@sessionmgr115" />
<input type="hidden" name="__vid" id="__vid" value="0" />
<input type="hidden" name="__CUSTOMVIEWSTATE" id="__CUSTOMVIEWSTATE" value="H4sIAAAAAAAEAI1V227cNhBFFMsbx2hdJIFfCngnKNB6AUvwJU7sNCjgOHEbIBfDMfIacMmRxIYiVZLyWv2qfkM/oT/UDintet2+9EXgRTPnzJkL/76zITbS9WzvyZPjg6fHz54dbmwmWw8/MSUF83iBv7Xo/DsjMLmzmRQbYvgkd8J/K4rpMk1Q08HdcJByxZxLX3Cj2lrvZaZBDf1mv98oWUuPInOctqBN5r5IDQI9kwqwMs6Tr5QgvpnjBL+jT9LJqcJKiGQUzwPc+G41MKGVSO5tDCuRrA22YnOluDlduVnej6uyj2a1h1oNSOufH7/xWJ+aVvvk7uZqdJIM3xE5uL9179S50xjntxa5sSLzHcVSGFszn3HpmZdGb62fmrpmWrxnNabHr8/zt1gy3uWnRntrlELr8lcx7vwsmrr8dLDt9y9b78nPxuDnxJZtjdqLrZXPjz++TAJ/iiRZpGOcpA8GEiRxDYx7eYWCpFqI0UcwTtMHy8wbUWSFH6+mL/4Xy3NRnLVKXeK1XyY6HonxvTmrrwhnZWsl/JOOtvfzw3cvJ/3NUhLS+ZLoJbReHa+lj86kFvCRqkQxCxfoWuXd1ujSGHUpm/TRBXor8QqBKQW2vx6P0r/WPiJKXYKfGXBSoAPmwWiOz+F1USD3DkwBVxJnjZHaAwkKZvorXYDztuW+tUj/Q9Cl1PL36KuyiJmQpLqjrDA1WLgczkhZwOsGbbgl5wJro8kTNY0gQwJnInADzjQQH+1l0QFHS1JqqvsrVJFNpHVwC6SmbltABU6+QpgyJ+O/X7SZKRQlhg2jWHWpKIWUpwZjwnP4GU0dZOKkUQfSWizboCannJrWwvZMWpxECVxrC8YRtqlxu8kCNFyVt7zMfSwMGtngjcEMST7GeRsEiL/3MpIYs4ra3hofldk72v3zD2BT0/oY1hUJEhBgm9DZtSRpfzEzyq/dmfsIDTG45h3MmIPGGBvaLdZAqOAozILKMh6z1MgiQnVZ8A/TDo6JQw5v5RecSYc7JDiRd+3Uk5DRkBAdxLHU+Aq2ZY75DhxE5uTsKS0mdC1aTggCuUXmMBosUyZKU0Pmc9WX5V7U0WWFDud1TNdlSeN2qJ9BzX87ncc5D3HBkyLr5a1ZB9OQjip0SoieVCgraF0sGqn7KRH8kfHg7wfYv1WF0yBc6IKBv9uJKrqKDXrPKskrIleHgpb6ilnJqLGosik5SjY3CuZA04TK11MXwEwSVV/JcO3RNhZ7wXeomAsZkJ1vRUdAVAdDK1EUGpySZeWXEsRmFGdhTT2EzVkIkBxy2UiiTE1pTfOfvMxLKYftc9fxN+/PPsAr5hl1GNLACWOR2mEC+7t7B3ByfrLTj5oA7kKm0JKmEwopOqQxcDKVSvoOPvTj5GIJi56tliI6k6jEfHOOlpMq4fokjIjKGALcO4LOOvie1c2PYJRAO6EJud4/U2GeLh7DfnZ+fftg8URFi7X0uxePsywk3D4Hd3i0u3t4cJyHTOXxjQXIsp/EP50wtpD9BwAA" />
<input type="hidden" name="__ScreenResolution" id="__ScreenResolution" value="" />
<input type="hidden" name="__VIEWSTATE" id="__VIEWSTATE" value="" />

		<!--[if lt IE 7]>	
		

<div class="ie6_req_block">
	<div class="ie6_req_text">
		IE6 users please note our browser requirements are changing! See  <a href="http://support.epnet.com/knowledge_base/detail.php?id=25" target="_blank" title="EBSCO's Support Site">EBSCO's Support Site</a>  for more information.
	</div>
</div>
		<![endif]-->
		
		<div id="outerContainer">
			<div id="innerContainer">
				
					
				
				
	
	
		

				
	
	

				
				<div id="header" class="clearfix" role="banner" >
					<div id="pageInstruction" tabindex="-1" class="hidden">citation_instruction</div><p tabindex="0" class="hidden"><a href="javascript:openWideTip('http://support.ebsco.com/help/?int=ehost&lang=en&feature_id=access&TOC_ID=Always&SI=0&BU=0&GU=1&PS=0&ver=&dbs=pdh')">Accessibility Information and Tips</a> Revised Date: 07/2015</p>
					<h1 title="Seeing two sides at once: Effects of viewpoint and object structure on recognizing three-dimensional objects" class="hidden">Seeing two sides at once: Effects of viewpoint and object structure on recognizing three-dimensional objects</h1>
					
					
	<div class="customerLogo"><a href="javascript:__doPostBack('ctl00$ctl00$FindField$customerLogo');"><img src="http://www.tilburguniversity.edu/static/uvtpresentation/images/framework/logo.jpg" alt="Library Logo" /></a></div>
	

					
				</div>
					<div id="mainContentArea" >
						<div id="content" role="main" class="text-normal" >
							
	
	<div class="content-header" >
	 

	</div>
	
	
	

	
	<div id="ToolPanelContent" class="bg-p2" >
		<div class="wrapper clearfix" >
		</div>
		<a  href="#" title="Close Panel" class="close-panel"></a>
	</div>

	<!-- If citation is being displayed it will be rendered inside this placeholder.
		 If citation is not being displayed, full text will be rendered in this placeholder. -->
	<div class="ft-translation hidden"><label for="transLanguage">Translate Full Text:</label></div><div class="ft-translation"><a name="Translate"> </a><select id="transLanguage" name="transLanguage" title="Choose Language"><option value="" selected="selected">Choose Language</option><option value="Arabic">الإنجليزية/العربية</option><option value="Bulgarian">английски език/български</option><option value="SimplifiedChinese">英语/简体中文</option><option value="TraditionalChinese">英語/繁體中文</option><option value="Czech">angličtina/čeština</option><option value="Danish">Engelsk/dansk</option><option value="Dutch">Engels/Nederlands</option><option value="French">Anglais/Français</option><option value="German">Englisch/Deutsch</option><option value="Greek">Αγγλικά/Ελληνικά</option><option value="Hausa">English/Hausa</option><option value="Hebrew">אנגלית/עברית</option><option value="Hindi">अंग्रेज़ी/हिंदी</option><option value="Hungarian">angol/magyar</option><option value="Indonesian">Inggris/bahasa Indonesia</option><option value="Italian">Inglesi/Italiano</option><option value="Japanese">英語/日本語</option><option value="Korean">영어/한국어</option><option value="Norwegian">Engelsk/Norsk</option><option value="Persian">انگليسی/فارسی</option><option value="Polish">angielski/polski</option><option value="Portuguese">Inglés/Português</option><option value="Pashto">English/Pashto</option><option value="Romanian">Engleză/română</option><option value="Russian">Английский/Русский</option><option value="Spanish">Inglés/Español</option><option value="Serbian">English/Serbian</option><option value="Swedish">Engelska/svenska</option><option value="Thai">อังกฤษ/ไทย</option><option value="Turkish">İngilizce/Türk</option><option value="Ukranian">Англійська/Українська</option><option value="Urdu">انگریزی/اردو</option></select>&nbsp;<input type="button" id="translateBtn" class="translate" value="Translate" title="Translate" /><input type="button" id="translateOriginal" class="translate" value="Back to English" title="Back to English" /></div><div id="translationProgressContainer" style="display: none;"><span>Translation in Progress:</span><div class="translationProgressBar"><div id="translationProgressBar" class="bg-p1"> </div></div></div><div id="translationErrorContainer" class="medium-normal translation-message" style="display: none;"> </div><div id="translationDisclaimerContainer" style="display: none;"><div class="translation-message"><span class="medium-bold"><span class="txt-red" id="translationDisclaimerLine1"> </span></span><span class="medium-normal" id="translationDisclaimerLine2"> </span><span class="medium-bold" id="translationDisclaimerLine3"> </span><div class="medium-normal">Translations powered by Language Weaver Service<br /></div></div></div><script type="text/javascript">
				ep.getInstance("ep.controller.control.translation");
				ep.require( "common/translation.css" );
			</script><dl class="short-citation" data-auto="short_citation" xmlns:viewExtensions="http://www.ebscohost.com/schema/viewExtensions"><dt class="medium-bold" data-auto="short_citation_title_label">Title: </dt><dd class="medium-normal" data-auto="short_citation_title">Seeing two sides at once: Effects of viewpoint and object structure on recognizing three-dimensional objects.<span class="updated-short-citation"> By: Johnson, Scott H., Journal of Experimental Psychology: Human Perception and Performance, 00961523, 20011201,  Vol. 27,  Issue 6</span></dd><dt class="medium-bold" data-auto="short_citation_long_dbname_label">Database: </dt><dd class="medium-normal" data-auto="short_citation_long_dbname">PsycARTICLES</dd></dl><div class="full-text-container border" data-auto="fulltext_container" xmlns:viewExtensions="http://www.ebscohost.com/schema/viewExtensions"><h2 class="hidden" data-auto="fulltext_title_hidden">HTML Full Text</h2><h2 data-auto="local_abody_title" class="ft-title border color-p4 bar4">Seeing Two Sides at Once: Effects of Viewpoint and Object Structure on Recognizing Three-Dimensional Objects</h2><div class="html-ft-toc" data-auto="html_toc"><h3 class="small-bold" id="toc" data-auto="html_toc_title">Contents</h3><ol data-auto="html_toc_list"><li class="link-medium" data-auto="html_toc_list_item"><a data-auto="ep_link" href="#xhp-27-6-1468-ID0ENCAA" id="hd_xhp-27-6-1468-ID0ENCAA" title="The Problem of Shape-Based Object Recognition">The Problem of Shape-Based Object Recognition</a></li><li class="link-medium" data-auto="html_toc_list_item"><a data-auto="ep_link" href="#xhp-27-6-1468-ID0EMCAA" id="hd_xhp-27-6-1468-ID0EMCAA" title="Theories of Shape-Based Recognition">Theories of Shape-Based Recognition</a></li><li class="link-medium" data-auto="html_toc_list_item"><a data-auto="ep_link" href="#xhp-27-6-1468-ID0ELCAA" id="hd_xhp-27-6-1468-ID0ELCAA" title="Recognizing Novel Views of 3-D Objects">Recognizing Novel Views of 3-D Objects</a></li><li class="link-medium" data-auto="html_toc_list_item"><a data-auto="ep_link" href="#xhp-27-6-1468-ID0EKCAA" id="hd_xhp-27-6-1468-ID0EKCAA" title="General Method">General Method</a></li><li data-auto="html_toc_list_item" class="link-medium html-toc-hd1"><a data-auto="ep_link" href="#xhp-27-6-1468-ID0ECKCAA" id="hd1_xhp-27-6-1468-ID0ECKCAA" title="Apparatus">Apparatus</a></li><li data-auto="html_toc_list_item" class="link-medium html-toc-hd1"><a data-auto="ep_link" href="#xhp-27-6-1468-ID0EBKCAA" id="hd1_xhp-27-6-1468-ID0EBKCAA" title="Stimuli">Stimuli</a></li><li data-auto="html_toc_list_item" class="link-medium html-toc-hd1"><a data-auto="ep_link" href="#xhp-27-6-1468-ID0EAKCAA" id="hd1_xhp-27-6-1468-ID0EAKCAA" title="Procedure">Procedure</a></li><li class="link-medium" data-auto="html_toc_list_item"><a data-auto="ep_link" href="#xhp-27-6-1468-ID0EJCAA" id="hd_xhp-27-6-1468-ID0EJCAA" title="Experiment 1: Geometrically Irregular Contour (Wire) Objects">Experiment 1: Geometrically Irregular Contour (Wire) Objects</a></li><li data-auto="html_toc_list_item" class="link-medium html-toc-hd1"><a data-auto="ep_link" href="#xhp-27-6-1468-ID0EBJCAA" id="hd1_xhp-27-6-1468-ID0EBJCAA" title="Method">Method</a></li><li data-auto="html_toc_list_item" class="link-medium html-toc-hd1"><a data-auto="ep_link" href="#xhp-27-6-1468-ID0EAJCAA" id="hd1_xhp-27-6-1468-ID0EAJCAA" title="Results and Discussion">Results and Discussion</a></li><li class="link-medium" data-auto="html_toc_list_item"><a data-auto="ep_link" href="#xhp-27-6-1468-ID0EICAA" id="hd_xhp-27-6-1468-ID0EICAA" title="Experiment 2: Geometrically Irregular Surface (Clay) Objects">Experiment 2: Geometrically Irregular Surface (Clay) Objects</a></li><li data-auto="html_toc_list_item" class="link-medium html-toc-hd1"><a data-auto="ep_link" href="#xhp-27-6-1468-ID0EBICAA" id="hd1_xhp-27-6-1468-ID0EBICAA" title="Method">Method</a></li><li data-auto="html_toc_list_item" class="link-medium html-toc-hd1"><a data-auto="ep_link" href="#xhp-27-6-1468-ID0EAICAA" id="hd1_xhp-27-6-1468-ID0EAICAA" title="Results and Discussion">Results and Discussion</a></li><li class="link-medium" data-auto="html_toc_list_item"><a data-auto="ep_link" href="#xhp-27-6-1468-ID0EHCAA" id="hd_xhp-27-6-1468-ID0EHCAA" title="Experiment 3: Geometrically Regular Surface Objects (Pipes)">Experiment 3: Geometrically Regular Surface Objects (Pipes)</a></li><li data-auto="html_toc_list_item" class="link-medium html-toc-hd1"><a data-auto="ep_link" href="#xhp-27-6-1468-ID0EBHCAA" id="hd1_xhp-27-6-1468-ID0EBHCAA" title="Method">Method</a></li><li data-auto="html_toc_list_item" class="link-medium html-toc-hd1"><a data-auto="ep_link" href="#xhp-27-6-1468-ID0EAHCAA" id="hd1_xhp-27-6-1468-ID0EAHCAA" title="Results and Discussion">Results and Discussion</a></li><li class="link-medium" data-auto="html_toc_list_item"><a data-auto="ep_link" href="#xhp-27-6-1468-ID0EGCAA" id="hd_xhp-27-6-1468-ID0EGCAA" title="Summary of Experiments 1–3">Summary of Experiments 1–3</a></li><li class="link-medium" data-auto="html_toc_list_item"><a data-auto="ep_link" href="#xhp-27-6-1468-ID0EFCAA" id="hd_xhp-27-6-1468-ID0EFCAA" title="Viewpoint-Dependent Recognition of 90° Views">Viewpoint-Dependent Recognition of 90° Views</a></li><li class="link-medium" data-auto="html_toc_list_item"><a data-auto="ep_link" href="#xhp-27-6-1468-ID0EECAA" id="hd_xhp-27-6-1468-ID0EECAA" title="Experiments 4A and 4B: Effects of Subtle Depth Rotations on Recognition of Contour and Surface Objects">Experiments 4A and 4B: Effects of Subtle Depth Rotations on Recognition of Contour and Surface Objects</a></li><li data-auto="html_toc_list_item" class="link-medium html-toc-hd1"><a data-auto="ep_link" href="#xhp-27-6-1468-ID0ECECAA" id="hd1_xhp-27-6-1468-ID0ECECAA" title="Method">Method</a></li><li data-auto="html_toc_list_item" class="link-medium html-toc-hd1"><a data-auto="ep_link" href="#xhp-27-6-1468-ID0EBECAA" id="hd1_xhp-27-6-1468-ID0EBECAA" title="Results of Experiment 4A: Contour Objects">Results of Experiment 4A: Contour Objects</a></li><li data-auto="html_toc_list_item" class="link-medium html-toc-hd1"><a data-auto="ep_link" href="#xhp-27-6-1468-ID0EAECAA" id="hd1_xhp-27-6-1468-ID0EAECAA" title="Results of Experiment 4B: Surface Objects">Results of Experiment 4B: Surface Objects</a></li><li class="link-medium" data-auto="html_toc_list_item"><a data-auto="ep_link" href="#xhp-27-6-1468-ID0EDCAA" id="hd_xhp-27-6-1468-ID0EDCAA" title="Viewpoint-Invariant Recognition of 180° Views">Viewpoint-Invariant Recognition of 180° Views</a></li><li class="link-medium" data-auto="html_toc_list_item"><a data-auto="ep_link" href="#xhp-27-6-1468-ID0ECCAA" id="hd_xhp-27-6-1468-ID0ECCAA" title="Experiment 5: Effects of Manipulating 2-D Bounding Contour Shape">Experiment 5: Effects of Manipulating 2-D Bounding Contour Shape</a></li><li data-auto="html_toc_list_item" class="link-medium html-toc-hd1"><a data-auto="ep_link" href="#xhp-27-6-1468-ID0EBCCAA" id="hd1_xhp-27-6-1468-ID0EBCCAA" title="Method">Method</a></li><li data-auto="html_toc_list_item" class="link-medium html-toc-hd1"><a data-auto="ep_link" href="#xhp-27-6-1468-ID0EACCAA" id="hd1_xhp-27-6-1468-ID0EACCAA" title="Results">Results</a></li><li class="link-medium" data-auto="html_toc_list_item"><a data-auto="ep_link" href="#xhp-27-6-1468-ID0EBCAA" id="hd_xhp-27-6-1468-ID0EBCAA" title="What's in a Silhouette?">What's in a Silhouette?</a></li><li class="link-medium" data-auto="html_toc_list_item"><a data-auto="ep_link" href="#xhp-27-6-1468-ID0EACAA" id="hd_xhp-27-6-1468-ID0EACAA" title="Conclusions">Conclusions</a></li><li class="link-medium" data-auto="html_toc_list_item"><a data-auto="ep_link" href="#xhp-27-6-1468-ID0EAA" id="hd_xhp-27-6-1468-ID0EAA" title="Footnotes">Footnotes</a></li><li class="link-medium" data-auto="html_toc_list_item"><a data-auto="ep_link" href="#xhp-27-6-1468-ID0E03B0ABAA" id="hd_xhp-27-6-1468-ID0E03B0ABAA" title="References">References</a></li></ol></div><section id="TextToSpeech" class="full-text-content textToSpeechDataContainer" data-auto="text_to_speech" data-text-to-speech-cache-key="pdh_2001-05318-014" data-text-to-speech-title="Seeing two sides at once: Effects of viewpoint and object structure on recognizing three-dimensional objects." data-text-to-speech-author="Johnson, Scott H." data-text-to-speech-additional-filename="20011201"><span id="textToSpeechPlaceholder"> </span><div class="center" xmlns:Translation="urn:EBSCO-Translation"><strong>By: Scott H. Johnson</strong><br /><em>Center for Cognitive Neuroscience, Dartmouth College</em>;<br /><em>Department of Psychology, University of Massachusetts at Amherst</em>;</div><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><strong>Acknowledgement: </strong>Portions of work reported in this article were completed in partial fulfillment of the PhD requirements at Cornell University. This work was supported by National Institute of Mental Health National Research Service Award 1F31MH10251–01 and a grant from the James S. McDonnell Foundation.<br /><br />This research benefited from the insightful comments of James E. Cutting, Barbara Finlay, Steve Palmer, Irv Rock, Maggie Shiffrar, Mary Peterson, Peter Gehardstein, and anonymous reviewers. Many thanks to Sam Tuttle and Tedra Fazendeiro for assistance with testing.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Imagine that you are planning to visit a museum on your first venture to a foreign city. In preparation for that visit, you peruse a brochure that contains single photographs of several sculptures presently on display. When you arrive at the exhibition, will you be able to identify some or all of the pieces presented in the brochure? Most of us would probably answer affirmatively despite the extremely low probability that perspectives depicted in the brochure and views initially observed in the museum would be identical, or even very similar. For example, consider that the qualitatively unique projections potentially available in the well-known sculpture <em>Astronomy</em> by Giovanni Bologna have been estimated to number in the many thousands (<a href="#c29">Koenderink &amp; van Doorn, 1979</a>). Given these slim odds, it seems clear that humans must be able to recognize objects from views that have never before been experienced. How is this possible?</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Considerable attention has been devoted to this question in the psychological (e.g., <a href="#c2">Biederman, 1987</a>; <a href="#c21">Johnson, 1991</a>, <a href="#c22">1993</a>; <a href="#c23">Johnson &amp; Fazendeiro, 1997</a>; <a href="#c49">Tarr, 1995</a>), computational (e.g., <a href="#c13">Grimson, 1991</a>; <a href="#c32">Longuet-Higgins, 1990</a>; <a href="#c33">Lowe, 1985</a>, <a href="#c34">1987</a>; <a href="#c36">Marr, 1982</a>), and neuroscience literatures (e.g., <a href="#c38">Perrett et al., 1985</a>; <a href="#c60">Weiskrantz &amp; Saunders, 1984</a>). This interest attests to the paradoxical nature of shape-based recognition. From virtually any viewpoint, objects are recognized immediately and effortlessly on the basis of their projected shapes; in other words, real-world object recognition is generally <em>viewpoint-independent</em>. However, research has shown that under controlled conditions, recognition can be surprisingly <em>viewpoint-dependent</em> (e.g., <a href="#c45">Rock &amp; Di Vita, 1987</a>; <a href="#c46">Rock, Di Vita, &amp; Barbeito, 1981</a>; <a href="#c47">Rock, Wheeler, &amp; Tudor, 1989</a>; <a href="#c49">Tarr, 1995</a>; <a href="#c51">Tarr &amp; Pinker, 1989</a>, <a href="#c52">1990</a>). This paradox has led to considerable debate concerning the nature of object representations (cf. <a href="#c5">Biederman &amp; Gerhardstein, 1995</a>; <a href="#c15">Hayward &amp; Tarr, 1997</a>; <a href="#c50">Tarr &amp; Bülthoff, 1995</a>). This debate reflects differences in the ways that researchers and theoreticians have grappled with the problem of recognizing objects on the basis of their shapes.</p><a id="xhp-27-6-1468-ID0ENCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><span class="medium-bold" xmlns:Translation="urn:EBSCO-Translation"><a data-auto="ep_link" href="#toc" id="hd_toc_11" title="The Problem of Shape-Based Object Recognition">The Problem of Shape-Based Object Recognition</a></span><br xmlns:Translation="urn:EBSCO-Translation" /><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">From a computational stance, the ease and flexibility that characterizes everyday recognition behavior is truly remarkable and belies the fact that recognizing objects on the basis of their shapes is actually an ill-posed problem. Formally, the problem of shape-based recognition can be thought of as solving a function that maps a variety of projected visual images to an object representation stored in memory (<a href="#c56">Ullman, 1989</a>, <a href="#c57">1990</a>). As illustrated in <a href="#fig1">Figure 1</a>, there is a mapping (M) that relates a representation of an object (Oi ), to one of a potentially large set of proximal images, or views (Vi1, Vi2, …Vi<em>k</em> ). Successful recognition involves inverting this mapping to index the correct object model from corresponding information available in projected images (<a href="#c36">Marr, 1982</a>; <a href="#c56">Ullman, 1989</a>). This mapping is complicated by the fact that as an observer traverses a 3-D orbit through space, each object can give rise to an indefinite number of projected views, bounded only by the constraints of optics. Consequently, recognizing an object from all possible viewpoints amounts to solving a complicated many-to-one mapping between a potentially infinite set of unique projected images and an object representation, or model, stored in memory (<a href="#c56">Ullman, 1989</a>, <a href="#c57">1990</a>).<br /><br /><a id="fig1"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/77bb5eeb65d74820310d2c44dbe53784/571c6483/pdh/xhp/xhp-27-6-1468-fig1a.gif" alt="xhp-27-6-1468-fig1a.gif" title="Figure 1. The mapping (M) relationship between an object representation (Oi) of a lollipop person to a small sampling of its potentially projected views (Vi1, Vi2, …Vik) as rotated around the vertical (y) axis. Successful recognition involves inverting M to recover the correct Oi on the basis of the projected views (Vi1, Vi2, …Vik )" /><em>Figure 1. The mapping (M) relationship between an object representation (Oi) of a lollipop person to a small sampling of its potentially projected views (Vi1, Vi2, …Vik) as rotated around the vertical (y) axis. Successful recognition involves inverting M to recover the correct Oi on the basis of the projected views (Vi1, Vi2, …Vik )</em></span></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Clearly, this problem cannot be solved by brute, computational force alone; any attempt to exhaustively search for corresponding features between each object model and projected image is combinatorially explosive and therefore computationally intractable (e.g., <a href="#c13">Grimson, 1991</a>). To achieve its level of efficiency, the visual recognition system must impose powerful constraints on the space searched for correspondences between projected images and stored object models. Theories of shape-based recognition suggest a number of possibilities.</p><a id="xhp-27-6-1468-ID0EMCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><span class="medium-bold" xmlns:Translation="urn:EBSCO-Translation"><a data-auto="ep_link" href="#toc" id="hd_toc_16" title="Theories of Shape-Based Recognition">Theories of Shape-Based Recognition</a></span><br xmlns:Translation="urn:EBSCO-Translation" /><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">In contemporary theories of recognition, constraints on the relationship between projected images and object models come in the form of decisions made about design characteristics of shape representations (<a href="#c37">Marr &amp; Nishihara, 1978</a>; <a href="#c41">Plaut &amp; Farah, 1990</a>). Of particular importance to recognizing novel views is the coordinate system, or reference frame, relative to which the size, location, and orientation of an object's features are defined. A 3-D reference frame has a full seven degrees of freedom: Three are required to specify position along orthogonal axes (<em>x</em>, <em>y</em>, and <em>z</em>), another three are required to specify orientation relative to these axes, and a final degree is needed to specify scaling (e.g., <a href="#c17">Hinton &amp; Parsons, 1988</a>).</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">The reference frame chosen for a system of shape representations influences two components of the shape recognition task: (a) deriving a description of an object from its projected image, and (b) matching this description against a catalog of stored object representations.<a id="b-fn1"> </a><sup><a href="#fn1" /></sup>
These components are only relatively independent: Reducing demands on processes involved in deriving a description of an object will increase the load on processes needed to match that description with a stored representation. Conversely, increasing demands on matching processes will lessen the load on processes involved in deriving descriptions from projected images.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Theories of shape-based recognition can be grouped into two broad categories, depending on whether the object representations they posit are specified relative to a frame of reference that is intrinsic to the object (i.e., object centered) or centered on the viewpoint of the observer (i.e., viewer centered). As represented schematically in B<a href="#fig2">Figure 2B</a>, viewpoint-invariant (VI) theories generally use object-centered representations that provide explicit information about objects' entire 3-D structures.<a id="b-fn2"> </a><sup><a href="#fn2" /></sup>
Object-centered descriptions are difficult to derive from an image, but once they are successfully computed, they can be easily matched against stored object-centered representations regardless of the observer's viewpoint. In some cases, however, it may also be possible to achieve VI recognition by matching images and representations on the basis of diagnostic features that remain invariant over changes in orientation (e.g., <a href="#c8">Corballis, 1988</a>; <a href="#c25">Jolicœur, 1990</a>).<br /><br /><a id="fig2"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/a8e2d41d4f32c595628742f50f293720/571c6483/pdh/xhp/xhp-27-6-1468-fig2a.gif" alt="xhp-27-6-1468-fig2a.gif" title="Figure 2. Object- and viewer-centered reference frames, showing the effects of rotating a stick figure in the frontal plane on object-centered and viewer-centered shape representations. A: Upright view with head at bottom and feet at top; B: Object-centered description, with head at top and feet at bottom; C: Viewer-centered description, with head at bottom and feet at top. Note that the object-centered representation (B) is the same regardless of the orientation of the figure, because shape is specified relative to a reference frame that is intrinsic to the object itself. In contrast, notice that the viewer-centered representation (C) changes as the figure is rotated because shape is specified relative to a reference frame centered on a particular viewpoint" /><em>Figure 2. Object- and viewer-centered reference frames, showing the effects of rotating a stick figure in the frontal plane on object-centered and viewer-centered shape representations. A: Upright view with head at bottom and feet at top; B: Object-centered description, with head at top and feet at bottom; C: Viewer-centered description, with head at bottom and feet at top. Note that the object-centered representation (B) is the same regardless of the orientation of the figure, because shape is specified relative to a reference frame that is intrinsic to the object itself. In contrast, notice that the viewer-centered representation (C) changes as the figure is rotated because shape is specified relative to a reference frame centered on a particular viewpoint</em></span></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">As depicted in C<a href="#fig2">Figure 2C</a>, viewpoint-dependent (VD) theories utilize viewer-centered representations that provide only explicit information about portions of objects visible from a single point of view. Viewer-centered descriptions are relatively easy to derive from an image—which itself is inherently viewpoint dependent—but can be difficult to match against stored viewer-centered representations that may or may not depict the same view of the object. A number of strategies have been undertaken to manage this issue, including using nonlinear methods to predict the appearance of novel views from a set of VD models (e.g., <a href="#c6">Bülthoff &amp; Edelman, 1992</a>; <a href="#c9">Edelman &amp; Bülthoff, 1992</a>; <a href="#c10">Edelman &amp; Weinshall, 1991</a>; <a href="#c42">Poggio &amp; Edelman, 1990</a>; <a href="#c59">Vetter, Poggio, &amp; Bülthoff, 1994</a>), or using transformations to align models and images (e.g., <a href="#c49">Tarr, 1995</a>; <a href="#c56">Ullman, 1989</a>). As reviewed by <a href="#c49">Tarr (1995)</a>, these theories also have certain limiting conditions. For example, it may only be possible to align a VD model with an image if both depict some of the same visible parts or features (e.g., <a href="#c49">Tarr, 1995</a>; <a href="#c56">Ullman, 1989</a>).</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">It is also important to note that several other possibilities have been raised. <a href="#c41">Plaut and Farah (1990)</a> pointed out that each of the seven individual parameters needed to fully describe a 3-D reference frame can be specified independently, relative to a coordinate system centered on either the object or the viewer. Consequently, representations of shape could have orientation on the <em>z</em>-axis specified relative to a viewer-centered reference frame; orientation on the <em>x</em>- and <em>y</em>-axes might, however, be specified relative to an object-centered frame of reference. Such hybrid representations are not used by contemporary theories, but would lead to VI recognition following certain transformations and VD recognition following others.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">By contrast, <a href="#c8">Corballis (1988)</a> suggested that objects' orientations may initially be represented independent of any reference frame. Corballis claimed that this representation is sufficient to index an object representation in memory, from which information about objects' internal axes can then be retrieved to specify their orientation relative to the observer. At this point, descriptions can be normalized into a canonical orientation and compared in detail with indexed representations. Although enticing in its simplicity, more recent findings suggest that normalization may occur prior to indexing object representations (<a href="#c12">Gibson &amp; Peterson, 1994</a>).</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Finally, a number of authors have argued that the visual system may represent objects in a variety of formats, so as to support optimally the unique demands of different visually guided behaviors. For instance, although VI representations may be ideal for solving the problem of recognition, navigating through environments or reaching for objects require representations that describe shape relative to the viewer's present location (e.g., <a href="#c22">Johnson, 1993</a>; <a href="#c36">Marr, 1982</a>). As initially suggested by Rock and colleagues (<a href="#c46">Rock et al., 1981</a>)—and discussed at length below— even within the domain of object recognition, there may be both VI and VD subsystems. Studies of brain-injured patients by Turnbull and colleagues (<a href="#c54">Turnbull, Beschin, &amp; Della Sala, 1997</a>; <a href="#c54">Turnbull, Carey, &amp; McCarthy, 1997</a>), for example, suggest that VI and VD recognition systems coexist and are mediated by functionally and anatomically dissociable processes. Likewise, results from visual priming studies in healthy adults are consistent with the existence of two distinct representational systems. <a href="#c48">Stankiewicz, Hummel, and Cooper (1998)</a> have suggested that access to representations in one system is dependent on focal attention and does not encode left and right, whereas the other system can be activated automatically and distinguishes handedness. It may also be the case that recognition becomes increasingly VD as the similarity between test items and distractors increases independent of the representational format. <a href="#c53">Tjan and Legge (1998)</a> used an ideal observer model to illustrate how the similarity between stimulus items and accuracy criterion of the task can conspire to influence how much detail object representations must possess to support a given level of accuracy. In general, as view complexity increases, so does viewpoint dependency.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">In short, there are reasons to believe that a variety of different factors may influence whether recognition is affected by changes in objects' orientations relative to an observer. Specific attempts to capitalize on the advantages of representing shape information in VI versus VD formats have been discussed at length elsewhere (<a href="#c4">Biederman &amp; Gerhardstein, 1993</a>, <a href="#c5">1995</a>; <a href="#c15">Hayward &amp; Tarr, 1997</a>; <a href="#c49">Tarr, 1995</a>; <a href="#c50">Tarr &amp; Bülthoff, 1995</a>) and will not be considered in detail here. Instead, this article focuses on two less frequently raised issues: First, how is recognition of novel views of 3-D objects affected by rotations in depth? With few notable exceptions, recognition studies have relied on 2-D representations or pictures of objects rather than 3-D objects themselves. However, pictures differ in important ways from the objects they depict. Most important, unlike pictures, 3-D objects enable recovery of binocular depth cues, which may be important for computing VI representations (<a href="#c11">Farah, Rochlin, &amp; Klein, 1994</a>; <a href="#c36">Marr, 1982</a>). Second, are all objects represented relative to the same reference frame, or does the visual system use different formats to describe structurally different classes of objects? Computational science tells us that no one representational format is ideally suited for describing all types of information or solving all varieties of tasks (<a href="#c36">Marr, 1982</a>; <a href="#c41">Plaut &amp; Farah, 1990</a>). It seems reasonable to inquire as to whether the same is also true of computations performed by biological visual systems.</p><a id="xhp-27-6-1468-ID0ELCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><span class="medium-bold" xmlns:Translation="urn:EBSCO-Translation"><a data-auto="ep_link" href="#toc" id="hd_toc_27" title="Recognizing Novel Views of 3-D Objects">Recognizing Novel Views of 3-D Objects</a></span><br xmlns:Translation="urn:EBSCO-Translation" /><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">The strongest test of the ability to recognize novel views is to examine whether subjects can identify other perspectives of an object following a brief exposure to a single view. VI and VD classes of theories make two straightforward predictions for performance of this task. Provided that certain minimal conditions are met, VI theories predict that subjects should be able to accurately recognize virtually all novel perspectives of an object on the basis of knowledge of a single familiar view. According to <a href="#c4">Biederman and Gerhardstein (1993)</a>, for example, three conditions must be satisfied for object recognition to be invariant over rotations in depth. First, stimuli must be capable of activating VI structural descriptions. More precisely, objects must be able to be decomposed into VI 3-D primitives called geometric ions (geons), and their relations. Second, these geon structural descriptions (GSDs) must be unique for each object in the stimulus set. That is, to be distinguishable, objects must be composed of either different parts or sufficiently different arrangements of the same parts (for details, see <a href="#c4">Biederman &amp; Gerhardstein, 1993</a>). Third, the same GSD must be activated by both the studied and tested views (see <a href="#c2">Biederman, 1987</a>; <a href="#c19">Hummel &amp; Biederman, 1992</a>). Because changes in viewpoint can induce accretion (revelation) and deletion (occlusion) of parts, it is possible that different GSDs will be derived from two views of the same object. Likewise, a different GSD can be activated when “small changes in orientation lead to large changes in part structure, despite the presence of the same contours in the different views” (<a href="#c4">Biederman &amp; Gerhardstein, 1993</a>, p. 1165). In short, unless all three conditions are met, recognition will depend on familiarity with one's viewpoint. Although the validity of these specific criteria have been questioned (<a href="#c15">Hayward &amp; Tarr, 1997</a>), they are used here to operationalize the minimal conditions under which VI recognition might be expected.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">In contrast, if representations of shape are always VD, then familiar views should always be recognized more accurately than novel views (e.g., <a href="#c49">Tarr, 1995</a>; <a href="#c50">Tarr &amp; Bülthoff, 1995</a>). Although view interpolation theories are not applicable to this situation because only a single view has been familiarized, VD alignment models predict that recognition of novel views should be less efficient than recognition of familiar views.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Given its importance for distinguishing between VD and VI representational formats, surprisingly little attention has been devoted to this simple task. One influential exception is work by <a href="#c46">Rock et al. (1981)</a> that examined whether subjects could recognize novel views of 3-D wire forms on the basis of a brief exposure to a single view. Rock et al. found that certain novel views of these wire objects were accurately recognized following familiarization with a single view, whereas others were not. Changes in orientation of 90° or 180° about either the <em>x</em>- (horizontal) or <em>z</em>- (line of sight) axes induced substantial decreases in recognition accuracy. Rock et al. interpreted these findings as consistent with the well-documented fact that subjects have difficulty perceiving certain objects (e.g., faces or script) that have undergone rotations that disrupt the locations of their tops and bottoms (e.g., <a href="#c44">Rock, 1973</a>). In other words, representations of shape often seem to specify position on the <em>z</em>- and <em>x</em>-axes relative to a viewer-centered frame of reference (e.g., <a href="#fig2">Figure 2C</a>).</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">As shown in <a href="#fig3">Figure 3</a>, <a href="#c46">Rock et al.'s (1981)</a> subjects also had great difficulty recognizing wire objects that had been rotated 90° around the <em>y</em> (vertical) axis. Because this transformation leaves top and bottom locations unchanged, viewpoint dependency was not anticipated. Equally puzzling, and often overlooked, is the fact that rotating the very same objects by 180° about the <em>y</em>-axis had no effect on recognition accuracy (<a href="#fig3">Figure 3</a>). One possibility, considered below, is that subjects were relying on some features that remain visible when wire objects are rotated 180° in depth because of the relative absence of self-occlusion.<br /><br /><a id="fig3"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/b3688e03e04a46a6cf61a714aee65740/571c6483/pdh/xhp/xhp-27-6-1468-fig3a.gif" alt="xhp-27-6-1468-fig3a.gif" title="Figure 3. Results from Rock et al. (1981). To be consistent with the original article, I have expressed performance as the percentage of trials on which subjects correctly identified objects as previously studied items (i.e., responded “yes”) when viewed in the familiarized orientation (0°) or rotated 90° or 180°. Note that recognition is impaired for novel views created by 90° y-axis rotations and is completely unaffected for novel views rotated by 180°" /><em>Figure 3. Results from Rock et al. (1981). To be consistent with the original article, I have expressed performance as the percentage of trials on which subjects correctly identified objects as previously studied items (i.e., responded “yes”) when viewed in the familiarized orientation (0°) or rotated 90° or 180°. Note that recognition is impaired for novel views created by 90° y-axis rotations and is completely unaffected for novel views rotated by 180°</em></span></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><a href="#c46">Rock et al.'s (1981)</a> findings regarding rotations about the <em>y</em>-axis are difficult to account for exclusively in terms of VI or VD models of shape representation. As would be predicted by theories using VI representations, subjects are able to recognize novel views of objects created by 180° <em>y</em>-axis rotations. However, VI models would not predict that subjects would have difficulty recognizing these same objects when rotated around the <em>y</em>-axis by 90°. Conversely, models using VD representations predict that novel views created by either 90° or 180° rotations would be difficult to recognize, because novel views depict qualitatively different views of any reasonably complex objects. Subjects nevertheless easily recognized novel views created by 180° rotations.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><a href="#c46">Rock et al. (1981)</a> interpreted these seemingly conflicting results as evidence for the existence of two distinct modes of shape representation. More precisely, they suggested that shape is represented in both <em>egocentric</em> (i.e., VD) and <em>objective</em> (i.e., VI) formats. They argued that dramatic qualitative changes caused by 90° <em>y</em>-axis rotations forced subjects to describe these objects egocentrically, and that this type of description overrode a more typical, objective mode based on objects' actual 3-D structures. Subsequent work by Rock and his colleagues (<a href="#c45">Rock &amp; Di Vita, 1987</a>; <a href="#c47">Rock et al., 1989</a>) supported this interpretation. <a href="#c45">Rock and Di Vita (1987)</a> showed that recognition was compromised if wire objects were displaced to new locations that induced changes in the shapes of their projected images. If these displacements were combined with rotations that corrected for changes in the shape of the projected image, recognition was successful. In addition, <a href="#c47">Rock et al. (1989)</a> demonstrated that subjects have difficulty imagining how these wire objects would look if rotated by 90° around the <em>y</em>-axis. This would not be the case if subjects had access to a VI representation.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Despite their reliability, there are reasons to question whether these findings are typical of the way that shape information is ordinarily represented. With the notable exceptions of leafless trees, scaffolding, wire sculpture, and psychologists' stimuli, wire forms are not representative of the vast majority of objects. As a result of their minimal surface area, effects of <em>y</em>-axis rotations on visible surfaces in wire objects are less dramatic than with objects composed of opaque surfaces. In contrast to surface objects, the same contours are often visible before and after rotations in depth because of the relative lack of self-occlusion. For example, 180° rotations entirely interchange visible and occluded portions of surface objects but often leave the same features visible in wire objects. Recognition might therefore be based on a certain portion of the object that is partially occluded or foreshortened when rotated by 90°. Furthermore, unlike many ordinary objects, these stimuli may be difficult to describe with generalized cones or a similar system of volumetric primitives (e.g., <a href="#c2">Biederman, 1987</a>; <a href="#c36">Marr, 1982</a>; <a href="#c37">Marr &amp; Nishihara, 1978</a>), perhaps making it difficult to compute VI representations (<a href="#c11">Farah et al., 1994</a>). For these reasons, it can be argued that <a href="#c46">Rock et al.'s (1981)</a> tasks do not provide a particularly strong test of the relationship between viewpoint and recognition.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Several follow-up studies have attempted to address these issues by exploring recognition of novel views using a wider variety of 3-D objects. <a href="#c11">Farah et al. (1994)</a> allowed subjects to study a small number of wire objects from a single viewpoint and then presented the objects in either the same orientation or rotated by 45° about the <em>y</em>-axis. Similar to <a href="#c46">Rock et al.'s (1981)</a> 90° condition, they found evidence for use of VD representations; subjects recognized 89.1% of these contour stimuli when they appeared in the previously studied orientation and only 68.7% of the rotated items. In a second experiment, Farah et al. created opaque surfaces on the wire objects by molding them with modeling clay. In contrast to performance with only contour information, the addition of surfaces greatly facilitated recognition of novel views. Subjects displayed VI recognition, correctly identifying 75.8% of the rotated objects compared with 77.3% of unrotated objects. In both studies, correct rejection rates were quite high (73.4% for surfaces and 89.8% for contours), suggesting that subjects were indeed performing well above chance. By contrast, a similar study of clay objects consisting of geometrically regular parts did find significant effects of orientation change on both recognition accuracy and latency. Results of <a href="#c20">Humphrey and Khan (Experiment 3, 1993</a>) raise the question of whether the presence of surface information alone is sufficient for the recovery of VI representations of shape. In contrast to Farah et al.'s findings, the authors found evidence for VD recognition of clay surface objects rotated by 40° or 80° in depth. One factor contributing to this inconsistency might be differences in the stimulus sets. Whereas Farah et al.'s objects resembled bent potato chips, many of Humphrey and Khan's items more closely resembled plumper versions of Rock et al.'s wire forms. Perhaps Farah et al.'s objects could be accurately represented by VI volumetric primitives, whereas Humphrey and Khan's objects could not be described in this manner.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">In a final experiment, <a href="#c11">Farah et al. (1994)</a> tested recognition of the contour and surface objects at novel views created by 30° and 60° <em>y</em>-axis rotations. Recognition of surface objects was less affected by orientation change (90.0% vs. 72.3%) than recognition of contour objects (87.3% vs. 56.9%). For both types of items, however, recognition accuracy did clearly decrease as a function of orientation change.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Overall, these findings suggest that single views of certain objects—those with substantial surface area—may allow computation of shape representations that are to some extent VI. However, because the range of orientations tested did not exceed 60°, it is difficult to compare these findings directly with those of <a href="#c46">Rock et al. (1981)</a>. One possibility is that recognition of objects with surface structure is only VI over a certain range of perspectives (e.g., <a href="#c23">Johnson &amp; Fazendeiro, 1997</a>). If so, then objects with substantial surface area, such as <a href="#c11">Farah et al.'s (1994)</a> clay forms, might also be difficult to recognize if rotated by 90° or more around the <em>y</em>-axis. A second possibility is that objects with surface structure do indeed allow for computation of truly VI representations of shape (i.e., <a href="#c46">Rock et al.'s, 1981</a>, objective mode). If so, then similar objects should be easily recognized over a more substantial range of rotations. Findings of <a href="#c20">Humphrey and Khan (1993)</a>, however, argue against this possibility.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">In summary, results from the few attempts to study recognition of novel views of actual 3-D objects created by rotations around the <em>y</em>-axis are equivocal. On the one hand, there is reason to believe that contour (i.e., wire) objects are represented in a VD format (<a href="#c11">Farah et al., 1994</a>; <a href="#c22">Johnson, 1993</a>; <a href="#c46">Rock et al., 1981</a>). On the other hand, several findings suggest that objects with surface properties that can, in principle, be described by volumetric primitives (e.g., clay surfaces) may be represented in a VI format (<a href="#c11">Farah et al., 1994</a>). However, more substantial rotations of surface objects do sometimes appear to significantly reduce recognition accuracy (Experiment 3, <a href="#c20">Humphrey &amp; Khan, 1993</a>). In addition, there is the curious finding that contour objects are accurately recognized when rotated by 180° (<a href="#c22">Johnson, 1993</a>; <a href="#c46">Rock et al., 1981</a>). Because this effect even occurs for objects that are not recognized accurately when rotated by 90°, it is not consistent with either VD or VI accounts of shape representation.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">These findings raise a number of intriguing possibilities. Perhaps, as <a href="#c46">Rock et al. (1981)</a> suggested, some views of objects are represented in a VI manner, whereas others are VD. Likewise, as concluded by <a href="#c11">Farah et al. (1994)</a>, perhaps objects are represented in different formats depending on their structural properties. Each of the experiments discussed above used a very similar procedure. Whether evidence was found for VI or VD representations appeared to depend on the types of stimulus objects used.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">In the following experiments, I attempted to address these possibilities by using <a href="#c46">Rock et al.'s (1981)</a> procedure to test subordinate-level recognition of highly similar, geometrically irregular, contour (wire) and surface (clay) objects, as well as items created from geometrically regular surface parts (plumbing fittings), following various transformations about the <em>y</em>-axis.<a id="b-fn3"> </a><sup><a href="#fn3" /></sup></p><a id="xhp-27-6-1468-ID0EKCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><span class="medium-bold" xmlns:Translation="urn:EBSCO-Translation"><a data-auto="ep_link" href="#toc" id="hd_toc_43" title="General Method">General Method</a></span><br xmlns:Translation="urn:EBSCO-Translation" /><a id="xhp-27-6-1468-ID0ECKCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Apparatus</h4><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">A three-channel Gerbrands tachistoscope was modified to present 3-D model objects. A base consisting of a 0.5-in. (1.3-cm) diameter plastic pipe was fixed inside the center of one chamber. Stimulus objects fit snugly over this base and could be manually interchanged through a door in the rear of the chamber, as well as rotated about the <em>y</em>-axis. The inside of the door was white and served as the background against which stimuli were displayed. The height of the base prevented it from being seen through the subjects' viewing portal. A second chamber contained a white tachistoscope card with a central fixation <em>x</em> in the center.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">The tachistoscope was interfaced with an IBM PC (Model xt), which displayed the trial lists to the experimenter and recorded subjects' responses from the keyboard.</p><a id="xhp-27-6-1468-ID0EBKCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Stimuli</h4><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Three different types of unfamiliar objects were used: (a) 16 geometrically irregular contour objects modeled from wire, (b) 16 geometrically irregular surface objects modeled from clay, and (c) 16 geometrically regular surface objects constructed from plumbing fittings. All of the objects used were asymmetrical about the axis of rotation (i.e., <em>y</em>- or vertical axis). Construction of the different stimulus objects for each experiment is described in detail below.</p><a id="xhp-27-6-1468-ID0EAKCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Procedure</h4><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">The design was such that subjects experienced each test item once from a single, static orientation, and then they were asked to recognize that item from either the same orientation or a novel view that was created by rotating the object clockwise by a specified amount around the <em>y</em>-axis prior to its exposure. Position on both the <em>z</em>- and <em>x</em>-axes was held constant.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">In each experiment, subjects were asked to study 4 objects from the total set of 16 items. At the beginning of each trial, subjects fixated a centrally presented <em>x</em>. An object was then displayed for 4 s. The object was subsequently replaced by the fixation point, and the object for the next trial was positioned by the experimenter. The time between displays was approximately 15 s. This sequence was repeated a total of four times until the subject had studied each target stimulus once.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">After studying each object, subjects performed a two-choice recognition task consisting of 8 objects: 4 were the previously studied target items, each appearing in one of four different orientations—the studied orientation or rotated by three different amounts around the <em>y</em>-axis. Details of the rotations used in each experiment are described in the <em>Method</em> sections below. The remaining 4 objects were previously unseen distractor items. Subjects were instructed to press a key labeled <em>yes</em> if they recognized the object as being one of the studied items or to press a key labeled <em>no</em> if the item seemed unfamiliar. If subjects were uncertain as to whether the item was familiar, they were instructed not to deliberate but to execute their first guess. Objects remained visible for 2 s during the recognition phase. Order of presentation in the study session was counterbalanced across each group of 8 subjects, and ordering of the test trials in the recognition phase was random for each subject. After a short break of approximately 2 min, the procedure was repeated with 8 previously unseen objects—4 new target objects and 4 new distractor objects.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">For an individual subject, each of the 16 stimulus objects appeared once in the recognition task, either as a target or as a distractor. Each of the four target orientations (studied view plus three different rotations) was, however, tested twice: once in each of the two blocks using different objects. Across every group of 8 subjects, each of the 16 stimulus objects appeared once as a target in each of four test orientations.</p><a id="xhp-27-6-1468-ID0EJCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><span class="medium-bold" xmlns:Translation="urn:EBSCO-Translation"><a data-auto="ep_link" href="#toc" id="hd_toc_59" title="Experiment 1: Geometrically Irregular Contour (Wire) Objects">Experiment 1: Geometrically Irregular Contour (Wire) Objects</a></span><br xmlns:Translation="urn:EBSCO-Translation" /><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Experiment 1 was an attempt to replicate the findings of <a href="#c46">Rock et al. (1981)</a> using geometrically irregular contour objects constructed from wire. With similar objects, Rock et al. found a large decrease in recognition accuracy for 90° <em>y</em>-axis rotations, but detected no effect for 180° <em>y</em>-axis rotations (<a href="#fig3">Figure 3</a>). As reviewed above, <a href="#c11">Farah et al. (1994)</a> also found that subjects had difficulty recognizing irregularly shaped wire objects rotated around the <em>y</em>-axis by 30°, 45°, or 60°, but did not test the effect of 90° or 180° rotations.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">I predicted that if these wire objects are represented in a VD format, then views rotated by 90° in either direction (i.e., 90° or 270°) and 180° would be more difficult to recognize than unrotated views. In contrast, if these objects are represented in a VI format, then novel and familiar views would be recognized equally well.</p><a id="xhp-27-6-1468-ID0EBJCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Method</h4><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><strong>Subjects</strong></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Twenty-four undergraduate students participated in the 15-min experiment for $2 or course credit. Subjects were naive to the hypotheses under investigation and had not participated in any related experiments.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><strong>Stimuli</strong></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Similar to those depicted in <a href="#fig4">Figure 4</a>, <a href="#fig16">16</a> wire objects were created using 16-in. (40.6-cm) lengths of a 2-mm diameter steel wire. Lengths of wire were bent into closed random shapes, with both ends inserted into a plastic base that fit onto the mount inside the tachistoscope. This allowed each object to be placed in a stable position and to be precisely rotated to the desired orientation by the experimenter.<br /><br /><a id="fig4"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/fad8cfe298afae9b555e007f7b404747/571c6483/pdh/xhp/xhp-27-6-1468-fig4a.gif" alt="xhp-27-6-1468-fig4a.gif" title="Figure 4. Effects of 90° and 180° rotations on a sample geometrically irregular contour (wire) object as used in Experiments 1 and 4A. Even in objects with minimal surface area, rotations of 90° induce dramatic changes in apparent shape as a result of foreshortening and, to a lesser extent, self-occlusion. Note that rotating wire contours 180° reflects the bounding contour and the sparse internal surface features, thereby creating what is essentially a 2-D mirror image of the studied view" /><em>Figure 4. Effects of 90° and 180° rotations on a sample geometrically irregular contour (wire) object as used in Experiments 1 and 4A. Even in objects with minimal surface area, rotations of 90° induce dramatic changes in apparent shape as a result of foreshortening and, to a lesser extent, self-occlusion. Note that rotating wire contours 180° reflects the bounding contour and the sparse internal surface features, thereby creating what is essentially a 2-D mirror image of the studied view</em></span><br /><br /><a id="fig16"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/95421f9fef3d1acc623a3623dba4199d/571c6483/pdh/xhp/xhp-27-6-1468-fig16a.gif" alt="xhp-27-6-1468-fig16a.gif" title="Figure 16. Results of Experiment 5: Effects of slight rotations away from 180° on recognition of geometrically irregular clay surface objects. Because subjects appear to rely on information in the 2-D bounding contour, rotating objects by as little as 30° in either direction from the recognized 180° view induces precipitous drops in recognition accuracy. Error bars reflect the percentage of standard error computed across subjects" /><em>Figure 16. Results of Experiment 5: Effects of slight rotations away from 180° on recognition of geometrically irregular clay surface objects. Because subjects appear to rely on information in the 2-D bounding contour, rotating objects by as little as 30° in either direction from the recognized 180° view induces precipitous drops in recognition accuracy. Error bars reflect the percentage of standard error computed across subjects</em></span></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><strong>Procedure</strong></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Subjects were told that “objects in the recognition test will appear at both the orientation studied and at other orientations created by rotating them around the vertical axis.” A model object, similar to those in the experiment, was used by the experimenter to demonstrate such rotations. Subjects were then told that “if the object is one of the four studied, then you should immediately press the key labeled <em>yes</em>, even if the object appears in an unfamiliar orientation.” This study phase was immediately followed by the two-choice recognition task described above in the General Method section. The average delay between study and test was approximately 2 min.</p><a id="xhp-27-6-1468-ID0EAJCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Results and Discussion</h4><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">As shown in <a href="#fig5">Figure 5</a>, relative to recognition accuracy, false-alarm rates were low, indicating that subjects were not merely guessing. Accuracy did not differ between 90° and 270° rotations (i.e., 90° in the opposite direction), <em>t</em>(23) = 0.9,<em> p </em>= .396. Data from these orientations were therefore combined to yield a single value for 90° rotations independent of direction. A one-way analysis of variance (ANOVA) revealed a significant effect of orientation change on recognition accuracy, <em>F</em>(2, 93) = 3.5,<em> p </em>= .033. Planned comparisons indicated a drop in recognition accuracy for views rotated by 90° when compared with unrotated views, <em>t</em>(23) = 2.2,<em> p </em>= .036. However, views rotated by 180° were recognized as accurately as unrotated views, <em>t</em>(23) = 0.7,<em> p </em>= .49.<br /><br /><a id="fig5"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/0f663edde9ceceac2fee9664b7d06651/571c6483/pdh/xhp/xhp-27-6-1468-fig5a.gif" alt="xhp-27-6-1468-fig5a.gif" title="Figure 5. Results from Experiment 1: Geometrically irregular contour (wire) objects. Results are very similar to those found by Rock et al. (1981). Subjects show a decrease in recognition accuracy for views rotated 90°, and performance is relatively unaffected by views rotated 180°. Error bars reflect the percentage of standard error computed across subjects" /><em>Figure 5. Results from Experiment 1: Geometrically irregular contour (wire) objects. Results are very similar to those found by Rock et al. (1981). Subjects show a decrease in recognition accuracy for views rotated 90°, and performance is relatively unaffected by views rotated 180°. Error bars reflect the percentage of standard error computed across subjects</em></span></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">These findings are highly consistent with those of <a href="#c46">Rock et al. (1981</a>, <a href="#fig3">Figure 3</a>), yet are equally ambiguous with respect to whether representations of shape are VD or VI. On the one hand, recognition of views rotated by 90° was VD. On the other hand, when the very same objects were rotated by 180°, recognition appeared VI. As illustrated in <a href="#fig4">Figure 4</a>, because of the relative lack of occluding surfaces, 180° rotations of contour objects produce views containing the same visible features as those studied.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">As discussed above, a case can be made to support that wire objects are unrepresentative of many kinds of objects that are typically recognized in everyday environments. One important difference is that they have very minimal surface structure, and this may make it difficult to perceive their 3-D structure. This fact could make it challenging to compute a VI representation (<a href="#c11">Farah et al., 1994</a>). A second difference concerns the possibility that diagnostic internal features may have remained visible over 180° rotations because of the relative absence of opaque surfaces. Such features may have been responsible for accurate recognition of these views, rather than VI representations per se (e.g., <a href="#c8">Corballis, 1988</a>; <a href="#c25">Jolicœur, 1990</a>). Experiment 2 addressed these issues by examining recognition of novel views of geometrically irregular objects with substantial opaque surfaces.</p><a id="xhp-27-6-1468-ID0EICAA" xmlns:Translation="urn:EBSCO-Translation"> </a><span class="medium-bold" xmlns:Translation="urn:EBSCO-Translation"><a data-auto="ep_link" href="#toc" id="hd_toc_80" title="Experiment 2: Geometrically Irregular Surface (Clay) Objects">Experiment 2: Geometrically Irregular Surface (Clay) Objects</a></span><br xmlns:Translation="urn:EBSCO-Translation" /><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">One prerequisite for computing VI representations may be recovery of objects' 3-D structures (e.g., <a href="#c37">Marr &amp; Nishihara, 1978</a>). Because wire objects have virtually no opaque surfaces, it may be difficult to perceive this information (<a href="#c11">Farah et al., 1994</a>). When <a href="#c11">Farah et al. (1994)</a> added surfaces to wire objects by modeling them with clay, they found that recognition of novel views improved considerably. As noted earlier, however, <a href="#c20">Humphrey and Khan (1993)</a> found evidence for VD recognition of clay objects despite substantial surface information. These conflicting results suggest that the presence of surface information alone may not be sufficient for computing VI shape representations. I created a set of geometrically irregular surface objects from clay to evaluate this possibility. I predicted that if the presence of surface information is critical for deriving VI representations, then these objects would be accurately recognized even when rotated by 90° and 180° around the <em>y</em>-axis. However, if these objects are not represented in a completely VI manner, then recognition would be more difficult for rotated views. Additionally, if subjects in Experiment 1 were relying on diagnostic internal features to accurately recognize views of wire forms rotated 180°, then—because of effects of self-occlusion—performance with clay objects at this orientation would drop significantly.</p><a id="xhp-27-6-1468-ID0EBICAA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Method</h4><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><strong>Subjects</strong></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Thirty-two undergraduate students received $2 or course credit for their voluntary participation in a 15-min experiment. None of the subjects were familiar with the hypotheses being tested nor had they taken part in any related experiments.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><strong>Stimuli</strong></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Sixteen asymmetrical objects were constructed from 2-in. × 2-in. × 4-in. (5.1-cm × 5.1-cm × 10.2-cm) blocks of gray modeling clay. The objects were formed into arbitrary 3-D shapes and fired in an oven. Plastic mountings were fixed to the bottoms of the objects so they could be mounted to the base in the tachistoscope and rotated to the desired orientation (see <a href="#fig6">Figure 6</a>).<br /><br /><a id="fig6"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/965ce4b1001e6be65ef77de97a7aa8bc/571c6483/pdh/xhp/xhp-27-6-1468-fig6a.gif" alt="xhp-27-6-1468-fig6a.gif" title="Figure 6. Effects of 90° and 180° rotations on a sample geometrically irregular surface (clay) object as used in Experiments 2 and 4B. Similar to wire forms, rotations of 90° induce substantial changes in the apparent shape of clay objects. This is the result of foreshortening and considerable self-occlusion. Unlike wire forms, the 3-D shape of these objects is also dramatically affected by 180° rotations. Due to their opaque surfaces, these transformations cause internal surface features to be completely interchanged by means of self-occlusion" /><em>Figure 6. Effects of 90° and 180° rotations on a sample geometrically irregular surface (clay) object as used in Experiments 2 and 4B. Similar to wire forms, rotations of 90° induce substantial changes in the apparent shape of clay objects. This is the result of foreshortening and considerable self-occlusion. Unlike wire forms, the 3-D shape of these objects is also dramatically affected by 180° rotations. Due to their opaque surfaces, these transformations cause internal surface features to be completely interchanged by means of self-occlusion</em></span></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><strong>Procedure</strong></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">The procedure was the same as in Experiment 1, except that 16 clay objects were substituted for the 16 wire objects.</p><a id="xhp-27-6-1468-ID0EAICAA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Results and Discussion</h4><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">As shown in <a href="#fig7">Figure 7</a>, false-alarm rates were again considerably below recognition accuracy in all orientation conditions. Likewise, accuracy data from 90° and 270° (90° in the opposite direction) rotations did not differ significantly, <em>t</em>(31) = 1.2,<em> p </em>= .25, and thus were combined for subsequent analyses. A one-way ANOVA revealed a highly significant effect of orientation change on recognition accuracy, <em>F</em>(2, 62) = 10.8,<em> p </em>&lt; .001. There was again no difference between the studied orientation and 180° views, <em>t</em>(31) = 0.6, <em>p</em> = .56; however, there was a significant drop in recognition accuracy for views rotated by 90° in either direction, <em>t</em>(31) = 2.8, <em>p</em> = .009.<br /><br /><a id="fig7"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/98ce07cd23f38d421c41797cb9d10d91/571c6483/pdh/xhp/xhp-27-6-1468-fig7a.gif" alt="xhp-27-6-1468-fig7a.gif" title="Figure 7. Results from Experiment 2: Geometrically irregular surface objects. Note the similarity of these results to those from Experiment 1. Recognition accuracy decreased for objects rotated 90°, but was relatively unaffected when objects were rotated 180°. Error bars reflect the percentage of standard error computed across subjects" /><em>Figure 7. Results from Experiment 2: Geometrically irregular surface objects. Note the similarity of these results to those from Experiment 1. Recognition accuracy decreased for objects rotated 90°, but was relatively unaffected when objects were rotated 180°. Error bars reflect the percentage of standard error computed across subjects</em></span></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">In summary, results of Experiment 2 were remarkably consistent with those of the first study and those of <a href="#c46">Rock et al. (1981)</a>. In contrast to <a href="#c11">Farah et al.'s (1994)</a> findings, subjects had considerable difficulty recognizing certain novel views of clay objects, even though they possessed substantial surface area. As would be expected if these objects were represented in a VD manner, views created by 90° <em>y</em>-axis rotations were difficult to recognize. These results are therefore consistent with those of <a href="#c20">Humphrey and Khan (1993)</a> in suggesting that the presence of surface area alone is not sufficient for computation of a truly VI representation of shape.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Nevertheless, recognition accuracy was again unaffected by 180° <em>y</em>-axis rotations. This viewpoint independence is not consistent with predictions of VD theories and cannot be explained by the presence of orientation-invariant internal features, because 180° rotations in depth completely interchange visible and occluded surfaces (<a href="#fig6">Figure 6</a>). As explained below, this later finding suggests reliance on information in the 2-D bounding contour (i.e., silhouette), which remains invariant over flips in depth, other than reversing left and right.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">One explanation for the similar recognition performance in Experiments 1 and 2 may have to do with the fact that both contour and surface objects lack a geometrically regular part structure. Both <a href="#c36">Marr's (1982</a>; <a href="#c37">Marr &amp; Nishihara, 1978</a>) and <a href="#c2">Biederman's (1987)</a> theories claim that VI representations of shape can only be computed if objects can be described using a relatively small class of geometrically regular volumetric primitives. Although generalized cones (<a href="#c37">Marr &amp; Nishihara, 1978</a>) and geons (<a href="#c2">Biederman, 1987</a>) can be used to accurately represent the shapes of a wide variety of objects (i.e., from airplanes to zebras) these primitives are not very useful for describing objects that cannot be easily decomposed into geometrically regular parts (e.g., faces or loaves of bread; see <a href="#c40">Pinker, 1984</a>, or <a href="#c56">Ullman, 1989</a>). The same might be said regarding the geometrically irregular objects used in Experiments 1 and 2. Experiment 3 was designed to evaluate this possibility by using objects constructed from a limited set of geometrically regular parts.</p><a id="xhp-27-6-1468-ID0EHCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><span class="medium-bold" xmlns:Translation="urn:EBSCO-Translation"><a data-auto="ep_link" href="#toc" id="hd_toc_101" title="Experiment 3: Geometrically Regular Surface Objects (Pipes)">Experiment 3: Geometrically Regular Surface Objects (Pipes)</a></span><br xmlns:Translation="urn:EBSCO-Translation" /><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">The third experiment required subjects to recognize novel views of objects constructed from a set of five plastic plumbing fittings. Unlike wire or clay objects, these items could be easily decomposed into geometrically regular parts that could be accurately described by volumetric primitives (<a href="#c2">Biederman, 1987</a>; <a href="#c37">Marr &amp; Nishihara, 1978</a>). I reasoned that if these objects are represented in a VI fashion, then rotating them by 90° or 180° around the <em>y</em>-axis would have no effect on recognition accuracy. If, however, these objects are represented in a VD manner, then recognition of novel views would be considerably less accurate than recognition of the familiar view.</p><a id="xhp-27-6-1468-ID0EBHCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Method</h4><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><strong>Subjects</strong></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Twenty-four students either were paid $2 or received course credit to participate in a single 15-min testing session. Subjects were naive to the hypotheses under investigation and did not participate in any related experiments.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><strong>Stimuli</strong></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Stimuli for Experiment 3 were created by joining five 1/2-in. (1.3-cm) plastic plumbing fittings together: a T joint, two 90° joints, and two 45° joints. Use of identical parts ensured that the objects could only be distinguished on the basis of their overall shape configuration and not on the presence or absence of distinctive features, overall differences in surface area, or volume. As illustrated in <a href="#fig8">Figure 8</a>, parts were arranged arbitrarily except that each object had a central T joint oriented vertically. This ensured that the principal axis of the objects was equally visible from each view and provided a trunk onto which the remaining four parts were fixed.<br /><br /><a id="fig8"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/9d2662d7f2e15f5074fd2cde8b863e2a/571c6483/pdh/xhp/xhp-27-6-1468-fig8a.gif" alt="xhp-27-6-1468-fig8a.gif" title="Figure 8. Effects of 90° and 180° rotations on a sample object constructed from geometrically regular plumbing fittings as used in Experiment 3. Similar to clay objects, these objects show a dramatic change in apparent 3-D shape when rotated 90° or 180° as a result of their opaque surfaces" /><em>Figure 8. Effects of 90° and 180° rotations on a sample object constructed from geometrically regular plumbing fittings as used in Experiment 3. Similar to clay objects, these objects show a dramatic change in apparent 3-D shape when rotated 90° or 180° as a result of their opaque surfaces</em></span></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><strong>Procedure</strong></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">The procedure was identical to that described in Experiment 1, except that the 16 objects were created from geometrically regular pipes, as described above.</p><a id="xhp-27-6-1468-ID0EAHCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Results and Discussion</h4><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Results from Experiment 3 were highly consistent with Experiments 1 and 2. <a href="#fig9">Figure 9</a> shows that false-alarm rates remained low relative to recognition performances. There was no difference in hit rates between the 90° and 270° (90° in the opposite direction) conditions, <em>t</em>(31) = 0.6,<em> p </em>= .54, and these scores were combined for further analyses. A one-way ANOVA performed across the 0°, 90°, and 180° conditions revealed a significant effect of orientation change on recognition accuracy, <em>F</em>(3, 92) = 3.2,<em> p </em>= .027. Planned comparisons showed a significant drop in recognition accuracy for views rotated 90°, <em>t</em>(31) = 2.6,<em> p </em>= .013. Again, objects rotated 180° were recognized with accuracy comparable with that of the unrotated condition, <em>t</em>(31) = 0.6,<em> p </em>= .557.<br /><br /><a id="fig9"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/202b25f45c0ec104127b17590102b169/571c6483/pdh/xhp/xhp-27-6-1468-fig9a.gif" alt="xhp-27-6-1468-fig9a.gif" title="Figure 9. Results from Experiment 3: Geometrically regular surface objects. Recognition accuracy decreased for views rotated by 90° and was relatively unaffected by 180° rotations. Error bars reflect the percentage of standard error computed across subjects" /><em>Figure 9. Results from Experiment 3: Geometrically regular surface objects. Recognition accuracy decreased for views rotated by 90° and was relatively unaffected by 180° rotations. Error bars reflect the percentage of standard error computed across subjects</em></span></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Despite being composed from geometrically regular parts, recognition of these items was strikingly consistent with that of geometrically irregular contour (Experiment 1) and surface (Experiment 2) objects. Accuracy decreased significantly for views rotated by 90° but was unaffected by rotations of 180°. Similar to surfaces (Experiment 2), the presence of geometrically regular parts does not appear to be sufficient for the computation of entirely VI representations.</p><a id="xhp-27-6-1468-ID0EGCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><span class="medium-bold" xmlns:Translation="urn:EBSCO-Translation"><a data-auto="ep_link" href="#toc" id="hd_toc_120" title="Summary of Experiments 1–3">Summary of Experiments 1–3</a></span><br xmlns:Translation="urn:EBSCO-Translation" /><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">These studies were motivated by two questions: First, do the many findings of viewpoint dependency in subordinate-level recognition of 2-D pictures extend to various types of 3-D objects? The present results suggest that regardless of objects' structural properties, the answer depends on the particular viewpoint. For certain novel views (those created by 180° rotations in depth) recognition appears not to be affected by changes in orientation. However, for other novel views (those rotated by 90° in depth) recognition is decidedly VD. These findings suggest that object recognition processes are neither exclusively VD nor VI. Instead, as discussed below, the system appears to pursue a more opportunistic course by exploiting a variety of different strategies in an attempt to solve the multifaceted problem of object recognition.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Second, are objects with different structural properties represented in a similar format? The remarkably consistent results across studies suggest that this may indeed be the case, at least for the limited types of objects tested (cf. <a href="#fig4">Figures 4</a>, <a href="#fig6">6</a>, and <a href="#fig8">8</a>). This observation was tested directly, by comparing recognition performances across all three experiments. Because these tests involved a total of 80 subjects, they also addressed the possibility that statistics performed on the individual experiments lacked sufficient power to detect differences between studied views and those rotated by 180°. As in the individual experiments, there was no difference between views rotated by 90° or 270°, <em>t</em>(79) = 0.3, <em>p</em> = .75, and data from these conditions were therefore averaged to yield a combined score reflecting recognition of views rotated by 90° in either direction. A two-way repeated measures ANOVA with task and stimulus orientation as factors revealed no significant difference in performance between tasks, <em>F</em>(2, 77) = 2.4, <em>p</em> = .10, <em>MSE</em> = 0.44. As with the individual studies, there was a main effect of stimulus orientation on recognition accuracy, <em>F</em>(3, 231) = 11.5, <em>p</em> &lt;.0001, <em>MSE </em>= 0.30. The interaction between task and orientation was, however, nonsignificant (<em>F</em> &lt; 1). As expected, there was a highly significant drop in recognition accuracy for views rotated by 90°, <em>t</em>(79) = 4.9, <em>p</em> &lt; .0001. Rotations of 180° did not cause a significant drop in performance, <em>t</em>(79) = 0.6, <em>p </em>= .57.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">In short, regardless of stimulus properties, subjects have considerable difficulty identifying objects rotated by 90°, even though the very same objects are accurately recognized when flipped in depth. The theoretical implications of these findings are considered below, along with two additional experiments.</p><a id="xhp-27-6-1468-ID0EFCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><span class="medium-bold" xmlns:Translation="urn:EBSCO-Translation"><a data-auto="ep_link" href="#toc" id="hd_toc_126" title="Viewpoint-Dependent Recognition of 90° Views">Viewpoint-Dependent Recognition of 90° Views</a></span><br xmlns:Translation="urn:EBSCO-Translation" /><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">For recognition to be VI, the same representation must be activated by both the studied and tested views (e.g., <a href="#c2">Biederman, 1987</a>; <a href="#c19">Hummel &amp; Biederman, 1992</a>; <a href="#c36">Marr, 1982</a>). The consistent drop in recognition accuracy in all three studies indicates that this clearly was not possible when objects were rotated 90° in depth. As noted by <a href="#c46">Rock et al. (1981)</a>, these transformations induce substantial qualitative changes in objects' appearances (see <a href="#fig4">Figures 4</a>, <a href="#fig6">6</a>, and <a href="#fig8">8</a>). One reason for this is that rotating an object 90° often causes one or more parts to appear foreshortened (e.g., <a href="#fig8">Figure 8</a>). Such foreshortening could make it difficult, if not impossible, to compute a VI shape representation (<a href="#c2">Biederman, 1987</a>; Humphrey &amp; Khan, 1993; <a href="#c33">Lowe, 1985</a>, <a href="#c34">1987</a>; <a href="#c36">Marr, 1982</a>; <a href="#c40">Pinker, 1984</a>). Consequently, the stimuli in my experiments fail to satisfy one or more of the conditions <a href="#c4">Biederman and Gerhardstein (1993)</a> deemed necessary for object recognition to be invariant over rotations in depth. Because 90° rotations can induce substantial accretion and deletion of parts, it is possible that different structural descriptions will be derived from two views of the same object. Likewise, geometrically irregular wire (Experiment 1) and clay (Experiment 2) objects may be difficult to describe using geometrically regular volumetric primitives such as geons (<a href="#c2">Biederman, 1987</a>). In contrast, plumbing part objects (Experiment 3) are built from geometrically regular elements that should be relatively easy to describe in terms of these primitives. However, because all plumbing part objects were constructed from a small set of parts in similar arrangements, they may not have yielded unique structural descriptions and therefore remained indistinguishable from one another. After all, <a href="#c2">Biederman's (1987)</a> recognition-by-components theory is intended to explain entry-level recognition as opposed to fine discriminations between instances of the same basic-level category. As discussed in detail below, despite failing to satisfy these criteria, all three classes of objects were accurately recognized when rotated 180°.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">The failure to recognize objects following 90° rotations can also be interpreted from the perspective of VD theories. As mentioned above, viewpoint interpolation models require familiarity with more than one perspective to identify novel views (e.g., see <a href="#c6">Bülthoff &amp; Edelman, 1992</a>; <a href="#c58">Ullman &amp; Basri, 1991</a>) and would thus predict failure in this situation. Likewise, alignment theories suggest that recognition will be difficult when familiar and test views depict different visible surfaces or aspects (e.g., see <a href="#c49">Tarr, 1995</a>). As illustrated in <a href="#fig4">Figures 4</a>, <a href="#fig6">6</a>, and <a href="#fig8">8</a>, this seems to be the case following 90° rotations. However, the fact that these very same objects were accurately identified following 180° rotations, which completely interchange visible and occluded surfaces, is clearly not consistent with these accounts.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">It is possible that subjects might be able to recognize novel views created by rotations in depth of less than 90°. In fact, results of <a href="#c11">Farah et al. (1994)</a> predict that this is indeed true for surface objects but not for contour objects. On the one hand, smaller rotations would tend to reduce foreshortening, possibly allowing the same part-based structural descriptions to be derived from both studied and novel views of surface objects. On the other hand, studied and novel views separated by smaller rotations would tend to have the same aspects visible, thereby making recognition through alignment of VD representations possible. To evaluate these possibilities, I performed a follow-up experiment using both wire contour and clay surface objects.</p><a id="xhp-27-6-1468-ID0EECAA" xmlns:Translation="urn:EBSCO-Translation"> </a><span class="medium-bold" xmlns:Translation="urn:EBSCO-Translation"><a data-auto="ep_link" href="#toc" id="hd_toc_132" title="Experiments 4A and 4B: Effects of Subtle Depth Rotations on Recognition of Contour and Surface Objects">Experiments 4A and 4B: Effects of Subtle Depth Rotations on Recognition of Contour and Surface Objects</a></span><br xmlns:Translation="urn:EBSCO-Translation" /><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">As discussed above, <a href="#c11">Farah et al.'s (1994)</a> results showed that 45° rotations in depth seemed not to affect recognition of surface objects. Because comparable rotations did induce drops in recognition of contour objects, they argued that surface information is critical to computing representations that are, at least to some degree, VI. However, this is at odds with <a href="#c20">Humphrey and Khan's (1993)</a> finding that recognition of solid clay objects is affected by depth rotations as slight as 40°. Because of inherent differences in the various stimulus sets, comparisons across studies must be interpreted with caution. However, given the similarity in recognition performance for rotated views of surface and contour objects in the current Experiments 1 and 2, an attempt was made to reevaluate these seemingly contradictory findings. In Experiment 4A, subjects were tested on their ability to recognize wire contour objects from Experiment 1 following rotations of 30°, 60°, and 90° in depth. Effects of identical rotations on identification of clay surface objects from Experiment 2 were explored in Experiment 4B. To the extent that recognition generalizes across more subtle changes in orientation, I expected that performance would be relatively unaffected by 30° and perhaps even 60° rotations. Furthermore, if surface information is necessary to achieve some degree of view independence, then generalization would be specific to clay objects.</p><a id="xhp-27-6-1468-ID0ECECAA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Method</h4><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Two separate groups of 24 naive undergraduates participated in Experiments 4A and 4B for course credit. The procedures were the same as in Experiments 1–3, except that objects could appear either in the studied view or rotated by 30°, 60°, or 90° around the <em>y</em>-axis. Stimuli for Experiment 4A consisted of the same 16 wire contour objects used in Experiment 1; stimuli for Experiment 4B consisted of the same clay surface objects used in Experiment 2. Effects of 30°, 60°, and 90° rotations on sample wire and clay objects are illustrated in <a href="#fig10">Figures 10</a> and 11, respectively.<br /><br /><a id="fig10"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/2975b8e6e31d4984b8aed322504e3b1e/571c6483/pdh/xhp/xhp-27-6-1468-fig10a.gif" alt="xhp-27-6-1468-fig10a.gif" title="Figure 10. Effects of rotating a wire contour object 30°, 60°, or 90°. For small rotations—30° and 60°—the object's apparent shape remains relatively unchanged because of minimal self-occlusion and foreshortening. However, there is a dramatic change in shape when the object is rotated from 60° to 90°" /><em>Figure 10. Effects of rotating a wire contour object 30°, 60°, or 90°. For small rotations—30° and 60°—the object's apparent shape remains relatively unchanged because of minimal self-occlusion and foreshortening. However, there is a dramatic change in shape when the object is rotated from 60° to 90°</em></span></p><a id="xhp-27-6-1468-ID0EBECAA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Results of Experiment 4A: Contour Objects</h4><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">The interest of Experiment 4A was in examining the extent to which successful recognition of the unrotated study view generalizes to rotated views. Therefore, the analysis was limited to those blocks in which the studied view was correctly identified. This was true for 22 out of 24 subjects and 38 out of a possible 44 blocks (two possible blocks per subject). A repeated measures ANOVA revealed a significant main effect of orientation change on recognition, <em>F</em>(3, 63) = 4.0, <em>p</em> = .01, <em>MSE </em>= 0.37. As shown in <a href="#fig12">Figure 12</a>, recognition performance tended to decrease linearly as a function of degrees of rotation away from the studied view, <em>F</em>(1, 21) = 15.6, <em>p </em>= .001, <em>MSE </em>= 0.26. As in the previous experiments, false-alarm rates remained low relative to recognition accuracy.<br /><br /><a id="fig12"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/9d90da962137e9cc83b7dce63f4de343/571c6483/pdh/xhp/xhp-27-6-1468-fig12a.gif" alt="xhp-27-6-1468-fig12a.gif" title="Figure 12. Results of Experiment 4A: Effects of 30°, 60°, and 90° rotations on recognition of geometrically irregular wire contour objects. Consistent with VD theories, recognition is affected by even subtle rotations away from the studied orientation. Error bars reflect the percentage of standard error computed across subjects" /><em>Figure 12. Results of Experiment 4A: Effects of 30°, 60°, and 90° rotations on recognition of geometrically irregular wire contour objects. Consistent with VD theories, recognition is affected by even subtle rotations away from the studied orientation. Error bars reflect the percentage of standard error computed across subjects</em></span></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Consistent with <a href="#c11">Farah et al.'s (1994)</a> results, planned contrasts revealed significant differences between the studied view and stimuli rotated by 30°, <em>t</em>(21) = 2.6, <em>p</em> = .02, by 60°, <em>t</em>(21) = 3.2, <em>p</em> = .005, and by 90°, <em>t</em>(21) = 3.8, <em>p</em> = .001. The fact that recognition accuracy was no worse at 30° than at either 60°, <em>t</em>(21) = 0.9, <em>p</em> = .39, or 90°, <em>t</em>(21) = 1.0, <em>p</em> = .31, suggests that there was little or no generalization from the studied view.</p><a id="xhp-27-6-1468-ID0EAECAA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Results of Experiment 4B: Surface Objects</h4><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">A total of 23 out of 24 subjects accurately recognized the studied view on 36 out of a possible 46 blocks of trials. <a href="#fig13">Figure 13</a> shows that false-alarm rates were below recognition accuracy in all conditions, as in Experiment 4A. In contrast to <a href="#c11">Farah et al.'s (1994)</a> findings, there was a strong effect of orientation change on recognition of surface objects, <em>F</em>(1, 22) = 11.6, <em>p</em> &lt; .0001, <em>MSE </em>= 0.31. Accuracy again tended to decrease linearly as objects were rotated further from the studied orientation, <em>F</em>(1, 22) = 27.9, <em>p</em> &lt; .0001, <em>MSE </em>= 0.31. Consistent with <a href="#c20">Humphrey and Khan's (1993)</a> findings, planned contrasts revealed significant drops in performance at 30°, <em>t</em>(22) = 3.5, <em>p</em> = .002, at 60°, <em>t</em>(22) = 5.5, <em>p</em> &lt; .0001, and at 90°, <em>t</em>(22) = 5.1, <em>p</em> &lt; .0001.<br /><br /><a id="fig13"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/15d0b528159ea650543fbf347377c0af/571c6483/pdh/xhp/xhp-27-6-1468-fig13a.gif" alt="xhp-27-6-1468-fig13a.gif" title="Figure 13. Results of Experiment 4B: Effects of 30°, 60°, and 90° rotations on recognition of geometrically irregular contour objects. In contrast to earlier findings (Farah et al., 1994), recognition of contour objects was also affected by rotations of as little as 30°. Error bars reflect the percentage of standard error computed across subjects" /><em>Figure 13. Results of Experiment 4B: Effects of 30°, 60°, and 90° rotations on recognition of geometrically irregular contour objects. In contrast to earlier findings (Farah et al., 1994), recognition of contour objects was also affected by rotations of as little as 30°. Error bars reflect the percentage of standard error computed across subjects</em></span></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">A repeated measures ANOVA comparing results from contour (Experiment 4A) and surface (Experiment 4B) objects failed to detect a significant difference, <em>F</em>(1, 43) = 2.3, <em>p</em> = .14, <em>MSE </em>= 0.77. Across both studies there was a strong tendency for recognition performance to decrease linearly with orientation disparity between studied and novel views, <em>F</em>(1, 43) = 42.8, <em>p</em> &lt; .0001, <em>MSE </em>= 0.28 (cf. Panels A and B in <a href="#fig10">Figure 10</a>). Unlike earlier findings by <a href="#c11">Farah et al. (1994)</a>, the drop in recognition of views rotated by 30° was as great, if not greater, for surface objects than for those consisting of contours. However, because of difficulties associated with equating the ease of discriminating stimulus items within these sets, this difference should be taken with caution.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Findings of these two studies suggest that recognition can be affected by rotations in depth of as little as 30°. As in Experiments 1–3, these effects appear to be remarkably similar regardless of objects' structural properties. Against this backdrop, accurate recognition of 180° views of these objects seems even more remarkable.</p><a id="xhp-27-6-1468-ID0EDCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><span class="medium-bold" xmlns:Translation="urn:EBSCO-Translation"><a data-auto="ep_link" href="#toc" id="hd_toc_148" title="Viewpoint-Invariant Recognition of 180° Views">Viewpoint-Invariant Recognition of 180° Views</a></span><br xmlns:Translation="urn:EBSCO-Translation" /><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">When <a href="#c46">Rock et al. (1981)</a> first observed accurate recognition of wire contour objects rotated 180° in depth, they attributed it to a VI, or objective, mode of recognition. Contrary to the results presented above, they believed that this VI mode was the rule rather than the exception. Because wire objects suffer minimal effects of self-occlusion when rotated in depth, Rock et al's finding—and that of the present Experiment 1—might also be accounted for by the presence of certain key surface features that remain visible across flips in depth (see, e.g., <a href="#c8">Corballis, 1988</a>; <a href="#c25">Jolicœur, 1990</a>). This does not, however, explain accurate recognition of opaque objects whose visible and occluded surfaces are completely interchanged as a result of 180° depth rotations (Experiments 2 and 3). One potential explanation of these results is that flips in depth do not dramatically alter the 3-D structural descriptions that are assigned to objects, because their gross part structure remains accessible. Consequently, the same 3-D structural descriptions may be activated by unrotated views and those flipped 180° (e.g., <a href="#c5">Biederman &amp; Gerhardstein, 1995</a>). Rotations of 180° do appear to induce less dramatic qualitative changes in objects' appearances (see <a href="#fig4">Figures 4</a>, <a href="#fig6">6</a>, and <a href="#fig8">8</a>).</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Another possibility is that observers were basing their decisions on representations constructed from information contained in the objects' 2-D silhouettes—or bounding contours—which separate visible and occluded surfaces. <a href="#fig14">Figure 14</a> illustrates that when objects are flipped in depth, left and right sides of bounding contours are reflected about the axis of rotation, but their shape remains invariant. Numerous studies have shown that perceptual processes often treat mirror reflections as equivalent (e.g., <a href="#c3">Biederman &amp; Cooper, 1991</a>; <a href="#c7">Cooper, Schacter, Ballesteros, &amp; Moore, 1992</a>). <a href="#c8">Corballis (1988)</a> has suggested that failure to distinguish between objects and their mirror reflections is because the parity—or handedness—of a shape only becomes distinguishable after recognition, and then only for those relatively rare instances for which it really matters, such as distinguishing between left and right shoes or gloves or discriminating between mirror images of an object as in the mental rotation task.<br /><br /><a id="fig14"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/4fc294bd2dabcce0845e0ad0f3f2075c/571c6483/pdh/xhp/xhp-27-6-1468-fig14a.gif" alt="xhp-27-6-1468-fig14a.gif" title="Figure 14. Effects of 90° and 180° y-axis rotations on a geometrically irregular clay object's visible surfaces (A) and silhouette (B). Note that 180° rotations in depth completely interchange opaque object's visible and occluded surfaces, inducing substantial change in apparent 3-D shape. The same transformation, however, has minimal effect on the 2-D shape of the bounding contour, which is reflected about the axis of rotation but otherwise unaltered" /><em>Figure 14. Effects of 90° and 180° y-axis rotations on a geometrically irregular clay object's visible surfaces (A) and silhouette (B). Note that 180° rotations in depth completely interchange opaque object's visible and occluded surfaces, inducing substantial change in apparent 3-D shape. The same transformation, however, has minimal effect on the 2-D shape of the bounding contour, which is reflected about the axis of rotation but otherwise unaltered</em></span></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">I conducted a final experiment in an effort to determine whether accurate recognition of 180° views involves use of part-based structural descriptions or information in objects' 2-D silhouettes.</p><a id="xhp-27-6-1468-ID0ECCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><span class="medium-bold" xmlns:Translation="urn:EBSCO-Translation"><a data-auto="ep_link" href="#toc" id="hd_toc_154" title="Experiment 5: Effects of Manipulating 2-D Bounding Contour Shape">Experiment 5: Effects of Manipulating 2-D Bounding Contour Shape</a></span><br xmlns:Translation="urn:EBSCO-Translation" /><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">The motivation behind this final study was simple: If recognition of 180° views is based on the 2-D bounding contour, then altering the shape of an object's silhouette—by rotating it slightly in depth away from the recognizable 180° view—would compromise recognition. However, if recognition of 180° views is based on recovery of a volumetric (i.e., 3-D), part-based structural description, then recognition would generalize to these adjacent views that have the same visible part structure. To eliminate the possibility of reliance on internal features that might be visible in both studied and novel views, this study was limited to the clay surface objects.</p><a id="xhp-27-6-1468-ID0EBCCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Method</h4><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Twenty-four naive undergraduates participated for course credit. The procedure and stimuli were identical to Experiments 2 and 4B except that objects could appear in the studied orientation or rotated by 150°, 180°, or 210°. As illustrated in <a href="#fig15">Figure 15</a>, even rotating objects 30° in either direction away from the 180° view affects the shape of the silhouette while leaving the visible part structure essentially unaltered.<br /><br /><a id="fig15"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/5060cf64370265bfa539a0c3ac47e06d/571c6483/pdh/xhp/xhp-27-6-1468-fig15a.gif" alt="xhp-27-6-1468-fig15a.gif" title="Figure 15. Effects of 150°, 180°, and 210° y-axis rotations on a geometrically irregular clay object's visible surfaces (A) and silhouette (B). Note that rotating the object 30° away from the recognized 180° view in either direction has only modest effects on the visible 3-D part structure (A). However, such rotations have substantial effects on the shape of the 2-D silhouette (B)" /><em>Figure 15. Effects of 150°, 180°, and 210° y-axis rotations on a geometrically irregular clay object's visible surfaces (A) and silhouette (B). Note that rotating the object 30° away from the recognized 180° view in either direction has only modest effects on the visible 3-D part structure (A). However, such rotations have substantial effects on the shape of the 2-D silhouette (B)</em></span></p><a id="xhp-27-6-1468-ID0EACCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Results</h4><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Because this study was intended to look at generalization from correct recognition of studied and/or 180° views, analyses were limited to those blocks on which both perspectives were correctly identified. This was true for 20 out of 24 subjects and 25 out of a possible 40 blocks. As shown in <a href="#fig16">Figure 16</a>, a repeated measures ANOVA detected a highly significant main effect of orientation, <em>F</em>(3, 57) = 15.6, <em>p</em> &lt; .0001, <em>MSE </em>= 0.16, which showed a significant linear trend, <em>F</em>(1, 19) = 15.1, <em>p</em> = .001, <em>MSE </em>= 0.18. Consistent with the idea that subjects were relying on similarities in the 2-D bounding contours to recognize 180° views, recognition did not generalize to views created by rotations of either 150°, <em>t</em>(19) = 3.9, <em>p</em> = .001, or 210°, <em>t</em>(19) = 5.5, <em>p</em> &lt; .0001. In short, the change in shape of the silhouette induced by rotating objects 30° in either direction from the recognized 180° view was enough to greatly disrupt recognition, <em>t</em>(19) = 7.7, <em>p</em> &lt; .0001. Because rotations of 30° induce minimal change in visible surface structure, this precipitous drop in accuracy would not be anticipated if subjects were basing their identification of 180° views on recovery of a volumetric, part-based, structural description (e.g., a GSD). Instead, it appears that the ability to accurately recognize objects flipped in depth is achieved on the basis of information in the 2-D bounding contour, or silhouette, the shape of which is affected even by slight rotations (<a href="#fig15">Figure 15</a>).</p><a id="xhp-27-6-1468-ID0EBCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><span class="medium-bold" xmlns:Translation="urn:EBSCO-Translation"><a data-auto="ep_link" href="#toc" id="hd_toc_164" title="What's in a Silhouette?">What's in a Silhouette?</a></span><br xmlns:Translation="urn:EBSCO-Translation" /><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">As any well-conceived line drawing illustrates, 2-D silhouettes are frequently adequate to provide an unambiguous perception of an object's 3-D shape (e.g., <a href="#c14">Hayward, 1998</a>; <a href="#c16">Hayward, Tarr, &amp; Corderoy, 1997</a>; <a href="#c37">Marr &amp; Nishihara, 1978</a>; <a href="#c39">Peterson &amp; Gibson, 1994</a>; <a href="#c44">Rock, 1973</a>). This is possible because there is a very precise relationship between convexities and concavities on the bounding contour and an object's 3-D shape (<a href="#c18">Hoffman &amp; Richards, 1984</a>; <a href="#c26">Koenderink, 1984</a>, <a href="#c27">1987</a>; <a href="#c35">Marr, 1977</a>). Curvature of the bounding contour is informative about the shape of objects' surface structures, even when those structures are occluded from a particular viewpoint (e.g., <a href="#c1">Beusmans, Hoffman, &amp; Bennett, 1987</a>; <a href="#c28">Koenderink &amp; van Doorn, 1976</a>, <a href="#c29">1979</a>; <a href="#c43">Richards, Koenderink, &amp; Hoffman, 1987</a>). <a href="#c26">Koenderink (1984)</a> demonstrated that the apparent curvature of the bounding contour enables certain inferences about an object's local surface geometry with a probability of 1. More specifically, Koenderink showed that “for any vantagepoint, and without any restriction on the shape of the rim [bounding contour between visible and occluded surfaces], a convexity of the contour corresponds to a convex patch of the surface, and a concavity to a saddle-shaped patch” (p. 328).</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">If recognition of objects rotated 180° in depth is indeed attributable to use of this information, then several interesting possibilities arise. For instance, these findings might be taken as empirical evidence that—at least in the case of 180° views—VI recognition of even novel views can be achieved using 2-D representations (e.g., <a href="#c6">Bülthoff &amp; Edelman, 1992</a>; <a href="#c9">Edelman &amp; Bülthoff, 1992</a>; <a href="#c33">Lowe, 1985</a>, <a href="#c34">1987</a>; <a href="#c42">Poggio &amp; Edelman, 1990</a>; <a href="#c59">Vetter et al., 1994</a>). <a href="#c59">Vetter et al. (1994)</a>, for instance, demonstrated that it is theoretically possible to exploit regularities (symmetries) in a 2-D image to derive a set of virtual views of an object that can then be directly compared against novel views. Perhaps during the study phase, observers constructed virtual views that depicted how the objects would look when rotated 180° in depth, on the basis of information in the 2-D bounding contour. These model views could then have been directly matched against descriptions of the novel views during the comparison phase of the experiment. However, as shown in Experiment 5, this interpretation is limited by the fact that even slight rotations that cause relatively subtle changes in the shape of the silhouette can have substantial effects on recognition accuracy.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Regardless of the precise mechanism involved, recognition of 180° views implies that the number of VD models required to represent all possible views of an object might be substantially reduced: Only one representation would be required for a view and its 180° complement, at least with respect to rotations about the vertical axis. As made clear by numerous previous studies, not all novel views created by 180° rotations are accurately recognized. It is well known that recognition is greatly compromised when objects are inverted (e.g., <a href="#c24">Jolicœur, 1985</a>; <a href="#c46">Rock et al., 1981</a>), a transformation that does not alter the shape of the silhouette, but changes the location of top and bottom. If 2-D representations of the bounding contour are used in these tasks as well, then it would seem that they do encode the location of objects' parts relative to the gravitationally defined upright.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Finally, one can speculate as to why the primate visual system displays such sensitivity to the shape of objects' bounding contours. One attractive possibility has to do with the importance of this information for planning and controlling manual prehension. As stated at the outset, recognizing objects outside the laboratory is often undertaken for the purposes of controlling object-oriented actions, many of which involve grasping. The shape of an object's bounding contour is key to planning how the hand must be configured in order to succeed at this task. Perhaps the effects observed in these studies reflect a representational system that has evolved to satisfy these demands.</p><a id="xhp-27-6-1468-ID0EACAA" xmlns:Translation="urn:EBSCO-Translation"> </a><span class="medium-bold" xmlns:Translation="urn:EBSCO-Translation"><a data-auto="ep_link" href="#toc" id="hd_toc_171" title="Conclusions">Conclusions</a></span><br xmlns:Translation="urn:EBSCO-Translation" /><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">This article began with a deceptively simple question: How is it possible to recognize views of an object that have never before been experienced? In contrast to contemporary theories, these results for experiments using 3-D objects suggest that the capacity for VI recognition of highly similar items may not be accounted for by use of exclusively VI or VD object representations. Instead, recognition of novel views of the very same objects may arise in several different ways, including use of information in the 2-D bounding contour. Together with conflicting results in the literature, the present findings illustrate that shape-based object recognition is not a unitary problem but rather a class of formally related problems whose preferred solutions may depend intimately on the demands of the task environment. In other words, the object recognition system might best be described as opportunistic in nature: capable of exploiting a wide variety of different avenues and sources of information to achieve recognition that is generally unaffected by the vagaries of factors including illumination, size, location, and orientation (see also <a href="#c30">Kourtzi &amp; Shiffrar, 1997</a>, <a href="#c31">1999</a>).</p><a id="xhp-27-6-1468-ID0EAA" xmlns:Translation="urn:EBSCO-Translation"> </a><span class="medium-bold" xmlns:Translation="urn:EBSCO-Translation"><a data-auto="ep_link" href="#toc" id="hd_toc_174" title="Footnotes">Footnotes</a></span><a id="fn1" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><sup><a href="#b-fn1">1</a></sup> I use the term <em>representation</em> to refer to the model or models of an object stored in memory, whereas I use the term <em>description</em> to refer to the perceptual representation of an object that is being experienced.</p><a id="fn2" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><sup><a href="#b-fn2">2</a></sup> I have chosen to use the terms <em>viewpoint invariant</em> (VI) and <em>viewpoint dependent</em> (VD) to refer to these two categories of theories, rather than the terms <em>object centered</em> and <em>viewer centered</em>. The latter terms have been typically associated with Marr's (e.g., <a href="#c37">Marr &amp; Nishihara, 1978</a>) approach to object recognition, whereas VI and VD are not committed to any one particular theory but instead reflect the general properties of two broad categories of theories.</p><a id="fn3" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><sup><a href="#b-fn3">3</a></sup> The term <em>subordinate level</em> is typically used to describe recognition tasks in which highly similar familiar objects belonging to the same category must be discriminated from one another. Because the nonsense objects used in the present studies do not belong to a specific semantic category, I use this term to reflect the fact that they are highly similar to one another in their physical appearance.</p><a id="xhp-27-6-1468-ID0E03B0ABAA" xmlns:Translation="urn:EBSCO-Translation"> </a><span class="medium-bold" xmlns:Translation="urn:EBSCO-Translation"><a data-auto="ep_link" href="#toc" id="hd_toc_182" title="References">References</a></span><a id="c1" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Beusmans, J. M. H., Hoffman, D. D., &amp; Bennett, B. M. (1987). Description of solid shape and its inference from occluding contours. <em>Journal of the Optical Society of America</em>, <em>4</em>, 1155–1167.</p><a id="c2" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Biederman, I. (1987). Recognition-by-components: A theory of human image understanding. <em>Psychological Review</em>, <em>94</em>, 115–145.</p><a id="c3" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Biederman, I., &amp; Cooper, E. E. (1991). Evidence for complete translational and reflectional invariance in visual object priming. <em>Perception</em>, <em>20</em>, 585–593.</p><a id="c4" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Biederman, I., &amp; Gerhardstein, P. C. (1993). Recognizing depth-rotated objects: Evidence and conditions for three-dimensional viewpoint invariance. <em>Journal of Experimental Psychology: Human Perception and Performance</em>, <em>19</em>, 1162–1182.</p><a id="c5" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Biederman, I., &amp; Gerhardstein, P. C. (1995). Viewpoint-dependent mechanisms in visual object recognition: Reply to Tarr and Bülthoff (1995). <em>Journal of Experimental Psychology: Human Perception and Performance</em>, <em>21</em>, 1506–1514.</p><a id="c6" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Bülthoff, H. H., &amp; Edelman, S. (1992). Psychophysical support for a two-dimensional view interpolation theory of object recognition<em>.</em>. <em>Proceedings of the National Academy of Sciences</em>, <em>89</em>, 60–64.</p><a id="c7" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Cooper, L., Schacter, D. L., Ballesteros, S., &amp; Moore, C. (1992). Priming and recognition of transformed three-dimensional objects: Effects of size and reflection. <em>Journal of Experimental Psychology: Learning, Memory, and Cognition</em>, <em>18</em>, 43–57.</p><a id="c8" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Corballis, M. C. (1988). Recognition of disoriented shapes. <em>Psychological Review</em>, <em>95</em>, 115–123.</p><a id="c9" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Edelman, S., &amp; Bülthoff, H. H. (1992). Orientation dependence in the recognition of familiar and novel views of three-dimensional objects. <em>Vision Research</em>, <em>32</em>, 2385–2400.</p><a id="c10" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Edelman, S., &amp; Weinshall, D. (1991). A self-organizing multiple view representation of three-dimensional objects. <em>Biological Cybernetics</em>, <em>64</em>, 209–219.</p><a id="c11" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Farah, M. J., Rochlin, R., &amp; Klein, K. L. (1994). Orientation invariance and geometric primitives in shape recognition. <em>Cognitive Science</em>, <em>18</em>, 325–344.</p><a id="c12" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Gibson, B. S., &amp; Peterson, M. A. (1994). Does orientation-independent object recognition precede orientation-dependent recognition? Evidence from a cuing paradigm. <em>Journal of Experimental Psychology: Human Perception and Performance</em>, <em>20</em>, 299–316.</p><a id="c13" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Grimson, W. E. L. (1991). <em>Recognition by computer</em>. Cambridge, MA: MIT Press.</p><a id="c14" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Hayward, W. G. (1998, November). <em>Silhouette recognition following constrained or unconstrained silhouette interpretations</em> Paper presented at the 39th annual meeting of the Psychonomic Society, Dallas, TX.</p><a id="c15" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Hayward, W. G., &amp; Tarr, M. J. (1997). Testing conditions for viewpoint invariance in object recognition. <em>Journal of Experimental Psychology: Human Perception and Performance</em>, <em>23</em>, 1511–1521.</p><a id="c16" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Hayward, W. G., Tarr, M. J., &amp; Corderoy, A. K. (1997, November). <em>Recognizing silhouettes and shaded images across viewpoint change</em> Paper presented at the 38th annual meeting of the Psychonomic Society, Chicago, IL.</p><a id="c17" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Hinton, G. E., &amp; Parsons, L. M. (1988). Scene-based and viewer-centered representations for comparing shapes. <em>Cognition</em>, <em>30</em>, 1–35.</p><a id="c18" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Hoffman, D. D., &amp; Richards, M. (1984). Parts of recognition. <em>Cognition</em>, <em>18</em>, 65–96.</p><a id="c19" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Hummel, J. E., &amp; Biederman, I. (1992). Dynamic binding in a neural network for shape recognition. <em>Psychological Review</em>, <em>99</em>, 480–517.</p><a id="c20" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Humphrey, G. K., &amp; Khan, S. C. (1993). Recognizing novel views of three-dimensional objects. <em>Canadian Journal of Psychology</em>, <em>46</em>, 170–190.</p><a id="c21" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Johnson, S. H. (1991). Commentary on Tarr and Pinker (1990). <em>Psychological Science</em>, <em>2</em>, 205–206.</p><a id="c22" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Johnson, S. H. (1993). <em>Recognizing novel views of three-dimensional objects</em> Unpublished doctoral dissertation, Cornell University, Ithaca, NY.</p><a id="c23" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Johnson, S. H., &amp; Fazendeiro, T. A. (1997, May). <em>Recognizing three-dimensional objects from novel viewpoints</em> Paper presented at the 1997 Convention of the American Psychological Society, Washington, DC.</p><a id="c24" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Jolicœur, P. (1985). The time to name disoriented natural objects. <em>Memory &amp; Cognition</em>, <em>13</em>, 289–303.</p><a id="c25" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Jolicœur, P. (1990). Identification of disoriented shapes: A dual systems theory. <em>Mind &amp; Language</em>, <em>5</em>, 387–410.</p><a id="c26" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Koenderink, J. J. (1984). What does occluding contour tell us about solid shape?<em>Perception</em>, <em>13</em>, 321–330.</p><a id="c27" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Koenderink, J. J. (1987). An internal representation for solid shape based on the properties of the apparent contour. In S.Ullman and W.Richards (Eds.), <em>Image understanding 1985–86</em> (pp. 257–285). Norwood, NJ: Ablex.</p><a id="c28" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Koenderink, J. J., &amp; van Doorn, A. J. (1976). The singularities of the visual mapping. <em>Biological Cybernetics</em>, <em>24</em>, 51–59.</p><a id="c29" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Koenderink, J. J., &amp; van Doorn, A. J. (1979). The internal representation of solid shape with respect to vision. <em>Biological Cybernetics</em>, <em>32</em>, 211–216.</p><a id="c30" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Kourtzi, Z., &amp; Shiffrar, M. (1997). One-shot view invariance in a moving world. <em>Psychological Science</em>, <em>8</em>, 461–466.</p><a id="c31" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Kourtzi, Z., &amp; Shiffrar, M. (1999). The visual representation of three-dimensional, rotating objects. <em>Acta Psychologica</em>, <em>102</em>, 265–292.</p><a id="c32" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Longuet-Higgins, C. H. (1990, January18). Recognizing three dimensions. <em>Nature</em>, <em>343</em>, 214–215.</p><a id="c33" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Lowe, D. G. (1985). <em>Perceptual organization and visual recognition</em>. Boston: Kluwer.</p><a id="c34" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Lowe, D. G. (1987). Three-dimensional object recognition from single two-dimensional images. <em>Artificial Intelligence</em>, <em>31</em>, 355–395.</p><a id="c35" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Marr, D. (1977). Analysis of occluding contour. <em>Proceedings of the Royal Society of London, Series B</em>, <em>197</em>, 187–217.</p><a id="c36" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Marr, D. (1982). <em>Vision</em>. San Francisco: Freeman.</p><a id="c37" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Marr, D., &amp; Nishihara, H. K. (1978). Representation and recognition of the spatial organization of three-dimensional shapes. <em>Proceedings of the Royal Society of London, Series B</em>, <em>200</em>, 269–294.</p><a id="c38" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Perrett, D. I., Smith, P. A. J., Potter, D. D., Mistlin, A. J., Head, A. S., Milner, A. D., &amp; Jeeves, M. A. (1985). Visual cells in the temporal cortex sensitive to face view and gaze direction. <em>Proceedings of the Royal Society of London, Series B</em>, <em>223</em>, 293–317.</p><a id="c39" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Peterson, M. A., &amp; Gibson, B. S. (1994). Must figure–ground organization precede object recognition? An assumption in peril. <em>Psychological Science</em>, <em>5</em>, 253–259.</p><a id="c40" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Pinker, S. (1984). Visual cognition: An introduction. <em>Cognition</em>, <em>18</em>, 1–63.</p><a id="c41" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Plaut, D. C., &amp; Farah, M. J. (1990). Visual object representation: Interpreting neurophysiological data within a computational framework. <em>Journal of Cognitive Neuroscience</em>, <em>2</em>, 320–343.</p><a id="c42" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Poggio, T., &amp; Edelman, S. (1990, January18). A network that learns to recognize three-dimensional objects. <em>Nature</em>, <em>343</em>, 263–266.</p><a id="c43" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Richards, W. A., Koenderink, J. J., &amp; Hoffman, D. D. (1987). Inferring three-dimensional shapes from two-dimensional silhouettes. <em>Journal of the Optical Society of America</em>, <em>4</em>, 1168–1175.</p><a id="c44" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Rock, I. (1973). <em>Orientation and form</em>. New York: Academic Press.</p><a id="c45" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Rock, I., &amp; Di Vita, J. (1987). A case of viewer-centered object perception. <em>Cognitive Psychology</em>, <em>19</em>, 280–293.</p><a id="c46" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Rock, I., Di Vita, J., &amp; Barbeito, R. (1981). The effect on form perception of change of orientation in the third dimension. <em>Journal of Experimental Psychology: Human Perception and Performance</em>, <em>7</em>, 719–732.</p><a id="c47" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Rock, I., Wheeler, D., &amp; Tudor, L. (1989). Can we imagine how objects look from other viewpoints?<em>Cognitive Psychology</em>, <em>21</em>, 185–210.</p><a id="c48" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Stankiewicz, B. J., Hummel, J. E., &amp; Cooper, E. E. (1998). The role of attention in priming for left–right reflections of object images: Evidence for a dual representation of object shape. <em>Journal of Experimental Psychology: Human Perception and Performance</em>, <em>24</em>, 732–744.</p><a id="c49" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Tarr, M. J. (1995). Rotating objects to recognize them: A case study on the role of viewpoint dependency in the recognition of three-dimensional objects. <em>Psychonomic Bulletin &amp; Review</em>, <em>2</em>, 55–82.</p><a id="c50" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Tarr, M. J., &amp; Bülthoff, H. H. (1995). Is human object recognition better described by geon structural descriptions or by multiple views?<em>Journal of Experimental Psychology: Human Perception and Performance</em>, <em>21</em>, 1494–1505.</p><a id="c51" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Tarr, M. J., &amp; Pinker, S. (1989). Mental rotation and orientation dependence in shape recognition. <em>Cognitive Psychology</em>, <em>21</em>, 233–282.</p><a id="c52" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Tarr, M. J., &amp; Pinker, S. (1990). When does human object recognition use a viewer-centered reference frame?<em>Psychological Science</em>, <em>1</em>, 253–256.</p><a id="c53" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Tjan, B. S., &amp; Legge, G. E. (1998). The viewpoint complexity of an object-recognition task. <em>Vision Research</em>, <em>38</em>, 2335–2350.</p><a id="c54" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Turnbull, O. H., Beschin, N., &amp; Della Sala, S. (1997). Agnosia for object orientation: Implications for theories of object recognition. <em>Neuropsychologia</em>, <em>35</em>, 153–163.</p><a id="c55" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Turnbull, O. H., Carey, D. P., &amp; McCarthy, R. A. (1997). The neuropsychology of object constancy. <em>Journal of the International Neuropsychological Society</em>, <em>3</em>, 288–298.</p><a id="c56" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Ullman, S. (1989). Aligning pictorial descriptions: An approach to object recognition. <em>Cognition</em>, <em>32</em>, 193–254.</p><a id="c57" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Ullman, S. (1990). Three-dimensional object recognition. <em>Cold Spring Harbour Symposium on Quantitative Biology</em>, <em>55</em>, 889–898.</p><a id="c58" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Ullman, S., &amp; Basri, R. (1991). Recognition by linear combinations of models. <em>IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</em>, <em>13</em>, 992–1006.</p><a id="c59" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Vetter, T., Poggio, T., &amp; Bülthoff, H. H. (1994). The importance of symmetry and virtual views in three-dimensional object recognition. <em>Current Biology</em>, <em>4</em>, 18–23.</p><a id="c60" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Weiskrantz, L., &amp; Saunders, R. C. (1984). Impairments of visual object transformations in monkeys. <em>Brain</em>, <em>107</em>, 1033–1072.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><em>Submitted: </em>November 25, 1996<em> Revised: </em>November 3, 2000<em> Accepted: </em>April 4, 2001</p><hr noshade="noshade" /><p class="body-paragraph" data-auto="copyright_info">This publication is protected by US and international copyright laws and its content may not be copied without the copyright holders express written permission except for the print or download capabilities of the retrieval software used for access. This content is intended solely for the use of the individual user.<br xmlns:ExtendedMarkupController="urn:ExtendedMarkupController" /><br xmlns:ExtendedMarkupController="urn:ExtendedMarkupController" /><strong xmlns:ExtendedMarkupController="urn:ExtendedMarkupController">Source:&nbsp;</strong>Journal of Experimental Psychology: Human Perception and Performance. Vol. 27. (6), Dec, 2001 pp. 1468-1484)<br xmlns:ExtendedMarkupController="urn:ExtendedMarkupController" /><strong xmlns:ExtendedMarkupController="urn:ExtendedMarkupController">Accession Number:&nbsp;</strong>2001-05318-014<br xmlns:ExtendedMarkupController="urn:ExtendedMarkupController" /><strong xmlns:ExtendedMarkupController="urn:ExtendedMarkupController">Digital Object Identifier:&nbsp;</strong>10.1037/0096-1523.27.6.1468</p></section></div>
		

		<div class="widget-loading loading"></div>
	
		<!-- WorldCat Widgets-->
		

	<!-- Full text will be rendered in this placeholder if citation is being displayed with
	full text. -->
	
	
	

	<div class="content-footer" >
	 

	</div>
	
	<div class="rs-placeholder" id="ctl00_ctl00_MainContentArea_MainContentArea_speaker_box" style="display:none;" data-parent="textToSpeechPlaceholder" data-readid="TextToSpeech" data-speed="MEDIUM" data-voice="ScanSoft_Jill_Full_22kHz" data-server="http://app.rs.ebscohost.com/cgi-bin/rsent?customerid=5845" data-download="true" data-isdetail="true"> </div>



						</div>
					</div>
					<div id="column1" class="collapsible" >
	<a class="collapsible-toggle" href="#" ></a>
	<div class="collapsible-content">
		
	


<h3 class="vis-hidden">View:</h3>
<ul class="format-control" >
		<li id="ctl00_ctl00_Column1_Column1_formatButtonsTop_formatButtonRepeater_ctl01_listItem" class="format-item">
			 
			<!-- Making assumption that we don't want spaces between MARC link and parenthesis. -->
			
			
			<a id="ctl00_ctl00_Column1_Column1_formatButtonsTop_formatButtonRepeater_ctl01_linkButton" title="Detailed Record" class="record-type format-citation" href="javascript:__doPostBack(&#39;ctl00$ctl00$Column1$Column1$formatButtonsTop$formatButtonRepeater$ctl01$linkButton&#39;,&#39;&#39;)">Detailed Record</a>
			
			
			
		</li>
	
		<li id="ctl00_ctl00_Column1_Column1_formatButtonsTop_formatButtonRepeater_ctl02_listItem" class="format-item active">
			 
			<!-- Making assumption that we don't want spaces between MARC link and parenthesis. -->
			
			
			
			<span id="ctl00_ctl00_Column1_Column1_formatButtonsTop_formatButtonRepeater_ctl02_label" title="HTML Full Text" class="record-type html-ftwg">HTML Full Text</span>
			
			
		</li>
	
		<li id="ctl00_ctl00_Column1_Column1_formatButtonsTop_formatButtonRepeater_ctl03_listItem" class="format-item">
			 
			<!-- Making assumption that we don't want spaces between MARC link and parenthesis. -->
			
			<span id="ctl00_ctl00_Column1_Column1_formatButtonsTop_formatButtonRepeater_ctl03_hddnInstructionMessage" class="hidden">This PDF document opens in a frame, to view the document outside of a frame, please change your Adobe Reader settings. To do this, open Adobe Reader, go to Help Menu and select Accessibility Setup Assistant option then select Use Recommend Settings and Skip Setup. You only need to do this once with the current computer you are using.</span>
			<a id="ctl00_ctl00_Column1_Column1_formatButtonsTop_formatButtonRepeater_ctl03_linkButton" title="PDF Full Text" class="record-type pdf-ft" href="javascript:__doPostBack(&#39;ctl00$ctl00$Column1$Column1$formatButtonsTop$formatButtonRepeater$ctl03$linkButton&#39;,&#39;&#39;)">PDF Full Text</a>
			
			
			<span id="ctl00_ctl00_Column1_Column1_formatButtonsTop_formatButtonRepeater_ctl03_suffix" class="format-note">(2.5MB)</span>
		</li>
	</ul>
	

	
	<div id="citedExternalSources"  style="display:none;">
		
	</div>
	

	
	
	
	
	</div>
</div>
					<div role="complementary" id="column2" class="collapsible" >
	<a class="collapsible-toggle" href="#" ></a>
	<div class="collapsible-content" >
		
	<hr class="vis-none" />
<h2 title="Tools" accesskey="5" tabindex="0" class="article-tools-header" id="ArticleTools"  >Tools</h2>
<ul class="article-tools delivery-control" >
		<li class="article-tool" >
			<a   href="#" title="Print" class="print-link"    data-panel='{"Id":"print","Url":"delivery/printpanel","Js":"ep/controller/control/citationdeliverypanel.js"}' >Print</a>
		</li>
		<li class="article-tool" >
			<a   href="#" title="E-mail" class="email-link"    data-panel='{"Id":"email","Url":"delivery/emailpanel","Js":"ep/controller/control/citationdeliverypanel.js"}' >E-mail</a>
		</li>
		<li class="article-tool" >
			<a   href="#" title="Save" class="save-link"    data-panel='{"Id":"save","Url":"delivery/savepanel","Js":"ep/controller/control/citationdeliverypanel.js"}' >Save</a>
		</li>
		<li class="article-tool" >
			<a   href="#" title="Cite" class="cite-link"    data-panel='{"Id":"cite","Url":"delivery/citepanel","Js":"ep/controller/control/citepanel.js"}' >Cite</a>
		</li>
		<li class="article-tool" >
			<a   href="#" title="Export" class="export-link"    data-panel='{"Id":"export","Url":"/ehost/delivery/exportpanel?sid=9978e2e2-c03c-4987-8900-75d99d89b0d3@sessionmgr115\u0026vid=0\u0026form=False","Js":"ep/controller/control/exportpanel.js"}' >Export</a>
		</li>
		<li class="article-tool" >
			<a   href="#" title="Permalink" class="permalink-link"    data-panel='{"Id":"permalink","Url":"delivery/permalinkpanel","Js":"ep/controller/control/plinkpanel.js"}' >Permalink</a>
		</li>
		<li class="article-tool" >
			<a   href="#" title="Share" class="bookmark-link"    data-panel='{"Id":"bookmark","Url":"addthis/addthispanel","Js":"ep/controller/control/bookmarkpanel.js"}' >Share</a>
		</li>

</ul>

	</div>
</div>
				
					
				
				<div class="extra1" role="presentation">&nbsp;</div>
			</div>
			<div class="footer-wrapper" >
				
	

				<div class="push-sticky-footer"></div>
			</div>
		</div>
		
	

				
		


	</form>
	
</body>
</html>
