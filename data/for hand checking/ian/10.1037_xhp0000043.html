
<!DOCTYPE html>
<html id="_htmlTag" lang="en">

<head><meta charset="utf-8" /><title>
	More than accuracy: Nonverbal dialects modulate the time course of vocal em...: EBSCOhost
</title>
	
<link rel="icon" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/ehost/favicon.ico" type="image/x-icon" />
<link rel="shortcut icon" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/ehost/favicon.ico" type="image/x-icon" />

	<link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/css/ehost/master_bundle.css" media="All" />
<link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/css/_layout2/rtac.css" media="All" />
<link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/css/common/abody.css" media="All" />
<link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/css/_layout2/selecteddatabasescontrol.css" media="All" />
<link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/css/_layout2/page/detail.css" media="All" />
<link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/css/_layout2/carousel.css" media="All" />
<link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/css/_layout2/bookcarousel.css" media="All" />
<link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/css/_layout2/emailprintdialog.css" media="All" />
<link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/css/_layout2/print.css" media="Print" />
<!--[if lt IE 9]><link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/css/_layout2/ie8.css" media="All" /><![endif]-->
<!--[if lt IE 8]><link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/css/_layout2/ie7.css" media="All" /><![endif]-->
<!--[if lt IE 7]><link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/css/_layout2/ie6.css" media="All" /><![endif]-->
<!--##EPCSS##-->
	
	<script>
var ep = {"version":"16.1.0.155","baseImagePath":"http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/","brandingPath":"http://imageserver.ebscohost.com/branding/","interfaceId":"ehost","cssLayout":2,"messages":{"Close":"Close","Loading":"Loading","show_this_area":"Show this area","hide_this_area":"Hide this area","column1-closed":"Show Left Column","column1-open":"Hide Left Column","column2-closed":"Show Right Column","column2-open":"Hide Right Column","sh_more":"Show More","sh_less":"Show Less","enter_email_address":"Please enter your e-mail address.","email_invalid_error":"Please provide a valid email address.","field_required":"This field is required.","your_subject_may_not_contain_html_markup":"Your subject may not contain HTML markup.","your_comments_may_not_contain_html_markup":"Your comments may not contain HTML markup.","err_sending_email":"Error Sending Email","your_message_may_not_contain_html_markup":"Your message may not contain HTML markup."},"clientData":{"googleTagManagerId":"GTM-NCMJP5","usrNo":0,"currentRecord":{"Db":"pdh","Tag":"AN","Term":"2015-11176-001"},"rtacView":"detail","rtacTimeout":30,"addThis":{"widgetUrl":"http://s7.addthis.com/js/250/addthis_widget.js#username=ebscohost","bookmarkUrl":"http://www.addthis.com/bookmark.php?v=250\u0026username=ebscohost"},"hoverPreviewLabelData":"{\"Abstract\":\"Abstract\",\"Date\":\"Date\",\"Source\":\"Source\",\"Subjects\":\"Subjects\",\"Title\":\"Title\",\"Citation\":\"Detail\",\"FullCitation\":\"Detailed Record\",\"AddToFolder\":\"Add to folder\",\"RemoveFromFolder\":\"Remove from folder\",\"FolderItem\":\"Folder Item\",\"AddExternalRecToFolder\":\"Add citation to Other Contents Folder\",\"RemoveExternalRecFromFolder\":\"Remove citation from Other Content Sources Folder\",\"AddToFolderTitle\":\"Add result to folder\",\"RemoveFromFolderTitle\":\"Remove result from folder\",\"AddRemoveToFolder\":\"Add/Remove \",\"AddRemoveToFolderTitle\":\"Add or remove from folders\",\"PublicationType\":\"Publication Type\",\"Database\":\"Database\",\"Duration\":\"Length (hours:minutes)\"}","plink":"http://search.ebscohost.com/login.aspx?direct=true\u0026db=pdh\u0026AN=2015-11176-001\u0026site=ehost-live"},"templates":{},"pageScripts":["bundled/jqueryplusui.js","bundled/underscore.js","bundled/_layout2/master.js","bundled/ehost/page/detail.js","bundled/buzzloader.js","bundled/buzzsessionsync.js","ep/selectdb.js","ep/widgets/epeditor.js","ckeditor/ckeditor.js","ckeditor/adapters/jquery.js","bundled/notesmodal.js","jquery/plugins/jquery.ba-bbq.js","ep/controller/realtimeavailabilitycontroller.js","ep/jqueryplugins/scrollto.js","ep/controller/concurrentaccesscontroller.js","ep/ep_readspeaker.js","ep/common/menubar.js","ep/googleclassroom/gc-boot.js"],"relativeRequestPath":"detail/detail","sid":"6609b2cc-c9f7-464c-97d7-6d549f841159@sessionmgr115","vid":"0","existingReturnUrl":"","newReturnUrl":"/ehost/detail/detail?sid=6609b2cc-c9f7-464c-97d7-6d549f841159@sessionmgr115\u0026vid=0\u0026hid=109\u0026bdata=JnNpdGU9ZWhvc3QtbGl2ZQ==","locale":"en"}
</script>
<script src="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/javascript/bundled/ep_boot.js"></script>
<!--[if lt IE 9]>
<script src="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/javascript/html5shiv/html5.js"></script>
<![endif]-->
<script>

ep.boot(function() {
	ep.updateSearchMode();
	focusOnMainContent();
	
ep.util.url.updateHash("db=pdh&AN=2015-11176-001");

	ep.getInstance( { epId: 'ep.controller.page.CitationController' });
	ep.getScreenResolution();

},
null);
</script>
<!--##EPJS##-->
	
	
</head>
<body id="ctl00_ctl00__bodyTag" class="column1-open column2-open limited-scope no-skin detail ehost">
	
	

	<div id="epAjaxActive">Loading...</div>	
	<form method="post" action="detail?sid=6609b2cc-c9f7-464c-97d7-6d549f841159%40sessionmgr115&amp;vid=0&amp;hid=109&amp;bdata=JnNpdGU9ZWhvc3QtbGl2ZQ%3d%3d" id="aspnetForm">
<input type="hidden" name="AddToFolderClientIDs" id="AddToFolderClientIDs" value="" />
<input type="hidden" name="RelRequestPath" id="RelRequestPath" value="detail/detail" />
<input type="hidden" name="__sid" id="__sid" value="6609b2cc-c9f7-464c-97d7-6d549f841159@sessionmgr115" />
<input type="hidden" name="__vid" id="__vid" value="0" />
<input type="hidden" name="__CUSTOMVIEWSTATE" id="__CUSTOMVIEWSTATE" value="H4sIAAAAAAAEAI1WbW8TRxAWF19eiGgQRflSKR5UqYol20pCDISiSsEQFbXQCFK+ovXt3t22d7vu7p6D+x/6m9tnd88vKID6Ic7eeWaeZ2aemfW/t/b4Xro7OD49PXv46Ozx49HefnLw7XtWSc6ceCv+aoR1rzUXya39JN/j7Udyy9t1KqaKNBEKLzb8izSrmLXps0xXTa2OB3oqFMWHk/hQyVo6wQc2wyMpPbB/SkVcOCYrEqW2DrFSQNxd4Pi4W++llZNKlJwnW+G9h+tulC0TnHiyvdeeeLLT+vL9Tr5621kdb4dTEbPZjlCbHmn3w4NXTtRj3SiXbOxvhiBJ+7mFALcPtsfWjkOe3xmRacMHbo5ccm1q5gaZdMxJrQ52x7qumeJvWC3Ss5eXw19FwbL5cKyVM7qqhLHDFyHv4UVwtcNx6xufnzfOIc5eG+fcFE0tlOMHnQ8P3j1PPH9kkizb0U3Sey0JlLgmljk5ExylWhYjZtBN03vrzKc8H+Suu5k++18sL3l+0VTVlfjo1ol2t3h3e8HqDnA6Bx1vk+4cPjk9Gj765XkvfrcRVQTCHS+qWNu8bUgndHyzu5PeHXuh0FuRCyNUJmx3K7299tTGisadw9NR73MGaz1PF0dgJq3f/QupOL2DKCtmgGWbytmDrSutqys5Te+/Fc5IMRPEqopM/Bow/9x5rY0gVzKFMmeNQcme0hutZsJMWEVcskpkzlKteVNhkGCKP1kLTENjrCCd00xnMBW19i0n35BCyXBmmdHWUga0BqBD+t1KVRCjAvLAYcoM47Ko+wgrLVnX8DlJgFsnYYKqeTjFvLdH8k9SDQqjmykxPmPKscK/+gqHyRxUayB5wBNkhNgqc2ukLq1ouEbvhWG+4LAHhbm3P6UJszJbBLZ9Eh+ncLKgBtSXqqikLQmqpp/RANmna3SNvAkU7vnrm0bYHdYJ5RVJr1AkziNT2JYi8syhS1oxAizLStRH1k3VWLpmlmJ9cqNrkmgQyAnnQ4C8daZBhiPw0PAONZegY2asst5mKtVU4w1dl1hlX6iqqIUpPLYnDkOYmdgnLnOI00bvmIyHQGDmt4eQBV5jozY+zOEif7TAyUxOEd7SHw0vvE8oSI+0gatF1wC18ozV+qxfG7Q3pHMF8lx8XOijFtDyQgiSow0yl1lYRxSTBsVYzzZjYDZqIplvKrytV4WPthgIYi7aB923hUMHoLgsDAUfLiaObKmvg2zhA4EEt2VCPr3Q6r7XNHbaSglRNznDowmMaj+XkYFrB1RpNbjhh6K3av9bLGU6pKuv9TRod6JduWJkieUeek0xp0dHVAeAUTyFkugGKswQIG5ouAYR+8pPZCV9plUr8wDexz/BfGht8A3ioxsSi22+PkAYGC6zIOgvlWB9psFk4dmm7Ic/TmbMH2vfhhqhVRKJtovM89Rm3icsL8o9S1VYT2oGpZAS1yT8CTMXW6hubkJZTwVsQ/l81iuVqLiuRL7M8MZy/GQgKWzHQVxEMMIA+63op32GwQrT5SmCUeMHl6EcpSzKar4qY1CZR8X+arvtGFrslqob0uGlnWev3lz8Ri+YY1hoAnL1tyYdZj06OToe0fnleT9eDYiPJP3+MrhzezQOG3y84PgijH64l3CONX3Zqo7eTYWA3scl1jqaafyqzSyd4+pwpdbAO35Cc4j2B1ZPfyRdcWF69BpR6ELU+IfLbTfen/7mXf5sitfeN5++WP6YCR476ffPHgwG1ID3U7KjJ0dHo4dnw5pJNQy/xogGg5/4f8X+bEknCgAA" />
<input type="hidden" name="__ScreenResolution" id="__ScreenResolution" value="" />
<input type="hidden" name="__VIEWSTATE" id="__VIEWSTATE" value="" />

		<!--[if lt IE 7]>	
		

<div class="ie6_req_block">
	<div class="ie6_req_text">
		IE6 users please note our browser requirements are changing! See  <a href="http://support.epnet.com/knowledge_base/detail.php?id=25" target="_blank" title="EBSCO's Support Site">EBSCO's Support Site</a>  for more information.
	</div>
</div>
		<![endif]-->
		
		<div id="outerContainer">
			<div id="innerContainer">
				
					
				
				
	
	
		

				
	
	

				
				<div id="header" class="clearfix" role="banner" >
					<div id="pageInstruction" tabindex="-1" class="hidden">citation_instruction</div><p tabindex="0" class="hidden"><a href="javascript:openWideTip('http://support.ebsco.com/help/?int=ehost&lang=en&feature_id=access&TOC_ID=Always&SI=0&BU=0&GU=1&PS=0&ver=&dbs=pdh')">Accessibility Information and Tips</a> Revised Date: 07/2015</p>
					<h1 title="More than accuracy: Nonverbal dialects modulate the time course of vocal emotion recognition across cultures" class="hidden">More than accuracy: Nonverbal dialects modulate the time course of vocal emotion recognition across cultures</h1>
					
					
	<div class="customerLogo"><a href="javascript:__doPostBack('ctl00$ctl00$FindField$customerLogo');"><img src="http://www.tilburguniversity.edu/static/uvtpresentation/images/framework/logo.jpg" alt="Library Logo" /></a></div>
	

					
				</div>
					<div id="mainContentArea" >
						<div id="content" role="main" class="text-normal" >
							
	
	<div class="content-header" >
	 

	</div>
	
	
	

	
	<div id="ToolPanelContent" class="bg-p2" >
		<div class="wrapper clearfix" >
		</div>
		<a  href="#" title="Close Panel" class="close-panel"></a>
	</div>

	<!-- If citation is being displayed it will be rendered inside this placeholder.
		 If citation is not being displayed, full text will be rendered in this placeholder. -->
	<div class="ft-translation hidden"><label for="transLanguage">Translate Full Text:</label></div><div class="ft-translation"><a name="Translate"> </a><select id="transLanguage" name="transLanguage" title="Choose Language"><option value="" selected="selected">Choose Language</option><option value="Arabic">الإنجليزية/العربية</option><option value="Bulgarian">английски език/български</option><option value="SimplifiedChinese">英语/简体中文</option><option value="TraditionalChinese">英語/繁體中文</option><option value="Czech">angličtina/čeština</option><option value="Danish">Engelsk/dansk</option><option value="Dutch">Engels/Nederlands</option><option value="French">Anglais/Français</option><option value="German">Englisch/Deutsch</option><option value="Greek">Αγγλικά/Ελληνικά</option><option value="Hausa">English/Hausa</option><option value="Hebrew">אנגלית/עברית</option><option value="Hindi">अंग्रेज़ी/हिंदी</option><option value="Hungarian">angol/magyar</option><option value="Indonesian">Inggris/bahasa Indonesia</option><option value="Italian">Inglesi/Italiano</option><option value="Japanese">英語/日本語</option><option value="Korean">영어/한국어</option><option value="Norwegian">Engelsk/Norsk</option><option value="Persian">انگليسی/فارسی</option><option value="Polish">angielski/polski</option><option value="Portuguese">Inglés/Português</option><option value="Pashto">English/Pashto</option><option value="Romanian">Engleză/română</option><option value="Russian">Английский/Русский</option><option value="Spanish">Inglés/Español</option><option value="Serbian">English/Serbian</option><option value="Swedish">Engelska/svenska</option><option value="Thai">อังกฤษ/ไทย</option><option value="Turkish">İngilizce/Türk</option><option value="Ukranian">Англійська/Українська</option><option value="Urdu">انگریزی/اردو</option></select>&nbsp;<input type="button" id="translateBtn" class="translate" value="Translate" title="Translate" /><input type="button" id="translateOriginal" class="translate" value="Back to English" title="Back to English" /></div><div id="translationProgressContainer" style="display: none;"><span>Translation in Progress:</span><div class="translationProgressBar"><div id="translationProgressBar" class="bg-p1"> </div></div></div><div id="translationErrorContainer" class="medium-normal translation-message" style="display: none;"> </div><div id="translationDisclaimerContainer" style="display: none;"><div class="translation-message"><span class="medium-bold"><span class="txt-red" id="translationDisclaimerLine1"> </span></span><span class="medium-normal" id="translationDisclaimerLine2"> </span><span class="medium-bold" id="translationDisclaimerLine3"> </span><div class="medium-normal">Translations powered by Language Weaver Service<br /></div></div></div><script type="text/javascript">
				ep.getInstance("ep.controller.control.translation");
				ep.require( "common/translation.css" );
			</script><dl class="short-citation" data-auto="short_citation" xmlns:viewExtensions="http://www.ebscohost.com/schema/viewExtensions"><dt class="medium-bold" data-auto="short_citation_title_label">Title: </dt><dd class="medium-normal" data-auto="short_citation_title">More than accuracy: Nonverbal dialects modulate the time course of vocal emotion recognition across cultures.<span class="updated-short-citation"> By: Jiang, Xiaoming, Paulmann, Silke, Robin, Jessica, Pell, Marc D., Journal of Experimental Psychology: Human Perception and Performance, 00961523, 20150601,  Vol. 41,  Issue 3</span></dd><dt class="medium-bold" data-auto="short_citation_long_dbname_label">Database: </dt><dd class="medium-normal" data-auto="short_citation_long_dbname">PsycARTICLES</dd></dl><div class="full-text-container border" data-auto="fulltext_container" xmlns:viewExtensions="http://www.ebscohost.com/schema/viewExtensions"><h2 class="hidden" data-auto="fulltext_title_hidden">HTML Full Text</h2><h2 data-auto="local_abody_title" class="ft-title border color-p4 bar4">More Than Accuracy: Nonverbal Dialects Modulate the Time Course of Vocal Emotion Recognition Across Cultures</h2><div class="html-ft-toc" data-auto="html_toc"><h3 class="small-bold" id="toc" data-auto="html_toc_title">Contents</h3><ol data-auto="html_toc_list"><li class="link-medium" data-auto="html_toc_list_item"><a data-auto="ep_link" href="#xhp-41-3-597-ID0EFCAA" id="hd_xhp-41-3-597-ID0EFCAA" title="Cross-Cultural Studies of Emotional Prosody Recognition">Cross-Cultural Studies of Emotional Prosody Recognition</a></li><li class="link-medium" data-auto="html_toc_list_item"><a data-auto="ep_link" href="#xhp-41-3-597-ID0EECAA" id="hd_xhp-41-3-597-ID0EECAA" title="Is There an In-Group Advantage in Recognition Speed?">Is There an In-Group Advantage in Recognition Speed?</a></li><li class="link-medium" data-auto="html_toc_list_item"><a data-auto="ep_link" href="#xhp-41-3-597-ID0EDCAA" id="hd_xhp-41-3-597-ID0EDCAA" title="The Current Study">The Current Study</a></li><li class="link-medium" data-auto="html_toc_list_item"><a data-auto="ep_link" href="#xhp-41-3-597-ID0ECCAA" id="hd_xhp-41-3-597-ID0ECCAA" title="Method">Method</a></li><li data-auto="html_toc_list_item" class="link-medium html-toc-hd1"><a data-auto="ep_link" href="#xhp-41-3-597-ID0EECCAA" id="hd1_xhp-41-3-597-ID0EECCAA" title="Participants">Participants</a></li><li data-auto="html_toc_list_item" class="link-medium html-toc-hd1"><a data-auto="ep_link" href="#xhp-41-3-597-ID0EDCCAA" id="hd1_xhp-41-3-597-ID0EDCCAA" title="Stimulus Selection">Stimulus Selection</a></li><li data-auto="html_toc_list_item" class="link-medium html-toc-hd1"><a data-auto="ep_link" href="#xhp-41-3-597-ID0ECCCAA" id="hd1_xhp-41-3-597-ID0ECCCAA" title="Gate Construction">Gate Construction</a></li><li data-auto="html_toc_list_item" class="link-medium html-toc-hd1"><a data-auto="ep_link" href="#xhp-41-3-597-ID0EBCCAA" id="hd1_xhp-41-3-597-ID0EBCCAA" title="Experimental Procedures">Experimental Procedures</a></li><li data-auto="html_toc_list_item" class="link-medium html-toc-hd1"><a data-auto="ep_link" href="#xhp-41-3-597-ID0EACCAA" id="hd1_xhp-41-3-597-ID0EACCAA" title="Statistical Analysis">Statistical Analysis</a></li><li class="link-medium" data-auto="html_toc_list_item"><a data-auto="ep_link" href="#xhp-41-3-597-ID0EBCAA" id="hd_xhp-41-3-597-ID0EBCAA" title="Results">Results</a></li><li data-auto="html_toc_list_item" class="link-medium html-toc-hd1"><a data-auto="ep_link" href="#xhp-41-3-597-ID0EDBCAA" id="hd1_xhp-41-3-597-ID0EDBCAA" title="Vocal Recognition Accuracy for Full Utterances (Gfull)">Vocal Recognition Accuracy for Full Utterances (Gfull)</a></li><li data-auto="html_toc_list_item" class="link-medium html-toc-hd1"><a data-auto="ep_link" href="#xhp-41-3-597-ID0ECBCAA" id="hd1_xhp-41-3-597-ID0ECBCAA" title="Time Course of Vocal Recognition: Accuracy at Early Gate Intervals (G200 to G700)">Time Course of Vocal Recognition: Accuracy at Early Gate Intervals (G200 to G700)</a></li><li data-auto="html_toc_list_item" class="link-medium html-toc-hd1"><a data-auto="ep_link" href="#xhp-41-3-597-ID0EBBCAA" id="hd1_xhp-41-3-597-ID0EBBCAA" title="Time Course of Vocal Recognition: EIPs">Time Course of Vocal Recognition: EIPs</a></li><li data-auto="html_toc_list_item" class="link-medium html-toc-hd1"><a data-auto="ep_link" href="#xhp-41-3-597-ID0EABCAA" id="hd1_xhp-41-3-597-ID0EABCAA" title="Effects of Language and Cultural Variables on Emotion Recognition for Hindi">Effects of Language and Cultural Variables on Emotion Recognition for Hindi</a></li><li class="link-medium" data-auto="html_toc_list_item"><a data-auto="ep_link" href="#xhp-41-3-597-ID0EACAA" id="hd_xhp-41-3-597-ID0EACAA" title="Discussion">Discussion</a></li><li data-auto="html_toc_list_item" class="link-medium html-toc-hd1"><a data-auto="ep_link" href="#xhp-41-3-597-ID0ECACAA" id="hd1_xhp-41-3-597-ID0ECACAA" title="Cultural Effects on the Buildup and Speed of Emotion Recognition">Cultural Effects on the Buildup and Speed of Emotion Recognition</a></li><li data-auto="html_toc_list_item" class="link-medium html-toc-hd1"><a data-auto="ep_link" href="#xhp-41-3-597-ID0EBACAA" id="hd1_xhp-41-3-597-ID0EBACAA" title="Effects of Cultural Exposure and Language Proficiency on the In-Group Advantage">Effects of Cultural Exposure and Language Proficiency on the In-Group Advantage</a></li><li data-auto="html_toc_list_item" class="link-medium html-toc-hd1"><a data-auto="ep_link" href="#xhp-41-3-597-ID0EAACAA" id="hd1_xhp-41-3-597-ID0EAACAA" title="Limitations and Conclusions">Limitations and Conclusions</a></li><li class="link-medium" data-auto="html_toc_list_item"><a data-auto="ep_link" href="#xhp-41-3-597-ID0EAA" id="hd_xhp-41-3-597-ID0EAA" title="Footnotes">Footnotes</a></li><li class="link-medium" data-auto="html_toc_list_item"><a data-auto="ep_link" href="#xhp-41-3-597-ID0E0NB0BBAA" id="hd_xhp-41-3-597-ID0E0NB0BBAA" title="References">References</a></li><li class="link-medium" data-auto="html_toc_list_item"><a data-auto="ep_link" href="#xhp-41-3-597-ID0EAA" id="hd_xhp-41-3-597-ID0EAA" title="APPENDICES">APPENDICES</a></li><li data-auto="html_toc_list_item" class="link-medium html-toc-hd1"><a data-auto="ep_link" href="#xhp-41-3-597-ID0EBABAA" id="hd1_xhp-41-3-597-ID0EBABAA" title="APPENDIX A: Scripts of (a) Hindi and (b) English  Pseudoutterances Presented in the Study">APPENDIX A: Scripts of (a) Hindi and (b) English  Pseudoutterances Presented in the Study</a></li><li data-auto="html_toc_list_item" class="link-medium html-toc-hd1"><a data-auto="ep_link" href="#xhp-41-3-597-ID0EAABAA" id="hd1_xhp-41-3-597-ID0EAABAA" title="APPENDIX B: Mean Number of Syllables per Gate Interval, by Emotion and Language Condition">APPENDIX B: Mean Number of Syllables per Gate Interval, by Emotion and Language Condition</a></li></ol></div><section id="TextToSpeech" class="full-text-content textToSpeechDataContainer" data-auto="text_to_speech" data-text-to-speech-cache-key="pdh_2015-11176-001" data-text-to-speech-title="More than accuracy: Nonverbal dialects modulate the time course of vocal emotion recognition across cultures." data-text-to-speech-author="Jiang, Xiaoming" data-text-to-speech-additional-filename="20150601"><span id="textToSpeechPlaceholder"> </span><div class="center" xmlns:Translation="urn:EBSCO-Translation"><strong>By: Xiaoming Jiang</strong><br /><em>School of Communication Sciences and Disorders and Center for Research on Brain, Language and Music, McGill University</em>;<br /><strong>Silke Paulmann</strong><br /><em>Department of Psychology and Centre for Brain Science, University of Essex</em><br /><strong>Jessica Robin</strong><br /><em>Department of Psychology, University of Toronto</em><br /><strong>Marc D. Pell</strong><br /><em>School of Communication Sciences and Disorders and Center for Research on Brain, Language and Music, McGill University</em></div><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><strong>Acknowledgement: </strong>This research was supported by an Insight Grant (435-2013-1027) awarded to Marc D. Pell from the Social Sciences and Humanities Research Council of Canada. We thank Catherine Knowles and Mary Giffen for their precious help in setting up and running the experiment, and with manuscript preparation.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Human emotions serve a variety of social functions, including the expression of discrete emotional meanings that can be recognized from facial or vocal expressions, gesture and whole body movements, and/or language (<a href="#c12">Dael, Goudbeek, &amp; Scherer, 2013</a>; <a href="#c33">Paulmann &amp; Pell, 2011</a>). The idea that emotion expression is biologically evolved and partly dictated by “universal” principles (e.g., <a href="#c15">Ekman &amp; Friesen, 1971</a>) is emphasized by cross-cultural studies showing that human facial and vocal displays of basic emotion (e.g., anger, joy, fear) are recognized at accuracy levels well exceeding chance, even by individuals from distant cultural backgrounds who have little contact with each other, when presented in a forced-choice response format (<a href="#c17">Elfenbein &amp; Ambady, 2002</a>; <a href="#c37">Pell, Paulmann, Dara, Alasseri, &amp; Kotz, 2009b</a>). These data underscore that recognition of emotions across cultural boundaries often succeeds at high accuracy levels through the application of “inference rules” that refer to how discrete emotions are commonly expressed (<a href="#c14">Ekman, 1972</a>; <a href="#c41">Scherer, Banse, &amp; Wallbott, 2001</a>).</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Emotions also seem to be marked by cultural variations in expression that can hamper recognition in the cross-cultural setting, with repeated evidence that they are decoded better when posed by a member of the decoder’s native culture than someone from a foreign culture (<a href="#c17">Elfenbein &amp; Ambady, 2002</a>). This “in-group advantage,” which may exist even when comparing distinct cultural groups that share the same native language, argues that culture-specific learning plays a role in how humans express and recognize emotions (<a href="#c16">Elfenbein, 2013</a>; <a href="#c27">Matsumoto, 1989</a>, <a href="#c28">2002</a>). Factors contributing to the in-group advantage in judgment accuracy include differences in how cultures consciously regulate and display (experienced) emotions for social purposes (<em>display</em> or <em>decoding</em> rules; <a href="#c14">Ekman, 1972</a>), motivational biases that may favor decoding of in-group over out-group expressions (<a href="#c26">Markham &amp; Wang, 1996</a>), and/or culture-specific beliefs (stereotypes) about how emotions are expressed and experienced by members of certain groups (<a href="#c24">Kirouac &amp; Hess, 1999</a>).</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Dialect theory argues that although emotional communication is culturally universal, it is characterized by <em>accents</em> that reflect the distinct cultural style for expressing nonverbal cues, akin to accents that differentiate native and non-native speakers in the phonetic realization of spoken language (<a href="#c16">Elfenbein, 2013</a>). According to this framework, individuals evaluate emotional expressions posed by a different culture based on their own learned expressive style; the impact of different accents on recognition will therefore vary according to the mismatch between the emotional cues expressed in one culture (e.g., culturally appropriate variations in facial or vocal cues) and the style expected by the decoder based on their own cultural rules of communication. These differences can be modulated by cultural exposure or familiarity, reducing or eliminating the in-group advantage (<a href="#c18">Elfenbein &amp; Ambady, 2003</a>). Dialect theory has attracted support from recent cross-cultural studies of both facial and vocal expression (<a href="#c13">Dailey et al., 2010</a>; <a href="#c34">Paulmann &amp; Uskul, 2014</a>; <a href="#c40">Sauter, Eisner, Ekman, &amp; Scott, 2010</a>), although empirical data, particularly outside the facial channel, are still much needed.</p><a id="xhp-41-3-597-ID0EFCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><span class="medium-bold" xmlns:Translation="urn:EBSCO-Translation"><a data-auto="ep_link" href="#toc" id="hd_toc_12" title="Cross-Cultural Studies of Emotional Prosody Recognition">Cross-Cultural Studies of Emotional Prosody Recognition</a></span><br xmlns:Translation="urn:EBSCO-Translation" /><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">The impact of culture on vocal expressions of emotion embedded in human speech (<em>emotional prosody</em>) merits special consideration in light of potential factors affecting the in-group advantage in judgment accuracy. Although vocal emotions can be expressed through relatively brief affective bursts (e.g., <a href="#c4">Belin, Fillion-Bilodeau, &amp; Gosselin, 2008</a>), speech-embedded emotional expressions affect ongoing energy, pitch, and durational features of the speech signal, revealing discrete emotional meanings over a certain time course that appears to differ according to which emotion is being expressed (<a href="#c35">Pell &amp; Kotz, 2011</a>). Cross-cultural studies of emotional speech confirm that listeners from different linguistic-cultural backgrounds decode vocal emotion expressions at far better than chance accuracy levels when listening to a foreign language, while exemplifying an in-group advantage for identifying vocal expressions in their native language (<a href="#c30">Min &amp; Schirmer, 2011</a>; <a href="#c34">Paulmann &amp; Uskul, 2014</a>; <a href="#c36">Pell, Monetta, Paulmann, &amp; Kotz, 2009a</a>; <a href="#c41">Scherer et al., 2001</a>). The extent to which the in-group advantage in vocal emotion recognition is modulated by language similarity or cultural exposure is still unclear in light of contradictory findings (<a href="#c36">Pell et al., 2009a</a>; <a href="#c41">Scherer et al., 2001</a>). A major shortcoming of this literature is the prevalence of the “unbalanced design”; most researchers have presented vocal expressions produced by a single cultural group to native and non-native listeners (<a href="#c3">Beier &amp; Zautra, 1972</a>; <a href="#c8">Bryant &amp; Barrett, 2008</a>; <a href="#c41">Scherer et al., 2001</a>; <a href="#c45">Van Bezooijen, Otto, &amp; Heenan, 1983</a>), or vocal expressions encoded by different cultural groups to listeners from a single culture (<a href="#c36">Pell et al., 2009a</a>; <a href="#c44">Thompson &amp; Balkwill, 2006</a>). These approaches prevent a clear test of how vocal emotions are interpreted across cultural boundaries (<a href="#c28">Matsumoto, 2002</a>). Moreover, there have been few concerted attempts to document what acoustic differences define cultural “dialects” in vocal emotion expression and their impact on recognition, nor how these are intertwined with linguistic-dialectal features that are known to affect non-native listeners (e.g., <a href="#c11">Cutler, Smits, &amp; Cooper, 2005</a>).</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">In a recent undertaking, <a href="#c34">Paulmann and Uskul (2014)</a> employed a crossed 2 × 2 design to compare recognition of seven vocal emotion expressions conveyed by semantically anomalous “pseudoutterances” produced in British English versus Chinese. Participants were monolingual White British or bilingual Chinese students studying in England who judged emotional meanings in each language. As expected, both groups demonstrated an in-group advantage for recognizing emotions in their native language; however, this advantage tended to be <em>smaller</em> in the Chinese group that was proficient in English and exposed to British culture (British participants were unfamiliar with Chinese). These data suggest that for the Chinese group, the in-group advantage in recognition accuracy was reduced by their cultural exposure to British people, their high proficiency in English, or possibly a combination of these factors (<a href="#c34">Paulmann &amp; Uskul, 2014</a>). These ideas were examined more closely in the current study, which looks at the relationship between vocal emotion recognition accuracy, cultural exposure, and language proficiency.</p><a id="xhp-41-3-597-ID0EECAA" xmlns:Translation="urn:EBSCO-Translation"> </a><span class="medium-bold" xmlns:Translation="urn:EBSCO-Translation"><a data-auto="ep_link" href="#toc" id="hd_toc_17" title="Is There an In-Group Advantage in Recognition Speed?">Is There an In-Group Advantage in Recognition Speed?</a></span><br xmlns:Translation="urn:EBSCO-Translation" /><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Timing differences could be another important factor that explains differences in cross-cultural emotion perception. There has been little attention to how the in-group advantage characterizes the <em>time course</em> of recognizing vocal emotion expressions; hypothetically, dialect differences that affect recognition accuracy should be detectable at specific time points as vocal emotions unfold across an utterance. Recent data for English affirm that recognition of vocal emotion expressions in a native language follows a distinct time course; basic emotions are first differentiated after 300 ms to 400 ms of acoustic input, with peak recognition of most emotions occurring within the 400 ms to 1,200 ms time range in forced-choice tasks (<a href="#c10">Cornew, Carver, &amp; Love, 2009</a>; <a href="#c35">Pell &amp; Kotz, 2011</a>; <a href="#c39">Rigoulot, Wassiliwizky, &amp; Pell, 2013</a>). Given parallel evidence that linguistic judgments such as comprehensibility are less efficient when listening to a foreign accent than one’s native dialect (<a href="#c31">Munro &amp; Derwing, 1995</a>), an in-group advantage in emotion recognition <em>speed</em> could well emerge, in a subtle manner, in conditions in which nonverbal accents do not have measureable effects on accuracy performance, or in combination with accuracy deficits. This hypothesis builds on a priming study by <a href="#c38">Pell and Skorup (2008)</a>, who showed that English listeners require <em>more</em> acoustic information to recognize vocal emotions posed by foreign (Arabic) versus native (English) speakers, although they ultimately achieved accurate recognition (i.e., priming) of emotions in both language conditions. As argued by <a href="#c38">Pell and Skorup (2008)</a>, procedures for inferring vocal emotions in speech based on universal principles are susceptible to delay when a foreign language is encountered (see also <a href="#c3">Beier &amp; Zautra, 1972</a>). This “noise” could be introduced by difficulties applying culture-specific models of how emotions are encoded to infer emotions from a foreign language (<a href="#c16">Elfenbein, 2013</a>), or possibly by interference from language-specific features of a foreign spoken language which delay emotion-related processes (<a href="#c29">Mesquita &amp; Frijda, 1992</a>; <a href="#c41">Scherer et al., 2001</a>). These data justify our current objective of testing how the in-group advantage in emotion recognition affects both accuracy and the time, or <em>amount</em> of acoustic information needed, to recognize emotions posed by out-group members in the vocal channel.</p><a id="xhp-41-3-597-ID0EDCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><span class="medium-bold" xmlns:Translation="urn:EBSCO-Translation"><a data-auto="ep_link" href="#toc" id="hd_toc_21" title="The Current Study">The Current Study</a></span><br xmlns:Translation="urn:EBSCO-Translation" /><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">It seems likely that a complex set of cultural and linguistic variables contributes to how well listeners understand emotions during cross-cultural speech communication; in addition to cultural exposure, <em>proficiency</em> in a language may be associated with the ability to accurately infer vocal emotions posed by out-group members (<a href="#c34">Paulmann &amp; Uskul, 2014</a>). Arguably, knowledge of a language could mitigate linguistic interference in both the accuracy <em>and</em> speed of emotional decoding (<a href="#c38">Pell &amp; Skorup, 2008</a>), modulating the in-group advantage in vocal recognition. In order to test these ideas and separate the effects of linguistic interference from cultural “dialects” in cross-cultural communication, we presented emotional pseudoutterances expressed in Canadian English and in Hindi to native listeners of each group in a fully factorial cross-cultural study that focused on both accuracy and speed of recognition. Listeners differed in their knowledge of the non-native language presented in each cultural context; in one condition, Hindi expressions were completely foreign to English participants, whereas in the other condition, English expressions could be processed as the second language of Hindi participants. This study builds on earlier work by <a href="#c37">Pell and colleagues (2009b)</a>, who presented English and Hindi pseudoutterances conveying basic emotions <em>only</em> to native listeners of each language, establishing a core set of perceptual and acoustic features for expressions in each language context. These stimuli provide a strong platform to test our hypotheses, while ensuring that the clarity of emotional signals in each language is tightly controlled based on the judgments of native listeners (<a href="#c28">Matsumoto, 2002</a>). <a href="#c37">Pell et al.’s (2009b)</a> data also furnish reasonable evidence that (Canadian) English and Hindi speakers have distinct “accents” when expressing vocal emotions (<a href="#c16">Elfenbein, 2013</a>), although it is unclear whether these accents represent true “dialects” that impede cross-cultural recognition of emotions in terms of either accuracy or speed.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">To understand <em>when</em> the in-group advantage occurs, we presented full utterances conveying emotions in English and Hindi as done in previous studies, as well as the same utterances gated to different stimulus durations that limit the amount of temporal and acoustic information listeners could use to infer vocal emotions in native and non-native speech (<a href="#c35">Pell &amp; Kotz, 2011</a>). Adapting the auditory gating paradigm (<a href="#c20">Grosjean, 1980</a>) to investigate <em>cross-cultural</em> vocal emotion recognition will shed new light on whether there is an in-group advantage in both accuracy and speed of processing, and whether this evolves in similar ways for English and Hindi speakers in each language condition, while considering the English proficiency of Hindi speakers on our performance measures. We predicted that each listener group would demonstrate an in-group accuracy advantage when judging emotions in full utterances for their native language, despite the fact that participants in the Hindi group are also highly proficient in English (<a href="#c18">Elfenbein &amp; Ambady, 2003</a>; <a href="#c34">Paulmann &amp; Uskul, 2014</a>). Moreover, we expected that acoustic differences in the form of vocal emotion expressions in English and Hindi (<a href="#c37">Pell et al., 2009b</a>) would promote accurate recognition at an <em>earlier time point</em> in each group’s native language when compared with their non-native language—that is, correct inferences about emotions would require more temporal or acoustic information in utterances that are inconsistent with culture-specific modes of display. Given that Hindi participants were highly proficient in English, we reasoned that the in-group advantage for Hindi listeners would be predicted in large part by cultural factors and not interference in basic-level linguistic (e.g., phonological) decoding of English stimuli (<a href="#c5">Best &amp; Tyler, 2007</a>; <a href="#c11">Cutler et al., 2005</a>); for example, increased language proficiency could provide opportunities for learning about cultural dialects in emotion expression that reduce the in-group advantage (<a href="#c34">Paulmann &amp; Uskul, 2014</a>). Finally, we anticipated that the in-group advantage in each language would not be uniform for all emotional expression types, although <em>relative</em> patterns of emotion recognition should demonstrate similarities for native and non-native listeners in each language condition, owing to the application of shared inference rules for interpreting emotions from acoustic cues in the absence of specific knowledge about culturally dictated forms of emotion expression.</p><a id="xhp-41-3-597-ID0ECCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><span class="medium-bold" xmlns:Translation="urn:EBSCO-Translation"><a data-auto="ep_link" href="#toc" id="hd_toc_26" title="Method">Method</a></span><br xmlns:Translation="urn:EBSCO-Translation" /><a id="xhp-41-3-597-ID0EECCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Participants</h4><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Seventy-seven young adults consented to participate and were compensated $20 CAD upon completion of the study. A group of 42 participants (21 females and 21 males) were native speakers of Canadian English, with a mean age of 23.2 years (<em>SD</em> = 7.6 years) and averaging 15.8 years of formal education (<em>SD</em> = 1.8 years). All English participants were born and had spent their whole lives in Canada or, in some cases, the United States, and spoke English in the home to both parents. The vast majority of participants within the English group (35/42) had knowledge or proficiency in languages other than English (e.g., French, Dutch, Spanish, Arabic), whereas a proportion (7/42) were monolingual. None of the English participants had any formal knowledge of the Hindi language, nor had any been extensively exposed to Hindi culture (e.g., through close personal relationships, extensive travel).</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">A second group of 35 participants (15 females and 20 males) were native speakers of Hindi, with a mean age 23.1 years (<em>SD</em> = 7.6 years) and a mean education of 17.7 years (<em>SD</em> = 2.8 years). On average, Hindi participants were significantly more educated than English participants in the study, <em>t</em> = −3.51, <em>p</em> = .001, although there were no significant group differences in age (<em>p</em> = .95). The Hindi participants were born and raised in different parts of India and all spoke Hindi in the home to both parents while growing up; each had moved as a young adult to Montreal, Canada, to study or work (largely in an English-speaking environment). The mean age of Hindi participants upon arrival in Canada was 21.5 years of age (range = 17 to 32 years). The majority of Hindi participants (21/35) had been in Canada for less than one year (<em>M</em> = 1.8 years, range = 0.5 to 7 years). The vast majority were early Hindi/English bilinguals, or in some cases, late English learners, many of whom knew other languages (e.g., French, Bengali, Punjabi, Marathi).</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">A detailed language questionnaire was administered to each participant in the Hindi group to establish their self-rated proficiency and usage of Hindi and English (separate 10-point scales were administered for reading, writing, listening, speaking, ease and frequency of code-switching; <a href="#c42">Schwanenflugel &amp; Rey, 1986</a>; see <a href="#tbl1">Table 1</a>). In total, 25/35 Hindi participants were exposed to both Hindi and English from birth, 19 spoke Hindi and English to family members, and 31 reported both Hindi and English (or English alone) as the language of instruction. Although Hindi participants were highly skilled in both Hindi and English, they rated their reading and writing abilities in Hindi significantly lower than in English (<em>t</em>s = −3.00 for both abilities, <em>ps</em> &lt; .01), although listening and speaking abilities in the two languages were comparable. They also reported a high frequency and low difficulty to switch between the two languages in their daily lives. A principal component analysis performed on the language assessment ratings identified two component factors accounting for 72.5% of the total variance in the language proficiency measures: The first component (“oral proficiency in English”) was jointly related to English listening (<em>r</em> = .91) and English speaking (<em>r</em> = .90); the second component (“code switching”) was jointly associated with the frequency (<em>r</em> = .79) and difficulty (<em>r</em> = .78) to switch between Hindi and English in speech communication. Based on this analysis, composite scores were created for later analysis of the Hindi group by averaging the two oral proficiency measures (listening/speaking) separately in English and in Hindi, which could be used as predictor variables in statistical analyses with other factors that might impact on emotion recognition (e.g., age of acquisition of English, years of exposure to Canadian culture).<br /><br /><a id="tbl1"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/bc394befd6e4c4062224b68204324d99/571c5f1d/pdh/xhp/xhp-41-3-597-tbl1a.gif" alt="xhp-41-3-597-tbl1a.gif" title="Linguistic and Cultural Features of Individuals in the Hindi Group (n = 35)" /><em>Linguistic and Cultural Features of Individuals in the Hindi Group (n = 35)</em></span></p><a id="xhp-41-3-597-ID0EDCCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Stimulus Selection</h4><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Emotionally inflected pseudoutterances were selected from the Canadian English (<em>n</em> = 80; seven to 10 syllables in duration) and Hindi (<em>n</em> = 80; six to 10 syllables in duration) recording inventory of <a href="#c37">Pell et al. (2009b)</a>, as shown in <a href="#A">Appendix A</a>. In each language condition of that study, pseudosentences were constructed by a native speaker of the target language by replacing content words with semantically meaningless strings; these were then produced by four native speakers of each language (lay actors) to vocally express seven different emotions in an expressive style of each culture, embedded in speech that highly resembles the speaker’s native language. Each set of recordings was then independently verified by a group of native listeners of the target language to establish the emotional validity of each item (forced-choice emotion recognition task) followed by acoustic analyses of emotional expressions that were associated with high target accuracy rates (<a href="#c37">Pell et al., 2009b</a>). These data allowed pseudoutterances that reliably communicate specific emotional meanings to English or Hindi listeners, in their own language, to be carefully selected for cross-cultural presentation.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">In this study, 16 exemplars conveying each of five different emotional meanings (fear, anger, happiness, sadness, neutral), produced in equal numbers by the two male speakers of each language (eight sentence tokens per speaker), were selected for cross-cultural presentation (80 utterances × 2 languages = 160 total utterances). Stimuli were restricted to male speakers to ensure that all items demonstrated comparable, high consensus rates across speakers, emotions, and languages prior to entering them in the cross-cultural study; for the same reason, we excluded stimuli conveying <em>surprise</em> and <em>disgust</em> from the database to focus on the four best recognized emotions and neutral utterances. In each language condition, there were equivalent exemplars of each emotion produced by the two speakers, and linguistic features of the stimuli were identical across emotion conditions, although there was some variation in what actual sentences were selected for each speaker (see <a href="#A">Appendix A</a>). Selected items were always identified by native listeners at rates exceeding 3 times chance accuracy in the validation study (&gt;55%); mean overall emotion recognition rates in both English and Hindi were actually much higher, although patterns varied predictably by emotion type within each set of language materials (see <a href="#tbl2">Table 2</a>). Identification rates for selected emotions were comparable in range for the two speakers in each language condition for English (SpeakerA = 63% to 94%, SpeakerB = 69% to 94%) and for Hindi (SpeakerA = 69% to 76%, SpeakerB = 69% to 78%). Attempts were also made to select stimuli of similar perceived intensity between language conditions for each emotion, although mean intensity ratings vary predictably by emotion type (see <a href="#tbl2">Table 2</a>). The average number of syllables was larger overall in the selected English versus Hindi pseudoutterances (7.4 vs. 6.8 syllables, respectively, <em>b</em> = 1.90, <em>SE</em> = 0.53, <em>t</em> = 3.59, <em>p</em> &lt; .001). Moreover, English utterances were substantially longer on average than Hindi utterances once recorded with different emotional inflections (mean duration = 1.79 vs. 1.29 s, respectively). In the validation study, items with elisions, dysfluencies, or interjected syllables were systematically excluded from the recording database (<a href="#c37">Pell et al., 2009b</a>), meaning that temporal differences can be traced to natural variations related to the expression of emotion in each cultural setting.<br /><br /><a id="tbl2"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/11829dbabf5833f326e5a2e36ac41ce4/571c5f1d/pdh/xhp/xhp-41-3-597-tbl2a.gif" alt="xhp-41-3-597-tbl2a.gif" title="Means (and Standard Deviations) of Normalized Frequency and Amplitude Measures and Speech Rate for the Selected English and Hindi Utterances of Each Emotion" /><em>Means (and Standard Deviations) of Normalized Frequency and Amplitude Measures and Speech Rate for the Selected English and Hindi Utterances of Each Emotion</em></span></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><a href="#tbl2">Table 2</a> summarizes major acoustic features of the selected stimuli in each language in reference to fundamental frequency, mean, and variation (f0Mean and f0Range, in hertz); mean and variation in amplitude (in dB); and speaking rate (syllables per second). Linear mixed models, treating emotion as a fixed factor and sentence token as a random factor, revealed significant differences between emotions for each of the five acoustic parameters in both language conditions. Multiple models treating different emotions as baseline were built in order to estimate the differences between each two emotions (<a href="#c9">Clopper, 2013</a>). These relationships are summarized in <a href="#tbl3">Table 3</a> to highlight the <em>relative</em> pattern of distinctions along each acoustic dimension in the form of ranks, in which emotions assigned a similar rank did not significantly differ in the dimension of interest. Patterns in <a href="#tbl3">Table 3</a> imply a number of similarities in how acoustic cues were used to encode emotional distinctions by speakers of English versus Hindi (<a href="#c37">Pell et al., 2009b</a>), as well as certain differences. For example, English expressions of anger were associated with the largest degree of f0 variation (f0Range), whereas this was associated with expressions of happiness in Hindi. These physical differences in our stimuli may well reflect variations in nonverbal “accents” for each speaker group.<br /><br /><a id="tbl3"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/d3cfebc61f08390f8069065c27063ee6/571c5f1d/pdh/xhp/xhp-41-3-597-tbl3a.gif" alt="xhp-41-3-597-tbl3a.gif" title="Relative Ranking of the Five Emotion Types by Acoustic Parameter, Across Both Languages" /><em>Relative Ranking of the Five Emotion Types by Acoustic Parameter, Across Both Languages</em></span></p><a id="xhp-41-3-597-ID0ECCCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Gate Construction</h4><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Each of the 160 utterances (80 English, 80 Hindi) was edited in Praat (<a href="#c6">Boersma &amp; Weenink, 2005</a>) to create five new “gated” versions of the original stimulus that varied only in duration from utterance onset. The gate intervals were 200 ms, 400 ms, 500 ms, 600 ms, 700 ms, and the full utterance (henceforth G200, G400, G500, G600, G700, Gfull). Stimuli were segmented based on time, rather than syllable boundaries (<a href="#c35">Pell &amp; Kotz, 2011</a>), to precisely control the duration of a unit from which listeners could glean emotional information at each gate across language conditions. This decision was necessary in part because a syllable in English tended to be longer overall (<em>M</em> = 0.25 s) than a syllable in Hindi (<em>M</em> = 0.19 s). <a href="#B">Appendix B</a> provides the mean syllables per gate in each emotion and language condition for comparative purposes. The specific duration of gates for the study was informed by data on English showing that most vocal emotion expressions are recognized in the 300 ms to 800 ms time window using the gating method (<a href="#c10">Cornew et al., 2009</a>; <a href="#c35">Pell &amp; Kotz, 2011</a>; <a href="#c39">Rigoulot et al., 2013</a>). The chosen gates thus provide several points within this temporal region of interest to evaluate factors affecting the time course of emotion recognition between cultures. Since the “sixth gate” of each exemplar was the unaltered pseudoutterance (Gfull), we could investigate both the evolution of emotion recognition at different gates and compare with previous cross-cultural studies based on data of the full utterance. After constructing 6 gates × 16 exemplars × 5 emotions × 2 languages, a total of 960 utterances were created for the experiment.</p><a id="xhp-41-3-597-ID0EBCCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Experimental Procedures</h4><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Participants were tested in a sound-attenuated room. Before the experiment, participants were instructed that they would hear utterances composed of nonsense words that may or may not sound like their native language; they were told to pay attention to the emotions expressed in the voice rather than the meanings of the sentence. Individual utterances were presented over volume adjustable headphones and responses were recorded by Superlab 4.0 (Cedrus, San Pedro, CA). Each trial began with a visual fixation marker presented on a computer monitor; a 1,500-ms blank screen; and the auditory stimulus presented for the assigned gate duration. Stimuli were blocked for presentation order according to gate duration, starting with the shortest gate (G200) and increasing incrementally to the full utterance (Gfull); this consecutive block presentation mode was necessary to avoid potential bias of hearing longer stimuli first (<a href="#c20">Grosjean, 1980</a>). Within each block, stimuli of the same duration were presented in a randomized order intermingling the four basic emotions and neutral and the two languages.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">After each stimulus, participants made two forced-choice decisions. First, they made an emotion identification judgment by clicking on one of five emotion labels (“anger,” “fear,” “happiness,” “sadness,” “neutral”) displayed on a computer screen that best corresponded with the speaker’s emotion. The position of emotion labels was randomized and varied across participants in each group. Immediately after the emotion decision, a confidence scale appeared and the participant rated how certain they were about their previous selection on a scale of 1 (<em>least sure</em>) to 7 (<em>most sure</em>). Before each block, four practice trials were presented in order to familiarize participants with the procedures, speaker voices, and the duration of stimuli in each condition. Short breaks were given after every 40 trials and between blocks.</p><a id="xhp-41-3-597-ID0EACCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Statistical Analysis</h4><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Statistical analyses considered both accuracy and time-based measures of emotion recognition. Accuracy scores were calculated for each participant as the total number of correct categorizations for each emotion at each gate interval; Hu scores (<a href="#c46">Wagner, 1993</a>) were then calculated to correct for individual biases in the use of particular emotion response categories and arcsine-transformed (<a href="#c35">Pell &amp; Kotz, 2011</a>). This yielded an unbiased recognition score for each type of emotional expression at each gate interval, per language for each group. To estimate the time course of emotion recognition, we compared how accuracy rates evolved with increasing gate durations of a stimulus, as well as calculated a single emotion identification point (EIP) for each item when judged by each of the 77 participants (<a href="#c35">Pell &amp; Kotz, 2011</a>). The EIP represents the specific gate duration (in milliseconds) among the six variants of each item for which accurate recognition of the target emotion occurred, without subsequent changes in accuracy at longer gate intervals of the same stimulus for that participant. To allow for some error, the EIP was also assigned to the shortest gate in cases for which accurate responses were observed for at least two successive gate intervals, followed by no more than one incorrect response at all longer gate intervals of the stimulus. Whereas Hu scores are a proportional value calculated for each participant across trials in each gate, emotion, and language condition, EIP’s are calculated on a trial-by-trial basis and each utterance received a single EIP value for each participant across gate intervals. In contrast to accuracy, the EIP reveals <em>how much information</em> is required for a listener to recognize the target emotion in a stable manner (<a href="#c35">Pell &amp; Kotz, 2011</a>).</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Statistical treatment of the data was accomplished using linear mixed effects models (LMEMs) within the <em>lme4</em> package in R (Version 2.15.3; <a href="http://cran.r-project.org" target="_blank">http://cran.r-project.org</a>). Separate models were created for each dependent measure of accuracy (Hu scores) and time (EIP scores). The main models included some combination of the following fixed effects: listener group (native, non-native; baseline = native), language type (English, Hindi; baseline = English), gate interval (G200, G400, G500, G600, G700), and emotion type (anger, fear, happiness, sadness, neutral), according to the hypothesis under scrutiny. Participant and item were random factors. Because of differences in the educational attainment of English and Hindi participants, education (in years) was entered as a fixed covariate to statistically control for this factor in all models (<a href="#c43">Snijders, 2005</a>). The <em>p</em> values of fixed effects were evaluated against the Markov Chain Monte Carlo (MCMC) sampling tests (<em>n</em> = 10,000; <a href="#c1">Baayen, Davidson, &amp; Bates, 2008</a>).</p><a id="xhp-41-3-597-ID0EBCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><span class="medium-bold" xmlns:Translation="urn:EBSCO-Translation"><a data-auto="ep_link" href="#toc" id="hd_toc_50" title="Results">Results</a></span><br xmlns:Translation="urn:EBSCO-Translation" /><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><a href="#tbl4">Table 4</a> shows the Hu score for each emotion type at each of the six gate intervals when English and Hindi listeners judged emotional expressions in each language. To compare with existing data on how vocal emotions are recognized from intact utterances, initial analyses concentrated on responses in the Gfull condition for English and Hindi. At a second step, the incremental build-up of emotion recognition was examined through models that compared accuracy scores at earlier time intervals (G200 to G700) and which compared the mean EIPs by language and native listener status. A final set of analyses focused on data for Hindi listeners to test whether performance in this group could be linked to linguistic or cultural variables such as language proficiency and cross-cultural exposure.<br /><br /><a id="tbl4"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/b058bed5f94305cd124891c13d72320c/571c5f1d/pdh/xhp/xhp-41-3-597-tbl4a.gif" alt="xhp-41-3-597-tbl4a.gif" title="Mean Hu Scores at Each Gate Interval of the English and Hindi Utterances for the Two Groups of Listeners" /><em>Mean Hu Scores at Each Gate Interval of the English and Hindi Utterances for the Two Groups of Listeners</em></span></p><a id="xhp-41-3-597-ID0EDBCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Vocal Recognition Accuracy for Full Utterances (Gfull)</h4><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">The effect of native listener status on vocal emotion recognition (dependent measure = Hu score), independent of emotion type, was assessed in an LMEM with fixed effects of listener group (baseline = English listeners) and language type (baseline = English utterances). Of main theoretical importance, a significant interaction of Listener Group × Language Type provides evidence that listeners were always more accurate to recognize emotions in their <em>native</em> language when compared with a non-native (foreign or second) language (<em>b</em> = −0.30, <em>SE</em> = 0.03, <em>t</em> = 10.04, <em>p</em>MCMC = 0.0001). These patterns are illustrated in <a href="#fig1">Figure 1</a>. Inspection of <a href="#tbl4">Table 4</a> also reveals that accuracy performance varied as a function of both listener group and language type, and that vocal emotion expressions in each language were associated with different patterns of recognition accuracy (although these patterns demonstrated similarities in each language when judged by English vs. Hindi listeners). Overall, English listeners identified emotions more accurately than Hindi listeners irrespective of language type (listener group effect, <em>b</em> = −0.19, <em>SE</em> = 0.04, <em>t</em> = −4.59, <em>p</em>MCMC = 0.0001); furthermore, we noted that vocal expressions in English were generally associated with higher accuracy rates than emotions expressed in Hindi for both listener groups (language effect, <em>b</em> = −0.30, <em>SE</em> = 0.02, <em>t</em> = −14.94, <em>p</em>MCMC = 0.0001).<br /><br /><a id="fig1"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/273830f8a1342c891e42bb49b6559b9f/571c5f1d/pdh/xhp/xhp-41-3-597-fig1a.gif" alt="xhp-41-3-597-fig1a.gif" title="Figure 1. Mean unbiased accuracy of all listeners (combining 42 English and 35 Hindi) to recognize English and Hindi full utterances conveying each emotion. Mean Hu scores in the full gate interval for the two listener groups in the two language types. See the online article for the color version of this figure." /><em>Figure 1. Mean unbiased accuracy of all listeners (combining 42 English and 35 Hindi) to recognize English and Hindi full utterances conveying each emotion. Mean Hu scores in the full gate interval for the two listener groups in the two language types. See the online article for the color version of this figure.</em></span></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">To understand whether native listener status affected how different emotional expressions were recognized in each language context, separate LMEMs were built on the Gfull accuracy scores in the English and Hindi language conditions including listener group (baseline = English) and emotion type (baseline = neutral) as fixed effects in each model. When English utterances were presented, there was a main effect of listener group (<em>b</em> = −0.18, <em>SE</em> = 0.06, <em>t</em> = −3.09, <em>p</em>MCMC = 0.0018), explained by the in-group advantage for native English (vs. Hindi) listeners in the recognition of emotions expressed in English. The relative difference in accuracy across emotions is shown in <a href="#tbl4">Table 4</a>: Accuracy was lowest for happiness, followed by neutral, sadness, fear, and anger. Multiple LMEMs were performed with each level of emotion type as baseline. The differences between emotion types with adjacent ranking were all significant except between neutral and sadness (<em>b</em>s &gt; 0.08, <em>SE</em>s = 0.04, <em>t</em>s &gt; 2.82, <em>p</em>MCMCs&lt; 0.05). There was no significant listener group by emotion interaction, implying that the ability to recognize vocal emotions in English was qualitatively similar when judged by English and Hindi listeners.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">When Hindi utterances were presented, native Hindi listeners were significantly more accurate than English listeners overall (<em>b</em> = 0.16, <em>SE</em> = 0.05, <em>t</em> = 3.18, <em>p</em>MCMC = 0.0016; baseline = neutral). Recognition accuracy was lowest for neutral, followed by sadness, anger, fear, and happiness (which was highest in Hindi). The main effect of emotion was significant when sadness, anger, or fear was compared with neutral emotion, respectively (<em>b</em>s &gt; 0.13, <em>SE</em>s = 0.04, <em>t</em>s &gt; 3.51, <em>p</em>MCMCs &lt; 0.003), and when happiness was compared with all emotions (<em>b</em>s &gt; 0.11, <em>SE</em>s = 0.04, <em>t</em>s &gt; 3.13, <em>p</em>MCMCs &lt; 0.006). Hindi expressions of sadness, anger, and fear were recognized with similar accuracy. Again, no significant interaction was found between listener group and emotion in the Hindi language condition.</p><a id="xhp-41-3-597-ID0ECBCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Time Course of Vocal Recognition: Accuracy at Early Gate Intervals (G200 to G700)</h4><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">The gating paradigm allows one to sample recognition within the time window in which vocal emotional meanings are first registered in memory during online speech processing (<a href="#c35">Pell &amp; Kotz, 2011</a>). Data representing the emotion recognition accuracy of listeners at five early gate intervals of English and Hindi utterances (G200, G400, G500, G600, G700) are furnished in <a href="#tbl4">Table 4</a> and plotted in <a href="#fig2">Figure 2</a>, separately for each listener group, language, and emotion type.<br /><br /><a id="fig2"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/c514f5ba4bf18a947651c13a7e39b1e1/571c5f1d/pdh/xhp/xhp-41-3-597-fig2a.gif" alt="xhp-41-3-597-fig2a.gif" title="Figure 2. Group differences in the mean unbiased accuracy to recognize English and Hindi utterances conveying each emotion as a function of gate duration. See the online article for the color version of this figure." /><em>Figure 2. Group differences in the mean unbiased accuracy to recognize English and Hindi utterances conveying each emotion as a function of gate duration. See the online article for the color version of this figure.</em></span></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">As expected, the mean recognition accuracy of both English and Hindi listeners tended to increase as a function of hearing longer gate intervals. Even at G200, unbiased accuracy rates were significantly above chance for all emotions in all language or listener group conditions, with the exception of <em>happiness</em> in English (see <a href="#fig2">Figure 2</a>). Increases in accuracy tended to be most pronounced in the 200 ms to 600 ms time window (between G200 and G500/G600) and to be more flat between later gate intervals, although this varied by emotion and language type; in English utterances, the largest increases were observed for <em>anger</em> between G200 and G700, whereas in Hindi utterances, accuracy for <em>happiness</em> increased the most between G200 and G700 (in a similar manner for both listener groups). The impact of increasing gate intervals on recognition of <em>happiness</em> in English, and <em>neutral</em> in Hindi utterances, in the early course of these expressions was notably less when compared with other emotions (see <a href="#fig2">Figure 2</a>). The steep rise from G700 to Gfull in English versus Hindi is likely explained by the fact that utterances tended to be longer overall in English.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Models performed using G200, G400, G500, and G600 as baselines, and treating listener group, language type, and gate interval as fixed effects, revealed a marginally significant two-way interaction of listener group and language type (<em>b</em> = 0.08, <em>SE</em> = 0.03, <em>t</em> = 1.85, <em>p</em>MCMC = 0.07). A significant three-way interaction of listener group, language type, and gate interval emerged when accuracy at G200 was compared with accuracy at G500 to G700 (<em>b</em>s &gt; 0.09, <em>SE</em>s = 0.04, <em>t</em>s &gt; 2.18, <em>p</em>MCMCs &lt; 0.03). In the time interval between G200 and G500 (and higher), vocal recognition accuracy improved significantly more for native listeners of each language when compared with non-native listeners (see <a href="#fig2">Figure 2</a>). As expected, recognition of emotions increased significantly between each adjacent gate interval in the 200 ms to 600 ms poststimulus onset time window, for each listener group in each language condition (<em>b</em>s &gt; 0.02, <em>SE</em>s = 0.02, <em>t</em>s &gt; 2.16; <em>p</em>MCMCs &lt; 0.03).</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">To further characterize how emotion recognition evolved for native and non-native listeners as a function of adding consecutive gate intervals, separate LMEMs were built for each language type with emotion type collapsed, treating listener group and gate interval as fixed effects. For English utterances, an interaction of Gate × Listener Group was observed when comparing the accuracy score at G200 with scores at G500 and G600 (<em>b</em>s &lt; −0.05, <em>SE</em>s = 0.03, <em>t</em>s &lt; −1.84, <em>p</em>MCMCs &lt; 0.04; baseline = 200 ms). <em>Native</em> English listeners benefited to a greater extent than non-native Hindi listeners from information in this time window, resulting in significantly higher emotion recognition scores. Models comparing listener group at each individual gate interval revealed that even when only 200 ms of English utterances were presented (G200), native English listeners were marginally better than Hindi listeners at recognizing vocal emotions (<em>b</em> = −0.04, <em>SE</em> = 0.02, <em>t</em> = −1.93, <em>p</em>MCMC = 0.0598). The native advantage was significant by G400 (<em>b</em> = −0.06, <em>SE</em> = 0.02, <em>t</em> = −2.71, <em>p</em>MCMC = 0.0094), and at all subsequent gate intervals (see <a href="#fig2">Figure 2</a>). For Hindi expressions, a significant Gate × Listener Group interaction emerged in the time window of G200 to G500 and G200 to G700 (<em>b</em>s &gt; 0.05, <em>SE</em>s = 0.02, <em>t</em>s &gt; 2.13, <em>p</em>MCMCs &lt; 0.04); the accuracy of native Hindi listeners increased to a greater extent in this time period when compared with non-native English listeners. In addition, results demonstrated that native Hindi listeners were significantly better than (foreign) English listeners at recognizing emotions in Hindi beginning at G500 (<em>b</em> = 0.06, <em>SE</em> = 0.03, <em>t</em> = 2.47, <em>p</em>MCMC = 0.0034), and at all consecutive gate intervals.<a id="b-fn1"> </a><sup><a href="#fn1" /></sup></p><a id="xhp-41-3-597-ID0EBBCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Time Course of Vocal Recognition: EIPs</h4><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">The EIP of each item was calculated for each participant to quantify the rate (speed) of emotion recognition. Mean EIPs were calculated from a maximum of 672 observations per emotion when judged by English listeners (42 participants × 16 items) and 560 observations per emotion when judged by Hindi listeners (35 participants × 16 items), with an average of 493 tokens contributing to the mean EIP in each emotion condition across languages. <a href="#tbl5">Table 5</a> summarizes the proportion of EIPs occurring at each of the six gate intervals by emotion type, the proportion of errors in each emotion category, and the mean EIPs for each emotion expressed in time, by listener group and language context.<br /><br /><a id="tbl5"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/5e2ba901aa717962b087734672bd379c/571c5f1d/pdh/xhp/xhp-41-3-597-tbl5a.gif" alt="xhp-41-3-597-tbl5a.gif" title="Proportion of Emotion Identification Points (EIPs) Observed at Each Gate Duration Interval, and Proportion of Errors for Each Emotion When Calculating EIP, for English and Hindi Listeners in Each Language Condition" /><em>Proportion of Emotion Identification Points (EIPs) Observed at Each Gate Duration Interval, and Proportion of Errors for Each Emotion When Calculating EIP, for English and Hindi Listeners in Each Language Condition</em></span></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Qualitative inspection of recognition times across emotions within the 450 ms to 750 ms time window (see <a href="#fig3">Figure 3</a>) highlights that the location of EIPs within an utterance varied by emotion type in each language; in addition, the speed of vocal emotion recognition appeared to differ according to native status of listeners in each language (see <a href="#tbl5">Table 5</a>). The broad impact of listener group on the time course of vocal emotion recognition (EIP as dependent variable) was first explored through a model with fixed effects of language type and listener group (baseline = English group), collapsing across emotions. Gfull duration was included as a covariate variable in the model to eliminate any potential effect of overall sentence duration on the EIP. Results show that emotions were recognized at a significantly earlier time point in utterances processed by native versus non-native listeners of each target language (interaction of Listener Group × Language Type, <em>b</em> = −117.51, <em>SE</em> = 18.33, <em>t</em> = −6.41, <em>p</em>MCMC = 0.0001). This effect occurred irrespective of whether non-native listeners were processing a completely foreign language or their second language (see <a href="#fig3">Figure 3</a>).<a id="b-fn2"> </a><sup><a href="#fn2" /></sup><br /><br /><a id="fig3"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/0d7f58b8a894d8641bb6e573c740e0f7/571c5f1d/pdh/xhp/xhp-41-3-597-fig3a.gif" alt="xhp-41-3-597-fig3a.gif" title="Figure 3. Differences between native and non-native group in the mean identification time (ms) for English and Hindi utterances conveying each emotion. Error bars refer to the standard error around the mean. See the online article for the color version of this figure." /><em>Figure 3. Differences between native and non-native group in the mean identification time (ms) for English and Hindi utterances conveying each emotion. Error bars refer to the standard error around the mean. See the online article for the color version of this figure.</em></span></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">To understand the impact of native listener status on the time needed to recognize discrete emotions in the voice, we created separate models for each language type to examine the interaction of listener group (native, non-native) and emotion type (anger, fear, happiness, sadness, neutral) on mean EIP times. For English utterances, there was a significant in-group advantage (i.e., shorter recognition times) when native English speakers processed expressions of anger, fear and sadness, but not happiness or neutral (see <a href="#fig3">Figure 3</a>). The mean EIP differed significantly for all five emotional expressions in English overall; listeners required the least amount of vocal cues to recognize neutral, followed by anger, sadness, fear, and happiness. The difference between emotion types with adjacent ranks was significant for all models (<em>b</em>s &gt; 57.34, <em>SE</em>s &lt; 32.34, <em>t</em>s &gt; 1.97, <em>p</em>MCMCs &lt; 0.05; baseline for listener group = English group), except for sadness and fear. The relative manner in which basic emotions unfolded and were detected in English utterances was similar for both native and non-native (second language, Hindi) listeners, with the exception that English listeners showed an advantage to recognize neutral more quickly than anger, whereas Hindi listeners showed the reverse pattern for these two emotions (Listener Group × Emotion interaction, <em>b</em> = −169.61, <em>SE</em> = 43.85, <em>t</em> = −3.87, <em>p</em>MCMC = 0.0001).</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">For Hindi vocal expressions, native listeners had a significant speed advantage for processing anger, happiness, and neutral expressions, but not fear or sadness. Overall, EIPs in Hindi were shortest for neutral expressions, which took less time to identify than expressions of anger and fear, followed by sadness and happiness. The differences between emotion types with adjacent rankings were significant (<em>b</em>s &gt; 52.86, <em>SE</em>s &lt; 27.40, <em>t</em>s &gt; 1.99, <em>p</em>MCMCs &lt; 0.05), except between anger and fear and between sadness and happiness. Again, emotion-specific patterns in the temporal unfolding of vocal expressions in Hindi were highly similar for native and non-native listeners, although there was a significant Listener Group × Emotion interaction: In contrast to English listeners, native Hindi listeners showed greater differentiation in recognition times for anger versus fear (<em>b</em> = 80.10, <em>SE</em> = 40.19, <em>t</em> = 1.99, <em>p</em>MCMC = 0.0454). These patterns are also illustrated in <a href="#fig3">Figure 3</a>.</p><a id="xhp-41-3-597-ID0EABCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Effects of Language and Cultural Variables on Emotion Recognition for Hindi</h4><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Given evidence that Hindi listeners were less accurate <em>and</em> slower to recognize emotions expressed in English—a language they were highly proficient in—LMEMs were built to evaluate the relationship between cultural/language-related predictors and (a) accuracy (mean Hu scores for Gfull) and (b) time course (mean proportional EIP) measures of vocal emotion processing, only for the Hindi listeners. Predictors were age of acquisition of English, oral proficiency in English, oral proficiency in Hindi, code-switching abilities, and years of exposure to Canadian culture. The ability to accurately recognize emotions from full English utterances was positively predicted by the oral proficiency of Hindi participants in English (<em>b</em> = 0.12, <em>SE</em> = 0.02, <em>t</em> = 5.58, <em>p</em>MCMC = 0.000005). Hindi participants with greater oral proficiency in English exhibited superior emotion recognition skills in English. Moreover, the accuracy measure was marginally predicted by code-switching abilities (<em>b</em> = −0.03, <em>SE</em> = 0.01, <em>t</em> = −1.75, <em>p</em>MCMC = 0.0630), suggesting that Hindi individuals with enhanced ability to switch from Hindi to English were <em>less</em> accurate in recognizing emotions expressed in English. The speed of emotion recognition for Hindi participants was negatively predicted by their oral proficiency in English (<em>b</em> = −135.96, <em>SE</em> = 20.24, <em>t</em> = −6.72, <em>p</em>MCMC = 0.000002) and their oral proficiency in Hindi (<em>b</em> = −38.56, <em>SE</em> = 19.74, <em>t</em> = −1.95, <em>p</em>MCMC = 0.0508). Increased oral proficiency in both their native and non-native language was associated with shorter latencies to identify emotion in a non-native language. The time course measure was also predicted by code-switching abilities (<em>b</em> = 28.88, <em>SE</em> = 9.65, <em>t</em> = 2.99), <em>p</em>MCMC = 0.0028, suggesting that enhanced code-switching abilities were associated with longer latencies to achieve stable emotion identification.</p><a id="xhp-41-3-597-ID0EACAA" xmlns:Translation="urn:EBSCO-Translation"> </a><span class="medium-bold" xmlns:Translation="urn:EBSCO-Translation"><a data-auto="ep_link" href="#toc" id="hd_toc_74" title="Discussion">Discussion</a></span><br xmlns:Translation="urn:EBSCO-Translation" /><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Our data furnish rigorous support for the position that in-group members have a significant advantage over out-group members during cross-cultural (vocal) emotion processing, in both accuracy <em>and</em> speed, even when the out-group language is highly familiar to the participant (<a href="#c16">Elfenbein, 2013</a>; <a href="#c34">Paulmann &amp; Uskul, 2014</a>). Our first goal was to corroborate whether there is an in-group advantage in recognition accuracy when vocal expressions are encountered in full utterances (<a href="#c36">Pell et al., 2009a</a>; <a href="#c41">Scherer et al., 2001</a>; <a href="#c45">Van Bezooijen et al., 1983</a>), using well-controlled stimuli in a 2 × 2 crossed experimental design. Consistent with <a href="#c34">Paulmann and Uskul (2014)</a>, our analysis of full utterances (Gfull condition) revealed significantly higher accuracy rates overall in each language condition when English or Hindi listeners decoded emotions in their <em>native</em> language; native listeners were always more accurate than non-native listeners irrespective of whether the non-native language was completely foreign (English participants listening to Hindi) or a highly proficient second language (Hindi participants listening to English).</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">It is not surprising that the in-group accuracy advantage emerged variably across emotion types (<a href="#c2">Beaupré &amp; Hess, 2005</a>; <a href="#c28">Matsumoto, 2002</a>; <a href="#c36">Pell et al., 2009a</a>); our data show that English listeners were superior to Hindi listeners for all five emotions expressed in English, whereas Hindi listeners demonstrated an advantage for recognizing only anger and neutral expressions in Hindi. It seems counterintuitive that the native advantage tended to be larger and more consistent across emotions in the English condition, for which out-group members were highly proficient in that language and had been briefly immersed in the out-group culture at time of testing. This finding underscores that the relative ease by which basic emotions are recognized from vocal patterns in speech shows central tendencies, but qualitatively differs to some degree across language contexts (<a href="#c37">Pell et al., 2009b</a>). Moreover, our results show that even when listeners are competent in a language, this does not necessarily render them an advantage in how well they recognize out-group emotional cues over listeners who attempt to decode emotions from foreign utterances. Rather, performance appears to be dictated in large part by the form in which emotions are encoded in each cultural context, and presumably how this varies from expected norms, consistent with the notion that nonverbal “dialects” impose limitations on the accuracy of (vocal) emotion recognition in cross-cultural settings (<a href="#c16">Elfenbein, 2013</a>; <a href="#c36">Pell et al., 2009a</a>). This claim is supported by evidence that the acoustic cues for encoding basic emotions in English and Hindi differ somewhat (<a href="#c37">Pell et al., 2009b</a>); for example, English and Hindi utterances varied notably in the relative importance of f0, amplitude, and speech rate cues that presumably led to recognition of discrete emotions such as anger, sadness, and happiness (among others).</p><a id="xhp-41-3-597-ID0ECACAA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Cultural Effects on the Buildup and Speed of Emotion Recognition</h4><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">A second aim of this study was to document how the recognition of vocal emotion <em>evolves</em> as a function of time when processed by native and non-native listeners. Our data suggest that recognition unfolds in a qualitatively similar manner when listening to vocal emotions expressed by a different culture, albeit not at the same rate. In terms of the approximate time point when the in-group recognition advantage emerges, we found a strong convergence in the English and Hindi language processing conditions: native listeners in each condition were superior to non-native listeners after monitoring 400 ms to 500 ms of acoustic information. This conclusion was corroborated by an analysis showing that native listeners always displayed larger gains in their recognition in the 200 ms to 500 ms time analysis window than non-native listeners in each cultural context. These data newly establish that the in-group advantage in vocal emotion processing, although seemingly most pronounced when full utterances are analyzed (Gfull), is actually triggered much earlier during speech processing based on a sampling of acoustic information in the first 500 ms of the emotional expression. In line with dialect theory, we hypothesize that in-group members show a significant, early advantage because they can rapidly and efficiently apply universal inferences rules in the context of culturally familiar rules for encoding emotional expressions; in contrast, out-group members (non-native listeners) are hampered by interference at the stage of mapping culturally inconsistent cues within the 200 ms to 500 ms time frame. Note that because the in-group advantage emerged at approximately the same time point in both cultural contexts, and Hindi listeners were proficient in English (both oral and written), it is unlikely that interference produced by unfamiliar linguistic (segmental or suprasegmental) elements explained the accuracy differences for native and non-native speakers in either language. Rather, these effects are likely attributable to differences in cultural modes for expressing vocal emotions.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">It merits emphasizing that recognition improved systematically as utterances unfolded (<a href="#c35">Pell &amp; Kotz, 2011</a>; <a href="#c39">Rigoulot et al., 2013</a>), with scores exceeding chance level for all emotions beginning at the 400-ms gate interval regardless of language type and native status of the listener (with rare exceptions; review <a href="#fig2">Figure 2</a>). This finding underscores that incremental analysis of spectrotemporal patterns in speech facilitates processes of explicit recognition and the ability to categorize the meaning of emotional prosody, regardless of the presence of nonverbal dialects in each cultural group. It appears that both native and non-native listeners quickly develop an increasingly reliable concept about the intended emotional meaning of utterances, irrespective of differences between in-group and out-group expressive patterns, or ultimate levels of recognition attained by each group with increased exposure. These data support the notion of universal inference rules for understanding a speaker’s emotion during speech processing (<a href="#c36">Pell et al., 2009a</a>; <a href="#c41">Scherer et al., 2001</a>), which are applied in a broadly similar manner across cultural groups.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Interestingly, marked accuracy improvements were witnessed in the 200 ms to 400 ms time window, reaching a local peak at around 500 ms to 600 ms poststimulus onset, in which accuracy scores tended to level off. This pattern fits well and extends earlier gating studies of English in which utterances were gated in 250-ms time increments (<a href="#c10">Cornew et al., 2009</a>) or by their syllable structure (<a href="#c35">Pell &amp; Kotz, 2011</a>), including when English stimuli were gated from the <em>end</em> of the utterance, arguing that recognition of vocal cues is largely dictated by time and not by linguistic properties of the utterance (<a href="#c39">Rigoulot et al., 2013</a>). These data also support electrophysiological evidence showing that implicit priming of discrete emotional qualities in the voice occurs between 200 ms and 400 ms poststimulus onset (<a href="#c32">Paulmann &amp; Pell, 2010</a>). Note that our analyses render it improbable that gross differences in utterance and/or syllable duration in the English versus Hindi stimuli influenced our results (e.g., that the longer duration of English stimuli might have led to a “later deployment” of acoustic information in English). Instead, our data may be taken as further proof that underlying mental representations corresponding to discrete emotions in the voice form rapidly after stimulus onset, and stabilize in most cases, after 500 ms to 600 ms of acoustic information is processed (<a href="#c35">Pell &amp; Kotz, 2011</a>).</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Another way to gauge how culture affects the time course of vocal emotion recognition is by comparing EIPs for native and non-native listeners. The EIP serves as a useful measure of the <em>rate</em> by which discrete emotions in the voice are recognized, investigated here for the first time in a cross-cultural setting. We found clear evidence that the in-group advantage in emotion recognition extends to the <em>speed</em> of processing; both English and Hindi listeners accurately recognized emotions from significantly shorter stimulus durations overall in their native language (for English stimuli: English = 728 ms, Hindi = 770 ms; for Hindi stimuli: English = 643 ms, Hindi = 595 ms). We also noted greater differentiation of mean EIPs across emotions in each language condition for in-group versus out-group participants, suggesting that native listeners may have been more sensitive to the emerging value of emotional meanings (see <a href="#fig3">Figure 3</a>). These findings allow the claim that familiarity with culture-specific cues for communicating emotions yields a more fine-grained advantage in recognition speed <em>in addition to</em> accuracy. Moreover, as mean EIPs always occurred after 450 ms to 750 ms of stimulus processing, with one exception (happiness in English; see <a href="#c35">Pell &amp; Kotz, 2011</a>, for similar patterns), our analysis of EIPs in a cross-cultural design provides powerful new data confirming the time course of vocal emotion recognition based on evidence from English and Hindi, for both native and non-native listeners.</p><a id="xhp-41-3-597-ID0EBACAA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Effects of Cultural Exposure and Language Proficiency on the In-Group Advantage</h4><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Our findings provide preliminary indications that general experience using a (non-native) language facilitates the decoding of vocal emotion cues. We found a significant link between overall measures of accuracy (Hu scores) and speed (EIPs) to recognize vocal expressions in English and the level of oral proficiency in English among Hindi participants; as English oral proficiency increased, out-group expressions were recognized more accurately and more quickly. Interestingly, the time course measures were further associated with self-ratings of oral proficiency in the participants’ <em>native</em> language, Hindi. According to dialect theory (<a href="#c16">Elfenbein, 2013</a>; <a href="#c18">Elfenbein &amp; Ambady, 2003</a>; <a href="#c19">Elfenbein, Beaupré, Lévesque, &amp; Hess, 2007</a>), subtle differences in nonverbal dialects—which here arguably impeded accurate <em>and</em> efficient cross-cultural communication of emotions for both English and Hindi participants—can be mitigated by increased cultural contacts and familiarity with out-group expressive patterns. Our rationale for selecting Hindi participants was twofold: They had only briefly been exposed to Canadian English culture, mostly through work or educational social networks in the minority English culture of Montreal, <em>and</em> they had extensive experience and near-native proficiency in the English language from an early age. This allowed a unique opportunity to simultaneously evaluate how cultural exposure, age of acquisition of an out-group language, and linguistic variables inform the in-group advantage in the Hindi group (<a href="#c34">Paulmann &amp; Uskul, 2014</a>). In addition to oral proficiency, it is curious that code-switching abilities <em>negatively</em> predicted the accuracy and efficiency of recognizing out-group stimuli in the Hindi group. There was no obvious relationship between cultural exposure (time in Canada) or age of English acquisition and vocal emotion processing in the Hindi group.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">These relationships could be explained in at least two ways: It is possible that Hindi listeners who were less “proficient” in spoken English were more susceptible to language-specific features unique to the Canadian English dialect, such as phonetic and suprasegmental contrasts that are foreign to Indian English, which interfered at the stage of emotion processing (<a href="#c29">Mesquita &amp; Frijda, 1992</a>; <a href="#c38">Pell &amp; Skorup, 2008</a>; <a href="#c41">Scherer et al., 2001</a>). However, this possibility seems unlikely, given the near-native status of many Hindi participants and the observation that proficiency in Hindi sometimes predicted recognition of out-group stimuli. The interference view is also unlikely, given that code-switching abilities, which reflect an ability to successfully resolve linguistic interference, were counterintuitively associated with <em>lower</em> accuracy and <em>slower</em> speed in emotion recognition. This suggests that cross-language interference does not govern differences in cross-cultural emotion recognition, although the simultaneous presentation of in-group and out-group stimuli may have had cognitive consequences for Hindi speakers who varied in code-switching abilities in our task. We speculate that the application of code-switching strategies may actually hamper the ability to retrieve information about nonverbal dialects in the voice, explaining why the “expert” code-switchers in our Hindi group performed at an inferior level. In terms of the more straightforward relationship between increased oral proficiency and vocal emotion recognition abilities, this could reflect the extent to which Hindi participants were familiar with conventions for expressing vocal emotions in the out-group language. Pending further studies, we propose that more proficient Hindi speakers could apply inference rules more successfully in the context of acquired knowledge about the culturally prescribed form of these expressions in English (<a href="#c19">Elfenbein et al., 2007</a>).</p><a id="xhp-41-3-597-ID0EAACAA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Limitations and Conclusions</h4><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Although our design permits solid claims for an in-group advantage in recognition accuracy <em>and</em> speed in the domain of vocal emotion processing, the generalizability of our findings are naturally limited in certain respects. In particular, effects referring to how discrete emotions are processed should be interpreted cautiously, as these patterns depend on the context in which stimuli are judged by listeners, the number of response alternatives, the perceived intensity of expressions conveying each meaning (<a href="#c23">Juslin &amp; Laukka, 2003</a>), and the level of consensus about emotional meanings encoded by the stimuli. Future studies will take into account how distinct linguistic properties in different languages interact with cultural factors to affect emotion perception and production. In terms of the time course, the more pronounced accuracy increase in the beginning than in the later part of the utterance leaves the role for the time course in emotion recognition to be further tested. Similarly, patterns of confusion as emotional representations build up over time, which influence how the EIP for each emotion was calculated, would vary according to which emotions are included or excluded in the paradigm, yielding potentially different results (cf. <a href="#c10">Cornew et al., 2009</a>; <a href="#c35">Pell &amp; Kotz, 2011</a>). Individual variability at the stage of <em>encoding</em> emotions, owing to different expressive styles and social constraints on male versus female speakers during emotion expression, is also likely to impact on recognition performance (<a href="#c21">Hall, 1984</a>). For example, our observation that both English and Hindi expressions of anger were identified reliably and quickly by listeners may have been biased by including only male expressions, which tend to be more salient than female expressions of anger (<a href="#c7">Brody, Lovas, &amp; Hay, 1995</a>).</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Nonetheless, our results contribute in a creative manner to the unresolved debate about how and <em>when</em> cultural-specific processes shape cultural-universal mechanisms that assign meaning to emotional displays, with new evidence of vocal behavior. Our finding that listeners are at a comparable disadvantage in both accuracy and speed when processing emotions in their second language, versus a completely foreign language, adds to a growing database showing that even subtle cultural differences in the form of emotional expressions (or “dialects”) hamper recognition and yield potential miscommunications in the cross-cultural setting (<a href="#c34">Paulmann &amp; Uskul, 2014</a>). Moreover, our results pinpoint when the in-group advantage for vocal emotion expressions emerges during speech processing, following 400 ms to 500 ms of exposure to acoustic representations in speech. Future researchers should carefully examine how different measures of “cultural exposure,” including oral proficiency measures, modulate the in-group advantage in emotion recognition. Our results highlight that both native and non-native listeners are exquisitely sensitive to the core “universal” features of vocal emotion expressions, and can adeptly decode emotional meanings posed by members of distant cultures with high accuracy and limited delay, even when the “surface structure” of these expressions is phonetically distinct.</p><a id="xhp-41-3-597-ID0EAA" xmlns:Translation="urn:EBSCO-Translation"> </a><span class="medium-bold" xmlns:Translation="urn:EBSCO-Translation"><a data-auto="ep_link" href="#toc" id="hd_toc_92" title="Footnotes">Footnotes</a></span><a id="fn1" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><sup><a href="#b-fn1">1</a></sup> We also fit a line referring to the increase of gate duration in each language by entering gate in the linear mixed model as a continuous variable. A significant Gate × Listener group interaction was produced for both the English stimuli (<em>b</em> = −.03, <em>SE</em> = 0.01, <em>t</em> = −2.32, <em>p</em>MCMC = 0.0218) and the Hindi stimuli (<em>b</em> = .02, <em>SE</em> = 0.01, <em>t</em> = 2.11, <em>p</em>MCMC = 0.0272). In each language, the slope of the linear increase of gate interval on recognition rate was larger for native versus non-native listeners.</p><a id="fn2" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><sup><a href="#b-fn2">2</a></sup> The in-group advantage for speed was significant irrespective of whether the mean EIP was expressed in time or as a proportion of full utterances, to control for gross differences in mean utterance duration across language sets.</p><a id="xhp-41-3-597-ID0E0NB0BBAA" xmlns:Translation="urn:EBSCO-Translation"> </a><span class="medium-bold" xmlns:Translation="urn:EBSCO-Translation"><a data-auto="ep_link" href="#toc" id="hd_toc_98" title="References">References</a></span><a id="c1" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Baayen, R., Davidson, D., &amp; Bates, D. (2008). Mixed-effects modeling with crossed random effects for subjects and items. <em>Journal of Memory and Language</em>, <em>59</em>, 390–412. 10.1016/j.jml.2007.12.005</p><a id="c2" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Beaupré, M., &amp; Hess, U. (2005). Cross-cultural emotion recognition among Canadian ethnic groups. <em>Journal of Cross-Cultural Psychology</em>, <em>36</em>, 355–370. 10.1177/0022022104273656</p><a id="c3" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Beier, E. G., &amp; Zautra, A. J. (1972). Identification of vocal communication of emotions across cultures. <em>Journal of Consulting and Clinical Psychology</em>, <em>39</em>, 166. 10.1037/h0033170</p><a id="c4" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Belin, P., Fillion-Bilodeau, S., &amp; Gosselin, F. (2008). The Montreal Affective Voices: A validated set of nonverbal affect bursts for research on auditory affective processing. <em>Behavior Research Methods</em>, <em>40</em>, 531–539. 10.3758/BRM.40.2.531</p><a id="c5" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Best, C., &amp; Tyler, M. (2007). Nonnative and second-language speech perception. In O.Bohn &amp; M.Munro (Eds.), <em>Language experience in second language speech learning</em> (pp. 13–34). Philadelphia, PA: John Benjamins. 10.1075/lllt.17.07bes</p><a id="c6" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Boersma, P., &amp; Weenink, D. (2005). <em>Praat: Doing phonetics by computer (Version 4.3.01)</em> [Computer program]. Retrieved from <a href="http://www.praat.org/" target="_blank">http://www.praat.org/</a></p><a id="c7" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Brody, L., Lovas, G., &amp; Hay, D. (1995). Gender differences in anger and fear as a function of situational context. <em>Sex Roles</em>, <em>32</em>, 47–78. 10.1007/BF01544757</p><a id="c8" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Bryant, G. A., &amp; Barrett, H. C. (2008). Vocal emotion recognition across disparate cultures. <em>Journal of Cognition and Culture</em>, <em>8</em>, 135–148. 10.1163/156770908X289242</p><a id="c9" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Clopper, C. (2013). Modeling multi-level factors using linear mixed effects. <em>Proceedings of Meetings on Acoustics</em>, <em>19</em>, 060028. 10.1121/1.4799729</p><a id="c10" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Cornew, L., Carver, L., &amp; Love, T. (2009). There’s more to emotion than meets the eye: A processing bias for neutral content in the domain of emotional prosody. <em>Cognition and Emotion</em>, <em>24</em>, 1133–1152. 10.1080/02699930903247492</p><a id="c11" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Cutler, A., Smits, R., &amp; Cooper, N. (2005). Vowel perception: Effects of non-native language vs. non-native dialect. <em>Speech Communication</em>, <em>47</em>, 32–42. 10.1016/j.specom.2005.02.001</p><a id="c12" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Dael, N., Goudbeek, M., &amp; Scherer, K. R. (2013). Perceived gesture dynamics in nonverbal expression of emotion. <em>Perception</em>, <em>42</em>, 642–657. 10.1068/p7364</p><a id="c13" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Dailey, M. N., Joyce, C., Lyons, M. J., Kamachi, M., Ishi, H., Gyoba, J., &amp; Cottrell, G. W. (2010). Evidence and a computational explanation of cultural differences in facial expression recognition. <em>Emotion</em>, <em>10</em>, 874–893. 10.1037/a0020019</p><a id="c14" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Ekman, P. (1972). Universals and cultural differences in facial expressions of emotion. In J.Cole (Ed.), <em>Nebraska symposium of motivation</em> (Vol. <em>19</em>, pp. 207–283). Lincoln, NE: University of Nebraska Press.</p><a id="c15" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Ekman, P., &amp; Friesen, W. V. (1971). Constants across cultures in the face and emotion. <em>Journal of Personality and Social Psychology</em>, <em>17</em>, 124–129. 10.1037/h0030377</p><a id="c16" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Elfenbein, H. A. (2013). Nonverbal dialects and accents in facial expressions of emotion. <em>Emotion Review</em>, <em>5</em>, 90–96. 10.1177/1754073912451332</p><a id="c17" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Elfenbein, H. A., &amp; Ambady, N. (2002). On the universality and cultural specificity of emotion recognition: A meta-analysis. <em>Psychological Bulletin</em>, <em>128</em>, 203–235. 10.1037/0033-2909.128.2.203</p><a id="c18" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Elfenbein, H. A., &amp; Ambady, N. (2003). When familiarity breeds accuracy: Cultural exposure and facial emotion recognition. <em>Journal of Personality and Social Psychology</em>, <em>85</em>, 276–290. 10.1037/0022-3514.85.2.276</p><a id="c19" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Elfenbein, H. A., Beaupré, M., Lévesque, M., &amp; Hess, U. (2007). Toward a dialect theory: Cultural differences in the expression and recognition of posed facial expressions. <em>Emotion</em>, <em>7</em>, 131–146. 10.1037/1528-3542.7.1.131</p><a id="c20" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Grosjean, F. (1980). Spoken word recognition processes and the gating paradigm. <em>Perception &amp; Psychophysics</em>, <em>28</em>, 267–283. 10.3758/BF03204386</p><a id="c21" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Hall, J. A. (1984). <em>Nonverbal sex differences: Communication accuracy and expressive styles</em>. Baltimore. MD: Johns Hopkins University Press.</p><a id="c23" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Juslin, P. N., &amp; Laukka, P. (2003). Communication of emotions in vocal expression and music performance: Different channels, same code?<em>Psychological Bulletin</em>, <em>129</em>, 770–814.</p><a id="c24" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Kirouac, G., &amp; Hess, U. (1999). Group membership and the decoding of nonverbal behavior. In P.Philippot, R.Feldman, &amp; E.Coats (Eds.), <em>The social context of nonverbal behavior</em> (pp. 182–210). Cambridge, UK: Cambridge University Press.</p><a id="c25" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Liu, P., &amp; Pell, M. D. (2012). Recognizing vocal emotions in Mandarin Chinese: A validated database of Chinese vocal emotional stimuli. <em>Behavior Research Methods</em>, <em>44</em>, 1042–1051. 10.3758/s13428-012-0203-3</p><a id="c26" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Markham, R., &amp; Wang, L. (1996). Recognition of emotion by Chinese and Australian children. <em>Journal of Cross-Cultural Psychology</em>, <em>27</em>, 616–643. 10.1177/0022022196275008</p><a id="c27" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Matsumoto, D. (1989). Cultural influences on the perception of emotion. <em>Journal of Cross-Cultural Psychology</em>, <em>20</em>, 92–105. 10.1177/0022022189201006</p><a id="c28" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Matsumoto, D. (2002). Methodological requirements to test a possible in-group advantage in judging emotions across cultures: Comment on Elfenbein and Ambady (2002) and evidence. <em>Psychological Bulletin</em>, <em>128</em>, 236–242. 10.1037/0033-2909.128.2.236</p><a id="c29" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Mesquita, B., &amp; Frijda, N. H. (1992). Cultural variations in emotions: A review. <em>Psychological Bulletin</em>, <em>112</em>, 179–204. 10.1037/0033-2909.112.2.179</p><a id="c30" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Min, C. S., &amp; Schirmer, A. (2011). Perceiving verbal and vocal emotions in a second language. <em>Cognition and Emotion</em>, <em>25</em>, 1376–1392. 10.1080/02699931.2010.544865</p><a id="c31" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Munro, M. J., &amp; Derwing, T. M. (1995). Processing time, accent, and comprehensibility in the perception of native and foreign-accented speech. <em>Language and Speech</em>, <em>38</em>, 289–306.</p><a id="c32" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Paulmann, S., &amp; Pell, M. D. (2010). Contextual influences of emotional speech prosody on face processing: How much is enough?<em>Cognitive, Affective &amp; Behavioral Neuroscience</em>, <em>10</em>, 230–242. 10.3758/CABN.10.2.230</p><a id="c33" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Paulmann, S., &amp; Pell, M. D. (2011). Is there an advantage for recognizing multi-modal emotional stimuli?<em>Motivation and Emotion</em>, <em>35</em>, 192–201. 10.1007/s11031-011-9206-0</p><a id="c34" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Paulmann, S., &amp; Uskul, A. K. (2014). Cross-cultural emotional prosody recognition: Evidence from Chinese and British listeners. <em>Cognition and Emotion</em>, <em>28</em>, 230–244. 10.1080/02699931.2013.812033</p><a id="c35" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Pell, M. D., &amp; Kotz, S. A. (2011). On the time course of vocal emotion recognition. <em>PLoS ONE</em>, <em>6</em>, e27256. 10.1371/journal.pone.0027256</p><a id="c36" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Pell, M. D., Monetta, L., Paulmann, S., &amp; Kotz, S. A. (2009a). Recognizing emotions in a foreign language. <em>Journal of Nonverbal Behavior</em>, <em>33</em>, 107–120. 10.1007/s10919-008-0065-7</p><a id="c37" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Pell, M. D., Paulmann, S., Dara, C., Alasseri, A., &amp; Kotz, S. A. (2009b). Factors in the recognition of vocally expressed emotions: A comparison of four languages. <em>Journal of Phonetics</em>, <em>37</em>, 417–435. 10.1016/j.wocn.2009.07.005</p><a id="c38" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Pell, M. D., &amp; Skorup, V. (2008). Implicit processing of emotional prosody in a foreign versus native language. <em>Speech Communication</em>, <em>50</em>, 519–530. 10.1016/j.specom.2008.03.006</p><a id="c39" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Rigoulot, S., Wassiliwizky, E., &amp; Pell, M. D. (2013). Feeling backwards? How temporal order in speech affects the time course of vocal emotion recognition. <em>Frontiers in Psychology</em>, <em>4</em>, 367. 10.3389/fpsyg.2013.00367</p><a id="c40" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Sauter, D. A., Eisner, F., Ekman, P., &amp; Scott, S. K. (2010). Cross-cultural recognition of basic emotions through nonverbal emotional vocalizations. <em>PNAS Proceedings of the National Academy of Sciences of the United States of America</em>, <em>107</em>, 2408–2412. 10.1073/pnas.0908239106</p><a id="c41" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Scherer, K., Banse, R., &amp; Wallbott, H. (2001). Emotion inferences from vocal expression correlate across languages and cultures. <em>Journal of Cross-Cultural Psychology</em>, <em>32</em>, 76–92. 10.1177/0022022101032001009</p><a id="c42" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Schwanenflugel, P., &amp; Rey, M. (1986). Interlingual semantic facilitation: Evidence for a common representational system in the bilingual lexicon. <em>Journal of Memory and Language</em>, <em>25</em>, 605–618. 10.1016/0749-596X(86)90014-8</p><a id="c43" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Snijders, T. (2005). Fixed and random effects. In B.Everitt &amp; D.Howell (Eds.), <em>Encyclopedia of statistics in behavioral science</em> (Vol. <em>2</em>, pp. 664–665). Chichester, UK: Wiley.</p><a id="c44" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Thompson, W., &amp; Balkwill, L. (2006). Decoding speech prosody in five languages. <em>Semiotica</em>, <em>158</em>, 407–424.</p><a id="c45" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Van Bezooijen, R., Otto, S., &amp; Heenan, T. (1983). Recognition of vocal expressions of emotion. A three-nation study to identify universal characteristics. <em>Journal of Cross-Cultural Psychology</em>, <em>14</em>, 387–406. 10.1177/0022002183014004001</p><a id="c46" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Wagner, H. (1993). On measuring performance in category judgment studies of nonverbal behavior. <em>Journal of Nonverbal Behavior</em>, <em>12</em>, 98–106.</p><a id="xhp-41-3-597-ID0EAA" xmlns:Translation="urn:EBSCO-Translation"> </a><span class="medium-bold" xmlns:Translation="urn:EBSCO-Translation"><a data-auto="ep_link" href="#toc" id="hd_toc_234" title="APPENDICES">APPENDICES</a></span><a id="A" xmlns:Translation="urn:EBSCO-Translation"> </a><a id="xhp-41-3-597-ID0EBABAA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">APPENDIX A: Scripts of (a) Hindi and (b) English  Pseudoutterances Presented in the Study</h4><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Pseudoutterances were produced by one of two native speakers in each language (initials shown in parentheses). Recordings were selected when all five emotion expressions of an utterance produced by a single speaker were recognized at high accuracy levels by native listeners; <a href="#c37">Pell et al., 2009b</a>).</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">(a) Hindi pseudo-utterances</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/bfc55bc08584e9173ec3b30681006314/571c5f1d/pdh/xhp/xhp-41-3-597-image1.gif" alt="xhp-41-3-597-image1.gif" title=" " /><em> </em></span></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/b6960d6c891e131db23d483b3edec45c/571c5f1d/pdh/xhp/xhp-41-3-597-image2.gif" alt="xhp-41-3-597-image2.gif" title=" " /><em> </em></span></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/52904285bcc7a11f0fb1d9339158049b/571c5f1d/pdh/xhp/xhp-41-3-597-image3.gif" alt="xhp-41-3-597-image3.gif" title=" " /><em> </em></span></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/84917453a288ffa68339adfab264823c/571c5f1d/pdh/xhp/xhp-41-3-597-image4.gif" alt="xhp-41-3-597-image4.gif" title=" " /><em> </em></span></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/c16a936d4b6699be937590ff0db51763/571c5f1d/pdh/xhp/xhp-41-3-597-image5.gif" alt="xhp-41-3-597-image5.gif" title=" " /><em> </em></span></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/253e21a962b29b8eecfe442f77075400/571c5f1d/pdh/xhp/xhp-41-3-597-image6.gif" alt="xhp-41-3-597-image6.gif" title=" " /><em> </em></span></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/6da4c1c7626263f88243d8e18e12fe23/571c5f1d/pdh/xhp/xhp-41-3-597-image7.gif" alt="xhp-41-3-597-image7.gif" title=" " /><em> </em></span></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/2a75c6a60565423b5b91bec84c2584d3/571c5f1d/pdh/xhp/xhp-41-3-597-image8.gif" alt="xhp-41-3-597-image8.gif" title=" " /><em> </em></span></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/08a1b6e30fad4bd9b3744edd698f0fc0/571c5f1d/pdh/xhp/xhp-41-3-597-image9.gif" alt="xhp-41-3-597-image9.gif" title=" " /><em> </em></span></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/9b42295386a034f9ed0acd4c5f47a4fc/571c5f1d/pdh/xhp/xhp-41-3-597-image10.gif" alt="xhp-41-3-597-image10.gif" title=" " /><em> </em></span></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/4f46cc23f8c7eea6bcf2717304f8b002/571c5f1d/pdh/xhp/xhp-41-3-597-image11.gif" alt="xhp-41-3-597-image11.gif" title=" " /><em> </em></span></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/773925ab7122f304eba73974ca449184/571c5f1d/pdh/xhp/xhp-41-3-597-image12.gif" alt="xhp-41-3-597-image12.gif" title=" " /><em> </em></span></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">(b) English pseudo-utterances</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">We revimerated the mesty yorns. (DF)</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">I tropped for swinty gowers. (DF)</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">I yaded it to zanber gip. (DF)</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">She krayed a jad ralition. (DF, MG)</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">We wanced on the nonitor. (DF)</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">We groffed for vappy laurits. (DF, MG)</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">I sarred the westical yars. (DF)</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">I marlipped the tovity. (DF)</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">They glayed medolinous wames. (MG)</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">The crinklet is boritate. (MG)</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">He brolated a wugster. (MG)</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">I chayed for his pality. (MG)</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">I democollated a fulf. (MG)</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">She kuvelled the noralind. (MG)</p><a id="B" xmlns:Translation="urn:EBSCO-Translation"> </a><a id="xhp-41-3-597-ID0EAABAA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">APPENDIX B: Mean Number of Syllables per Gate Interval, by Emotion and Language Condition</h4><table cellpadding="2"><colgroup span="1"><col align="left" span="1" /><col align="char" char="." span="1" /><col align="char" char="." span="1" /><col align="char" char="." span="1" /><col align="char" char="." span="1" /><col align="char" char="." span="1" /><col align="char" char="." span="1" /></colgroup><thead><tr valign="bottom"><th align="center" rowspan="1" colspan="1">Emotion</th><th align="center" rowspan="1" colspan="1">G200</th><th align="center" rowspan="1" colspan="1">G400</th><th align="center" rowspan="1" colspan="1">G500</th><th align="center" rowspan="1" colspan="1">G600</th><th align="center" rowspan="1" colspan="1">G700</th><th align="center" rowspan="1" colspan="1">Gfull</th></tr></thead><tbody><tr valign="top"><td rowspan="1" colspan="1">English utterances</td><td colspan="6" align="center" rowspan="1"> </td></tr><tr valign="top"><td rowspan="1" colspan="1"> Anger</td><td rowspan="1" colspan="1">0.74</td><td rowspan="1" colspan="1">1.47</td><td rowspan="1" colspan="1">1.84</td><td rowspan="1" colspan="1">2.21</td><td rowspan="1" colspan="1">2.57</td><td rowspan="1" colspan="1">7.38</td></tr><tr valign="top"><td rowspan="1" colspan="1"> Fear</td><td rowspan="1" colspan="1">1.08</td><td rowspan="1" colspan="1">2.16</td><td rowspan="1" colspan="1">2.70</td><td rowspan="1" colspan="1">3.24</td><td rowspan="1" colspan="1">3.78</td><td rowspan="1" colspan="1">7.38</td></tr><tr valign="top"><td rowspan="1" colspan="1"> Happiness</td><td rowspan="1" colspan="1">0.80</td><td rowspan="1" colspan="1">1.61</td><td rowspan="1" colspan="1">2.01</td><td rowspan="1" colspan="1">2.41</td><td rowspan="1" colspan="1">2.81</td><td rowspan="1" colspan="1">7.38</td></tr><tr valign="top"><td rowspan="1" colspan="1"> Sadness</td><td rowspan="1" colspan="1">0.76</td><td rowspan="1" colspan="1">1.51</td><td rowspan="1" colspan="1">1.89</td><td rowspan="1" colspan="1">2.27</td><td rowspan="1" colspan="1">2.65</td><td rowspan="1" colspan="1">7.38</td></tr><tr valign="top"><td rowspan="1" colspan="1"> Neutral</td><td rowspan="1" colspan="1">0.89</td><td rowspan="1" colspan="1">1.77</td><td rowspan="1" colspan="1">2.22</td><td rowspan="1" colspan="1">2.66</td><td rowspan="1" colspan="1">3.10</td><td rowspan="1" colspan="1">7.38</td></tr><tr valign="top"><td rowspan="1" colspan="1">Hindi utterances</td><td colspan="6" align="center" rowspan="1"> </td></tr><tr valign="top"><td rowspan="1" colspan="1"> Anger</td><td rowspan="1" colspan="1">1.33</td><td rowspan="1" colspan="1">2.67</td><td rowspan="1" colspan="1">3.34</td><td rowspan="1" colspan="1">4.00</td><td rowspan="1" colspan="1">4.67</td><td rowspan="1" colspan="1">6.75</td></tr><tr valign="top"><td rowspan="1" colspan="1"> Fear</td><td rowspan="1" colspan="1">1.24</td><td rowspan="1" colspan="1">2.47</td><td rowspan="1" colspan="1">3.09</td><td rowspan="1" colspan="1">3.71</td><td rowspan="1" colspan="1">4.33</td><td rowspan="1" colspan="1">6.75</td></tr><tr valign="top"><td rowspan="1" colspan="1"> Happiness</td><td rowspan="1" colspan="1">1.06</td><td rowspan="1" colspan="1">2.12</td><td rowspan="1" colspan="1">2.65</td><td rowspan="1" colspan="1">3.18</td><td rowspan="1" colspan="1">3.71</td><td rowspan="1" colspan="1">6.75</td></tr><tr valign="top"><td rowspan="1" colspan="1"> Sadness</td><td rowspan="1" colspan="1">0.76</td><td rowspan="1" colspan="1">1.53</td><td rowspan="1" colspan="1">1.91</td><td rowspan="1" colspan="1">2.29</td><td rowspan="1" colspan="1">2.68</td><td rowspan="1" colspan="1">6.75</td></tr><tr valign="top"><td rowspan="1" colspan="1"> Neutral</td><td rowspan="1" colspan="1">1.06</td><td rowspan="1" colspan="1">2.12</td><td rowspan="1" colspan="1">2.64</td><td rowspan="1" colspan="1">3.17</td><td rowspan="1" colspan="1">3.70</td><td rowspan="1" colspan="1">6.75</td></tr></tbody></table><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><em>Submitted: </em>July 17, 2014<em> Revised: </em>January 29, 2015<em> Accepted: </em>January 30, 2015</p><hr noshade="noshade" /><p class="body-paragraph" data-auto="copyright_info">This publication is protected by US and international copyright laws and its content may not be copied without the copyright holders express written permission except for the print or download capabilities of the retrieval software used for access. This content is intended solely for the use of the individual user.<br xmlns:ExtendedMarkupController="urn:ExtendedMarkupController" /><br xmlns:ExtendedMarkupController="urn:ExtendedMarkupController" /><strong xmlns:ExtendedMarkupController="urn:ExtendedMarkupController">Source:&nbsp;</strong>Journal of Experimental Psychology: Human Perception and Performance. Vol. 41. (3), Jun, 2015 pp. 597-612)<br xmlns:ExtendedMarkupController="urn:ExtendedMarkupController" /><strong xmlns:ExtendedMarkupController="urn:ExtendedMarkupController">Accession Number:&nbsp;</strong>2015-11176-001<br xmlns:ExtendedMarkupController="urn:ExtendedMarkupController" /><strong xmlns:ExtendedMarkupController="urn:ExtendedMarkupController">Digital Object Identifier:&nbsp;</strong>10.1037/xhp0000043</p></section></div>
		

		<div class="widget-loading loading"></div>
	
		<!-- WorldCat Widgets-->
		

	<!-- Full text will be rendered in this placeholder if citation is being displayed with
	full text. -->
	
	
	

	<div class="content-footer" >
	 

	</div>
	
	<div class="rs-placeholder" id="ctl00_ctl00_MainContentArea_MainContentArea_speaker_box" style="display:none;" data-parent="textToSpeechPlaceholder" data-readid="TextToSpeech" data-speed="MEDIUM" data-voice="ScanSoft_Jill_Full_22kHz" data-server="http://app.rs.ebscohost.com/cgi-bin/rsent?customerid=5845" data-download="true" data-isdetail="true"> </div>



						</div>
					</div>
					<div id="column1" class="collapsible" >
	<a class="collapsible-toggle" href="#" ></a>
	<div class="collapsible-content">
		
	


<h3 class="vis-hidden">View:</h3>
<ul class="format-control" >
		<li id="ctl00_ctl00_Column1_Column1_formatButtonsTop_formatButtonRepeater_ctl01_listItem" class="format-item">
			 
			<!-- Making assumption that we don't want spaces between MARC link and parenthesis. -->
			
			
			<a id="ctl00_ctl00_Column1_Column1_formatButtonsTop_formatButtonRepeater_ctl01_linkButton" title="Detailed Record" class="record-type format-citation" href="javascript:__doPostBack(&#39;ctl00$ctl00$Column1$Column1$formatButtonsTop$formatButtonRepeater$ctl01$linkButton&#39;,&#39;&#39;)">Detailed Record</a>
			
			
			
		</li>
	
		<li id="ctl00_ctl00_Column1_Column1_formatButtonsTop_formatButtonRepeater_ctl02_listItem" class="format-item active">
			 
			<!-- Making assumption that we don't want spaces between MARC link and parenthesis. -->
			
			
			
			<span id="ctl00_ctl00_Column1_Column1_formatButtonsTop_formatButtonRepeater_ctl02_label" title="HTML Full Text" class="record-type html-ftwg">HTML Full Text</span>
			
			
		</li>
	
		<li id="ctl00_ctl00_Column1_Column1_formatButtonsTop_formatButtonRepeater_ctl03_listItem" class="format-item">
			 
			<!-- Making assumption that we don't want spaces between MARC link and parenthesis. -->
			
			<span id="ctl00_ctl00_Column1_Column1_formatButtonsTop_formatButtonRepeater_ctl03_hddnInstructionMessage" class="hidden">This PDF document opens in a frame, to view the document outside of a frame, please change your Adobe Reader settings. To do this, open Adobe Reader, go to Help Menu and select Accessibility Setup Assistant option then select Use Recommend Settings and Skip Setup. You only need to do this once with the current computer you are using.</span>
			<a id="ctl00_ctl00_Column1_Column1_formatButtonsTop_formatButtonRepeater_ctl03_linkButton" title="PDF Full Text" class="record-type pdf-ft" href="javascript:__doPostBack(&#39;ctl00$ctl00$Column1$Column1$formatButtonsTop$formatButtonRepeater$ctl03$linkButton&#39;,&#39;&#39;)">PDF Full Text</a>
			
			
			<span id="ctl00_ctl00_Column1_Column1_formatButtonsTop_formatButtonRepeater_ctl03_suffix" class="format-note">(840.6KB)</span>
		</li>
	</ul>
	

	
	<div id="citedExternalSources"  style="display:none;">
		
	</div>
	
		<h3 class="hidden">Cited References</h3>
		<ul class="reference-links" >
	
		<li >
			<span id="ctl00_ctl00_Column1_Column1_referencebuttoncontrol_referenceButtonRepeater_ctl01_ReferenceLinkContainer">				
				<a id="ctl00_ctl00_Column1_Column1_referencebuttoncontrol_referenceButtonRepeater_ctl01_ReferenceLinkCitation" class="marc-link" href="javascript:__doPostBack(&#39;ctl00$ctl00$Column1$Column1$referencebuttoncontrol$referenceButtonRepeater$ctl01$ReferenceLinkCitation&#39;,&#39;&#39;)">Cited References</a>				
				<a id="ctl00_ctl00_Column1_Column1_referencebuttoncontrol_referenceButtonRepeater_ctl01_ReferenceLink" class="marc-link" href="javascript:__doPostBack(&#39;ctl00$ctl00$Column1$Column1$referencebuttoncontrol$referenceButtonRepeater$ctl01$ReferenceLink&#39;,&#39;&#39;)">(45)</a>				
			</span>			
			
		</li>
	</ul>

	
	
	
	
	</div>
</div>
					<div role="complementary" id="column2" class="collapsible" >
	<a class="collapsible-toggle" href="#" ></a>
	<div class="collapsible-content" >
		
	<hr class="vis-none" />
<h2 title="Tools" accesskey="5" tabindex="0" class="article-tools-header" id="ArticleTools"  >Tools</h2>
<ul class="article-tools delivery-control" >
		<li class="article-tool" >
			<a   href="#" title="Print" class="print-link"    data-panel='{"Id":"print","Url":"delivery/printpanel","Js":"ep/controller/control/citationdeliverypanel.js"}' >Print</a>
		</li>
		<li class="article-tool" >
			<a   href="#" title="E-mail" class="email-link"    data-panel='{"Id":"email","Url":"delivery/emailpanel","Js":"ep/controller/control/citationdeliverypanel.js"}' >E-mail</a>
		</li>
		<li class="article-tool" >
			<a   href="#" title="Save" class="save-link"    data-panel='{"Id":"save","Url":"delivery/savepanel","Js":"ep/controller/control/citationdeliverypanel.js"}' >Save</a>
		</li>
		<li class="article-tool" >
			<a   href="#" title="Cite" class="cite-link"    data-panel='{"Id":"cite","Url":"delivery/citepanel","Js":"ep/controller/control/citepanel.js"}' >Cite</a>
		</li>
		<li class="article-tool" >
			<a   href="#" title="Export" class="export-link"    data-panel='{"Id":"export","Url":"/ehost/delivery/exportpanel?sid=6609b2cc-c9f7-464c-97d7-6d549f841159@sessionmgr115\u0026vid=0\u0026form=False","Js":"ep/controller/control/exportpanel.js"}' >Export</a>
		</li>
		<li class="article-tool" >
			<a   href="#" title="Permalink" class="permalink-link"    data-panel='{"Id":"permalink","Url":"delivery/permalinkpanel","Js":"ep/controller/control/plinkpanel.js"}' >Permalink</a>
		</li>
		<li class="article-tool" >
			<a   href="#" title="Share" class="bookmark-link"    data-panel='{"Id":"bookmark","Url":"addthis/addthispanel","Js":"ep/controller/control/bookmarkpanel.js"}' >Share</a>
		</li>

</ul>

	</div>
</div>
				
					
				
				<div class="extra1" role="presentation">&nbsp;</div>
			</div>
			<div class="footer-wrapper" >
				
	

				<div class="push-sticky-footer"></div>
			</div>
		</div>
		
	

				
		


	</form>
	
</body>
</html>
