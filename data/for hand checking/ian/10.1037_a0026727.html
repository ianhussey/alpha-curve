
<!DOCTYPE html>
<html id="_htmlTag" lang="en">

<head><meta charset="utf-8" /><title>
	The role of semantic diversity in lexical organization: EBSCOhost
</title>
	
<link rel="icon" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/ehost/favicon.ico" type="image/x-icon" />
<link rel="shortcut icon" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/ehost/favicon.ico" type="image/x-icon" />

	<link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/css/ehost/master_bundle.css" media="All" />
<link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/css/_layout2/rtac.css" media="All" />
<link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/css/common/abody.css" media="All" />
<link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/css/_layout2/selecteddatabasescontrol.css" media="All" />
<link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/css/_layout2/page/detail.css" media="All" />
<link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/css/_layout2/carousel.css" media="All" />
<link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/css/_layout2/bookcarousel.css" media="All" />
<link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/css/_layout2/emailprintdialog.css" media="All" />
<link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/css/_layout2/print.css" media="Print" />
<!--[if lt IE 9]><link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/css/_layout2/ie8.css" media="All" /><![endif]-->
<!--[if lt IE 8]><link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/css/_layout2/ie7.css" media="All" /><![endif]-->
<!--[if lt IE 7]><link rel="stylesheet" type="text/css" href="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/css/_layout2/ie6.css" media="All" /><![endif]-->
<!--##EPCSS##-->
	
	<script>
var ep = {"version":"16.1.0.155","baseImagePath":"http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/","brandingPath":"http://imageserver.ebscohost.com/branding/","interfaceId":"ehost","cssLayout":2,"messages":{"Close":"Close","Loading":"Loading","show_this_area":"Show this area","hide_this_area":"Hide this area","column1-closed":"Show Left Column","column1-open":"Hide Left Column","column2-closed":"Show Right Column","column2-open":"Hide Right Column","sh_more":"Show More","sh_less":"Show Less","enter_email_address":"Please enter your e-mail address.","email_invalid_error":"Please provide a valid email address.","field_required":"This field is required.","your_subject_may_not_contain_html_markup":"Your subject may not contain HTML markup.","your_comments_may_not_contain_html_markup":"Your comments may not contain HTML markup.","err_sending_email":"Error Sending Email","your_message_may_not_contain_html_markup":"Your message may not contain HTML markup."},"clientData":{"googleTagManagerId":"GTM-NCMJP5","usrNo":0,"currentRecord":{"Db":"pdh","Tag":"AN","Term":"2012-15094-006"},"rtacView":"detail","rtacTimeout":30,"addThis":{"widgetUrl":"http://s7.addthis.com/js/250/addthis_widget.js#username=ebscohost","bookmarkUrl":"http://www.addthis.com/bookmark.php?v=250\u0026username=ebscohost"},"hoverPreviewLabelData":"{\"Abstract\":\"Abstract\",\"Date\":\"Date\",\"Source\":\"Source\",\"Subjects\":\"Subjects\",\"Title\":\"Title\",\"Citation\":\"Detail\",\"FullCitation\":\"Detailed Record\",\"AddToFolder\":\"Add to folder\",\"RemoveFromFolder\":\"Remove from folder\",\"FolderItem\":\"Folder Item\",\"AddExternalRecToFolder\":\"Add citation to Other Contents Folder\",\"RemoveExternalRecFromFolder\":\"Remove citation from Other Content Sources Folder\",\"AddToFolderTitle\":\"Add result to folder\",\"RemoveFromFolderTitle\":\"Remove result from folder\",\"AddRemoveToFolder\":\"Add/Remove \",\"AddRemoveToFolderTitle\":\"Add or remove from folders\",\"PublicationType\":\"Publication Type\",\"Database\":\"Database\",\"Duration\":\"Length (hours:minutes)\"}","plink":"http://search.ebscohost.com/login.aspx?direct=true\u0026db=pdh\u0026AN=2012-15094-006\u0026site=ehost-live"},"templates":{},"pageScripts":["bundled/jqueryplusui.js","bundled/underscore.js","bundled/_layout2/master.js","bundled/ehost/page/detail.js","bundled/buzzloader.js","bundled/buzzsessionsync.js","ep/selectdb.js","ep/widgets/epeditor.js","ckeditor/ckeditor.js","ckeditor/adapters/jquery.js","bundled/notesmodal.js","jquery/plugins/jquery.ba-bbq.js","ep/controller/realtimeavailabilitycontroller.js","ep/jqueryplugins/scrollto.js","ep/controller/concurrentaccesscontroller.js","ep/ep_readspeaker.js","ep/common/menubar.js","ep/googleclassroom/gc-boot.js"],"relativeRequestPath":"detail/detail","sid":"3f1a0278-d466-4092-b2fa-b3ec663b4f99@sessionmgr4004","vid":"0","existingReturnUrl":"","newReturnUrl":"/ehost/detail/detail?sid=3f1a0278-d466-4092-b2fa-b3ec663b4f99@sessionmgr4004\u0026vid=0\u0026hid=4206\u0026bdata=JnNpdGU9ZWhvc3QtbGl2ZQ==","locale":"en"}
</script>
<script src="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/javascript/bundled/ep_boot.js"></script>
<!--[if lt IE 9]>
<script src="http://smallcontent.ebsco-content.com/interfacefiles/16.1.0.155.1/javascript/html5shiv/html5.js"></script>
<![endif]-->
<script>

ep.boot(function() {
	ep.updateSearchMode();
	focusOnMainContent();
	
ep.util.url.updateHash("db=pdh&AN=2012-15094-006");

	ep.getInstance( { epId: 'ep.controller.page.CitationController' });
	ep.getScreenResolution();

},
null);
</script>
<!--##EPJS##-->
	
	
</head>
<body id="ctl00_ctl00__bodyTag" class="column1-open column2-open limited-scope no-skin detail ehost">
	
	

	<div id="epAjaxActive">Loading...</div>	
	<form method="post" action="detail?sid=3f1a0278-d466-4092-b2fa-b3ec663b4f99%40sessionmgr4004&amp;vid=0&amp;hid=4206&amp;bdata=JnNpdGU9ZWhvc3QtbGl2ZQ%3d%3d" id="aspnetForm">
<input type="hidden" name="AddToFolderClientIDs" id="AddToFolderClientIDs" value="" />
<input type="hidden" name="RelRequestPath" id="RelRequestPath" value="detail/detail" />
<input type="hidden" name="__sid" id="__sid" value="3f1a0278-d466-4092-b2fa-b3ec663b4f99@sessionmgr4004" />
<input type="hidden" name="__vid" id="__vid" value="0" />
<input type="hidden" name="__CUSTOMVIEWSTATE" id="__CUSTOMVIEWSTATE" value="H4sIAAAAAAAEAI1WYW8UNxAVSzYhRG0qivKlUs6oUptId6eEECAtqpSERkUFmkIKH5HP9u1a9dqL7eW4/qj+xfaNvXc5WlXql4vXu5735s2bcf66sS23y63R4YMHJ0cPTx49Ot7eKXa/fMONljyqV+p9p0J84aQqbuwU023Z/xQ36Ls1w21VFspi4yZtlMLwEMonwpmusYcj1yrL8sP9/GB0o6OSoyDwyKwbhd+1ZVJFrg1TtQsRsUpAfLHAobgbb3TQE6NqKYuNtE9wg5t1zwQrWdza7ley2OzPyp216fXu2vXydlpVOZv1DLVOSFvv7j2Lqjl3nY3FzZ31FKTofzcQ4PburfMQzlOeX3klnJejOEcuU+cbHkdCRx61s7tb565puJUveaPKkx8vx89VxcV8fO5s9M4Y5cP4acp7fJGOhvF5fzY/n3UxIs52H+fUV12jbJS7a+/uvT4riD8yKZblGBTlnZ4EJG4YF1F/UBJSLcXIGQzK8s4q81ZOR9M4WC+f/C+Wl3J60RlzpT7GVaKDDTm4tWD1GXDWdtfom3Jz7+j48fjhz2f7+d1KGcrFEgQLrNcHm+XdC20lew2fGO7ZKxU6E8PuxpVz5kq35d1XKnqtPijGjWE+vx5slH9uXdWKgbJibsqCgmRRCyYhgQ86zhlsZtRHLbhhzlfc6j+S1mNACMhKoRT3omY1D0zUiK5spSSLNfmUPsWSRzaDbmzqqTOsQNiQvuhDalux1msrdAsinZXKmzltLqC5ECqEIWudBj+80DZExQHjMlLXTJSnFAQqAPlCRuUZ1wnR+UC57J1KZZDkkJ15N8Ofb3jTfs9+Rb9yyYfs/sHBw/0xS0YmnGW4CooExJuoGIH0viOhyHeQdKojEak7BF5Slkqg+5A+TMgsbygahgMnYtTdPYDns0zPQxcgIAWiHMbsJzdDvfwQB6AWb1vvOHTWlXUQPaWtbXYuwXglIRwncRGD3i65I+9ZrXGWdlcEGSKfKfzuPJvBBGqKxJAHCvlvI4zZb4H4TlyscQxt0HZhNOEBtQ6xk/OUJ0+gqQPw4KOeaqEhBk28jleKqY+t8poackiYUjUOlfQYmjmhpnU+Igm1UsuOm9XsVhy5sEXoqgoDl/ilsn9ysFUoFBRKOiyJOGsolPAKKfQ++TawBoT8HBmhHBUy1VnK1SjcKwJ2TQvrIs0JUmeNk53JhQDKUr6eyJi9pWJBGNkJlVSqrE7eQVUpBwCjeawODVUAKhmubYJuORnOkh4TVfMP2nWeMGEXJ3PGasG+hwNdk53Zt8d/+ARenimVYbIDl9Klaq5G7l/3+kAPanwb+2FAQwQSKPPPPiGh4HRiryW+J0NkHmCYJsF1T6QAgWVP4QulQcCnDrkeHPCqdCKN9NxEw2w8ExxpSfpew4/6+UIebblIegTdUKXUdZFCHprJ5HuXYS6evbz4hT0FL6JCg45aZk/sYzocHrHTy9NhnqK6qtFdJITHjbGPqbGo6mWuKqg87616mqzKXuQcX/fYgb2l2ASRjiLrUxgp1s4B8fAxm2Ns5RHlDKbiPqb/Vr6E6bZYXvX5Xvj8043lBZxObJZfP7k3GrEObL9j4fjxwcHx0cm4gc/G6T8IxkajH+TfjakTKNsIAAA=" />
<input type="hidden" name="__ScreenResolution" id="__ScreenResolution" value="" />
<input type="hidden" name="__VIEWSTATE" id="__VIEWSTATE" value="" />

		<!--[if lt IE 7]>	
		

<div class="ie6_req_block">
	<div class="ie6_req_text">
		IE6 users please note our browser requirements are changing! See  <a href="http://support.epnet.com/knowledge_base/detail.php?id=25" target="_blank" title="EBSCO's Support Site">EBSCO's Support Site</a>  for more information.
	</div>
</div>
		<![endif]-->
		
		<div id="outerContainer">
			<div id="innerContainer">
				
					
				
				
	
	
		

				
	
	

				
				<div id="header" class="clearfix" role="banner" >
					<div id="pageInstruction" tabindex="-1" class="hidden">citation_instruction</div><p tabindex="0" class="hidden"><a href="javascript:openWideTip('http://support.ebsco.com/help/?int=ehost&lang=en&feature_id=access&TOC_ID=Always&SI=0&BU=0&GU=1&PS=0&ver=&dbs=pdh')">Accessibility Information and Tips</a> Revised Date: 07/2015</p>
					<h1 title="The role of semantic diversity in lexical organization" class="hidden">The role of semantic diversity in lexical organization</h1>
					
					
	<div class="customerLogo"><a href="javascript:__doPostBack('ctl00$ctl00$FindField$customerLogo');"><img src="http://www.tilburguniversity.edu/static/uvtpresentation/images/framework/logo.jpg" alt="Library Logo" /></a></div>
	

					
				</div>
					<div id="mainContentArea" >
						<div id="content" role="main" class="text-normal" >
							
	
	<div class="content-header" >
	 

	</div>
	
	
	

	
	<div id="ToolPanelContent" class="bg-p2" >
		<div class="wrapper clearfix" >
		</div>
		<a  href="#" title="Close Panel" class="close-panel"></a>
	</div>

	<!-- If citation is being displayed it will be rendered inside this placeholder.
		 If citation is not being displayed, full text will be rendered in this placeholder. -->
	<div class="ft-translation hidden"><label for="transLanguage">Translate Full Text:</label></div><div class="ft-translation"><a name="Translate"> </a><select id="transLanguage" name="transLanguage" title="Choose Language"><option value="" selected="selected">Choose Language</option><option value="Arabic">الإنجليزية/العربية</option><option value="Bulgarian">английски език/български</option><option value="SimplifiedChinese">英语/简体中文</option><option value="TraditionalChinese">英語/繁體中文</option><option value="Czech">angličtina/čeština</option><option value="Danish">Engelsk/dansk</option><option value="Dutch">Engels/Nederlands</option><option value="French">Anglais/Français</option><option value="German">Englisch/Deutsch</option><option value="Greek">Αγγλικά/Ελληνικά</option><option value="Hausa">English/Hausa</option><option value="Hebrew">אנגלית/עברית</option><option value="Hindi">अंग्रेज़ी/हिंदी</option><option value="Hungarian">angol/magyar</option><option value="Indonesian">Inggris/bahasa Indonesia</option><option value="Italian">Inglesi/Italiano</option><option value="Japanese">英語/日本語</option><option value="Korean">영어/한국어</option><option value="Norwegian">Engelsk/Norsk</option><option value="Persian">انگليسی/فارسی</option><option value="Polish">angielski/polski</option><option value="Portuguese">Inglés/Português</option><option value="Pashto">English/Pashto</option><option value="Romanian">Engleză/română</option><option value="Russian">Английский/Русский</option><option value="Spanish">Inglés/Español</option><option value="Serbian">English/Serbian</option><option value="Swedish">Engelska/svenska</option><option value="Thai">อังกฤษ/ไทย</option><option value="Turkish">İngilizce/Türk</option><option value="Ukranian">Англійська/Українська</option><option value="Urdu">انگریزی/اردو</option></select>&nbsp;<input type="button" id="translateBtn" class="translate" value="Translate" title="Translate" /><input type="button" id="translateOriginal" class="translate" value="Back to English" title="Back to English" /></div><div id="translationProgressContainer" style="display: none;"><span>Translation in Progress:</span><div class="translationProgressBar"><div id="translationProgressBar" class="bg-p1"> </div></div></div><div id="translationErrorContainer" class="medium-normal translation-message" style="display: none;"> </div><div id="translationDisclaimerContainer" style="display: none;"><div class="translation-message"><span class="medium-bold"><span class="txt-red" id="translationDisclaimerLine1"> </span></span><span class="medium-normal" id="translationDisclaimerLine2"> </span><span class="medium-bold" id="translationDisclaimerLine3"> </span><div class="medium-normal">Translations powered by Language Weaver Service<br /></div></div></div><script type="text/javascript">
				ep.getInstance("ep.controller.control.translation");
				ep.require( "common/translation.css" );
			</script><dl class="short-citation" data-auto="short_citation" xmlns:viewExtensions="http://www.ebscohost.com/schema/viewExtensions"><dt class="medium-bold" data-auto="short_citation_title_label">Title: </dt><dd class="medium-normal" data-auto="short_citation_title">The role of semantic diversity in lexical organization.<span class="updated-short-citation"> By: Jones, Michael N., Johns, Brendan T., Recchia, Gabriel, Canadian Journal of Experimental Psychology/Revue canadienne de psychologie expérimentale, 11961961, 20120601,  Vol. 66,  Issue 2</span></dd><dt class="medium-bold" data-auto="short_citation_long_dbname_label">Database: </dt><dd class="medium-normal" data-auto="short_citation_long_dbname">PsycARTICLES</dd></dl><div class="full-text-container border" data-auto="fulltext_container" xmlns:viewExtensions="http://www.ebscohost.com/schema/viewExtensions"><h2 class="hidden" data-auto="fulltext_title_hidden">HTML Full Text</h2><h2 data-auto="local_abody_title" class="ft-title border color-p4 bar4">The Role of Semantic Diversity in Lexical Organization</h2><div class="html-ft-toc" data-auto="html_toc"><h3 class="small-bold" id="toc" data-auto="html_toc_title">Contents</h3><ol data-auto="html_toc_list"><li class="link-medium" data-auto="html_toc_list_item"><a data-auto="ep_link" href="#cep-66-2-115-ID0EDCAA" id="hd_cep-66-2-115-ID0EDCAA" title="Experiment 1: A Role for Semantic Diversity">Experiment 1: A Role for Semantic Diversity</a></li><li data-auto="html_toc_list_item" class="link-medium html-toc-hd1"><a data-auto="ep_link" href="#cep-66-2-115-ID0EDDCAA" id="hd1_cep-66-2-115-ID0EDDCAA" title="Semantic Distinctiveness Count">Semantic Distinctiveness Count</a></li><li data-auto="html_toc_list_item" class="link-medium html-toc-hd1"><a data-auto="ep_link" href="#cep-66-2-115-ID0ECDCAA" id="hd1_cep-66-2-115-ID0ECDCAA" title="Method">Method</a></li><li data-auto="html_toc_list_item" class="link-medium html-toc-hd1"><a data-auto="ep_link" href="#cep-66-2-115-ID0EBDCAA" id="hd1_cep-66-2-115-ID0EBDCAA" title="Results">Results</a></li><li data-auto="html_toc_list_item" class="link-medium html-toc-hd1"><a data-auto="ep_link" href="#cep-66-2-115-ID0EADCAA" id="hd1_cep-66-2-115-ID0EADCAA" title="Discussion">Discussion</a></li><li class="link-medium" data-auto="html_toc_list_item"><a data-auto="ep_link" href="#cep-66-2-115-ID0ECCAA" id="hd_cep-66-2-115-ID0ECCAA" title="Experiment 2: Testing Diversity in an Artificial Language">Experiment 2: Testing Diversity in an Artificial Language</a></li><li data-auto="html_toc_list_item" class="link-medium html-toc-hd1"><a data-auto="ep_link" href="#cep-66-2-115-ID0ECCCAA" id="hd1_cep-66-2-115-ID0ECCCAA" title="Method">Method</a></li><li data-auto="html_toc_list_item" class="link-medium html-toc-hd1"><a data-auto="ep_link" href="#cep-66-2-115-ID0EBCCAA" id="hd1_cep-66-2-115-ID0EBCCAA" title="Results">Results</a></li><li data-auto="html_toc_list_item" class="link-medium html-toc-hd1"><a data-auto="ep_link" href="#cep-66-2-115-ID0EACCAA" id="hd1_cep-66-2-115-ID0EACCAA" title="Discussion">Discussion</a></li><li class="link-medium" data-auto="html_toc_list_item"><a data-auto="ep_link" href="#cep-66-2-115-ID0EBCAA" id="hd_cep-66-2-115-ID0EBCAA" title="A Computational Model of Semantic Diversity">A Computational Model of Semantic Diversity</a></li><li data-auto="html_toc_list_item" class="link-medium html-toc-hd1"><a data-auto="ep_link" href="#cep-66-2-115-ID0EDBCAA" id="hd1_cep-66-2-115-ID0EDBCAA" title="The Semantic Distinctiveness Model">The Semantic Distinctiveness Model</a></li><li data-auto="html_toc_list_item" class="link-medium html-toc-hd1"><a data-auto="ep_link" href="#cep-66-2-115-ID0ECBCAA" id="hd1_cep-66-2-115-ID0ECBCAA" title="Simulation 1: SDM Simulation of Experiment 1">Simulation 1: SDM Simulation of Experiment 1</a></li><li data-auto="html_toc_list_item" class="link-medium html-toc-hd1"><a data-auto="ep_link" href="#cep-66-2-115-ID0EBBCAA" id="hd1_cep-66-2-115-ID0EBBCAA" title="Simulation 2: SDM Simulation of Experiment 2">Simulation 2: SDM Simulation of Experiment 2</a></li><li data-auto="html_toc_list_item" class="link-medium html-toc-hd1"><a data-auto="ep_link" href="#cep-66-2-115-ID0EABCAA" id="hd1_cep-66-2-115-ID0EABCAA" title="Simulation 3: A Test of Semantic Similarity">Simulation 3: A Test of Semantic Similarity</a></li><li class="link-medium" data-auto="html_toc_list_item"><a data-auto="ep_link" href="#cep-66-2-115-ID0EACAA" id="hd_cep-66-2-115-ID0EACAA" title="General Discussion">General Discussion</a></li><li class="link-medium" data-auto="html_toc_list_item"><a data-auto="ep_link" href="#cep-66-2-115-ID0EAA" id="hd_cep-66-2-115-ID0EAA" title="Footnotes">Footnotes</a></li><li class="link-medium" data-auto="html_toc_list_item"><a data-auto="ep_link" href="#cep-66-2-115-ID0E0QB0ABAA" id="hd_cep-66-2-115-ID0E0QB0ABAA" title="References">References</a></li></ol></div><section id="TextToSpeech" class="full-text-content textToSpeechDataContainer" data-auto="text_to_speech" data-text-to-speech-cache-key="pdh_2012-15094-006" data-text-to-speech-title="The role of semantic diversity in lexical organization." data-text-to-speech-author="Jones, Michael N." data-text-to-speech-additional-filename="20120601"><span id="textToSpeechPlaceholder"> </span><div class="center" xmlns:Translation="urn:EBSCO-Translation"><strong>By: Michael N. Jones</strong><br /><em>Department of Psychological and Brain Sciences, Indiana University, Bloomington</em>;<br /><strong>Brendan T. Johns</strong><br /><em>Department of Psychological and Brain Sciences, Indiana University, Bloomington</em><br /><strong>Gabriel Recchia</strong><br /><em>Department of Psychological and Brain Sciences, Indiana University, Bloomington</em></div><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><strong>Acknowledgement: </strong>This research was supported by grants from Google Research and NSF BCS-1056744 to Michael N. Jones. Brendan T. Johns was supported by a postgraduate scholarship from NSERC.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">A consistent finding in studies of lexical access is that high-frequency words are identified faster than low-frequency words (<a href="#c8">Broadbent, 1967</a>; <a href="#c14">Forster &amp; Chambers, 1973</a>; <a href="#c22">Krueger, 1975</a>). The efficiency of processing suggests that high-frequency words have a privileged status over low-frequency words in the mental lexicon, as the effect is stable across different response tasks (e.g., word naming and 2AFC choices such as lexical, concreteness, and category decisions). Frequency is core to classic strength accounts of lexical access based on the assumption that each repetition increases memory strength for a word, boosting the efficiency of later access.<a id="b-fn1"> </a><sup><a href="#fn1" /></sup> This <em>principle of repetition</em> has been influential on formal models of lexical access, leading to the development of rank frequency models of the lexicon (<a href="#c30">Murray &amp; Forster, 2004</a>), threshold activation accounts (<a href="#c11">Coltheart, Rastle, Perry, Langdon, &amp; Ziegler, 2001</a>), and connectionist models (<a href="#c39">Seidenberg &amp; McClelland, 1989</a>).</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">However, recent research is questioning whether humans use frequency information to organise the lexicon. Word frequency may appear to be the organizing factor because it is confounded with a word's contextual diversity (CD)—the number of contexts in which the word has been experienced. But it may be a word's CD and not frequency that humans use to organise lexical priority. Unfortunately, CD is a slippery construct to define and takes on slightly different operational definitions across the handful of experiments that have studied it. Generally, CD is conceptualised as the number of distinct contexts in which a word occurs. If frequency is based on the principle of repetition, CD is based on the <em>principle of likely need</em> emphasised by rational models of memory (<a href="#c3">Anderson &amp; Milson, 1989</a>; <a href="#c4">Anderson &amp; Schooler, 1991</a>; <a href="#c12">Dennis &amp; Humphreys, 2001</a>): A word that has been experienced in many contexts during learning is more likely to be needed in unknown future contexts, hence it is more accessible in the lexicon. The variable is most commonly operationalized as the number of documents in which a word occurs across a text corpus (with no regard for frequency within documents). <a href="#c2">Adelman and Brown (2008)</a> summarise the theoretical position: “As words tend to cluster in contexts, the likely need of a word in an arbitrary new context relates to the number of contexts the word has been seen in before, not the number of occurrences of the word” (p. 223). This phenomenon is heavily related to the concept of word “burstiness” in information retrieval (<a href="#c21">Katz, 1996</a>). If humans are sensitive to word frequency information, then repeating a word should be beneficial to later identification of that word. If humans use CD information, however, then repeating a word is of limited use if the repetition is not also accompanied by a modulation in context.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><a href="#c38">Schwanenflugel and Shoben (1983)</a> originally demonstrated that a variable they termed “context availability” influences word recognition. They define this variable as the ease with which one can think of a particular circumstance in which a word might appear, and have argued that this is the “real” explanatory variable underlying the concreteness effect (cf. <a href="#c15">Galbraith &amp; Underwood, 1973</a>). In the recognition memory literature, robust evidence has been found that CD benefits item learning and retrieval efficiency. Items with a greater CD at encoding are more likely to be subsequently recognised and tend to be recognised faster (<a href="#c16">Goldinger &amp; Azuma, 2004</a>; <a href="#c28">McDonald &amp; Shillcock, 2001</a>; <a href="#c48">Nelson &amp; Shiffrin, 2006</a>; <a href="#c31">Pexman, Hargreaves, Siakaluk, Bodner, &amp; Pope, 2008</a>). <a href="#c43">Steyvers and Malmberg (2003)</a> have also demonstrated a role for CD as a strong component of the mirror effect of “frequency” seen in recognition memory. More recently, CD has been found to influence free recall as well (<a href="#c25">Lohnas, Polyn, &amp; Kahana, 2011</a>). Further, CD has been demonstrated to benefit learning of grammatical classes (<a href="#c34">Redington, Chater, &amp; Finch, 1998</a>), speech perception (<a href="#c5">Apfelbaum &amp; McMurray, 2011</a>), and word-referent mappings (<a href="#c42">Smith &amp; Yu, 2008</a>).</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">In a recent study, <a href="#c1">Adelman, Brown, and Quesada (2006)</a> conducted a corpus analysis of CD, operationalizing it as the number of documents in which a word occurs in large English corpora. They computed diversity and frequency counts for thousands of words and conducted a regression analysis to explore how the two measures predicted lexical decision and naming times for the words taken from <a href="#c6">Balota et al.'s (2002)</a> English Lexicon Database. Adelman et al. found clear evidence for the superiority of CD over word frequency in predicting word identification latency: CD predicted all variance in latency data that frequency did, and additional unique variance. They argue that previous theories have been constructed based on a false assumption that humans use frequency information to organise the lexicon, and their work suggests that many current models need to be abandoned or revised to adequately explain how the lexicon is organized.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">However, it is difficult to determine conclusively if CD is the organizing principle of lexical priority with the methodologies used in existing studies. First, it is questionable whether common operational definitions of CD are valid measures under the principle of likely need. The most common operational definition of CD is simply the number of documents in which a word appears in a text corpus: A word's frequency count is incremented each time it occurs in the corpus, but its diversity count is only incremented each time it occurs in a new document. This operationalization of frequency is fair, but operationalizing diversity as a document count is likely to be an invalid measure of the true contextual diversity of the word. Psychological notions of context differ greatly, and range from the list in which a word was encoded, to changes in time, to the room in which learning took place (<a href="#c37">Schmidt, 1991</a>; <a href="#c45">Verkoeijen, Rikers, &amp; Schmidt, 2004</a>; <a href="#c47">Wickens, 1987</a>). It is not directly obvious how counting documents corresponds to classic notions of a change in context, but it seems intuitive that if a document is repeated in the corpus, we should not consider the two repetitions to be different semantic contexts of the word. Further, a frequent discourse topic is likely to have many documents dedicated to it, and so a word that describes a frequent topic is likely to appear in more documents, even though the documents are not truly distinct contextual uses of the word. What is needed is a graded measure of CD by examining the information overlap among a word's linguistic contexts.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Second, the superiority of CD over frequency has only been demonstrated with regression analyses: CD accounts for all of the variance in identification latency that frequency does, and additional unique variance when frequency is partialed out (<a href="#c1">Adelman et al., 2006</a>). However, regression analyses alone do not provide conclusive evidence for the causal role of CD due to confounds with a variety of other variables in addition to frequency, any one of which could plausibly be the causal factor influencing lexical access. For example, access may simply be superior for words that have been experienced more recently; words with a greater CD or frequency are also likely to have a higher recency (but see <a href="#c7">Balota &amp; Spieler, 1999</a>). Ambiguity, abstractness, imageability, and word length are also confounded with document count and frequency, and are difficult to tease apart. Finally, it has been suggested (Balota; in <a href="#c1">Adelman et al., 2006</a>) that document count from a text corpus may actually be a better measure of real-world frequency due to the structure of the corpus. Recent corpus-based studies attempt to partial out the confounding variables as covariates. However, the effect of contextual diversity has never before been induced experimentally; to do so would require control over the statistical structure of the language being learned.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">The outline of this article is as follows: Experiment 1 addresses the issue of how to measure CD by introducing a graded <em>semantic distinctiveness count</em> and using a corpus analysis and large-scale fits to lexical decision and naming times to demonstrate its superiority over word frequency or document count. Experiment 2 demonstrates a causal effect of diversity using an artificial language paradigm to independently manipulate document count and semantic distinctiveness count. Finally, we introduce a computational model based on co-occurrence learning models and expectation-congruency, which adjusts its encoding strength for a word relative to the information redundancy between the current memorial representation of the word and the current linguistic context in which the word is experienced. Within the same model framework, we are able to compare semantic diversity with nested models considering only raw frequency or document count, and demonstrate the superiority of a semantic diversity learning mechanism in accounting for human word identification latency.</p><a id="cep-66-2-115-ID0EDCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><span class="medium-bold" xmlns:Translation="urn:EBSCO-Translation"><a data-auto="ep_link" href="#toc" id="hd_toc_16" title="Experiment 1: A Role for Semantic Diversity">Experiment 1: A Role for Semantic Diversity</a></span><br xmlns:Translation="urn:EBSCO-Translation" /><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><a href="#c1">Adelman et al. (2006)</a> demonstrated the superiority of document count over raw frequency in fitting lexical decision and naming times. However, operationalizing CD as a document count ignores semantic context—the information overlap between documents. Under the principle of likely need (<a href="#c3">Anderson &amp; Milson, 1989</a>), repetition of a word in distinct documents would increase its likely need to a greater extent than an equal number of repetitions in redundant documents. For example, if the word <em>bank</em> occurs in two very similar documents discussing the government-sponsored buyout of mortgage assets, we would consider the two documents to be very similar contextual uses of <em>bank</em> compared to the contextual similarity between one of these financial documents and a document discussing river banks. In addition, the government buyout may be a very frequently discussed discourse topic (having many documents on the topic), meaning that even though <em>bank</em> would receive a large document count, these are not truly distinct contextual uses of the word, hence it is a poor operational definition to be true to the principle of likely need. The example with a homograph like <em>bank</em> makes the point clear, but this pattern will be true of all words as a function of slight contextual modulations; a measure needs to consider the graded semantic coherence of the contexts in which words occur to estimate contextual diversity.</p><a id="cep-66-2-115-ID0EDDCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Semantic Distinctiveness Count</h4><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">To introduce a weighted context count, we first quantify the dissimilarity of any pair of documents in which a word occurs as a function of the proportion of overlapping words:
<a id="eq1"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/107bdbcb455f1928a8516225308924e2/571cd052/pdh/cep/cep-66-2-115-eq1a.gif" alt="cep-66-2-115-eq1a.gif" title=" " /><em> </em></span></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Document similarity is the intersection of the two sets of words, divided by the size of the smaller document. Function words (e.g., <em>the, is, of</em>, etc.) are filtered out of the calculation using the standard LSA stoplist (<a href="#c24">Landauer &amp; Dumais, 1997</a>). Document dissimilarity is then 1 − similarity. A dissimilarity value of 1 indicates that there is no content word overlap between the two documents (i.e., high distinctiveness), analogous to if it were presented in two separate lists in a standard memory experiment. A value of 0 indicates that the two documents are identical (i.e., low distinctiveness), similar to if the word were repeated in the same list in a standard memory experiment. A word's <em>semantic distinctiveness</em> is defined as the mean dissimilarity over all the documents in which it occurs. A word with a low mean distinctiveness tends to occur in documents that are semantically redundant, whereas a high mean distinctiveness indicates the set of documents that contain the word are semantically unique.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Finally, a word's <em>semantic distinctiveness count</em> (SD_Count) is computed to consider the number of documents in which it occurs weighted by the semantic uniqueness of those contexts. For the word set, the distribution of dissimilarity values is standardized to quantiles. In this article, we use septiles (dividing the cumulative distribution into seven equal bins) as previous research has pointed to seven as the optimal number of divisions to fit identification latencies (<a href="#c19">Johns &amp; Jones, 2008</a>). Document pairs that are in a higher quantile are more distinct than those in lower quantiles. For a given word, its SD_Count is calculated as the sum of the quantiles in which the set of documents containing it fall:
<a id="eq2"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/353569b4a388caeb3401a7f589a220d1/571cd052/pdh/cep/cep-66-2-115-eq2a.gif" alt="cep-66-2-115-eq2a.gif" title=" " /><em> </em></span></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Words that tend to appear in a greater number of unique documents will have a higher SD_Count. Note that if there were a single quantile, this function would be equal to a document count (incrementing the count by one each time the word appears in a new document with no regard for information overlap). When the distribution is split into more than one quantile, the function produces a greater count for more unique contextual uses of the word. Comparing two words that occur in an equal number of documents, the one that occurs in more redundant documents will have a lower SD_Count than the one that occurs in more distinct documents. If a word were to hypothetically only occur once in a document, and the document was repeated multiple times throughout the corpus, then the frequency count, document count, and SD_Count variables would all be equal. Using SD_Count, we have a variable that is sensitive to the uniqueness of semantic contexts in which words occur—more unique contexts are weighted more heavily than less unique contexts.</p><a id="cep-66-2-115-ID0ECDCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Method</h4><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">We computed word frequency, document count, and SD_Count from three corpora: (a) the Touchstone Applied Science Associates (TASA) corpus (<a href="#c24">Landauer &amp; Dumais, 1997</a>), (b) a Wikipedia corpus (<a href="#c33">Recchia &amp; Jones, 2009</a>), and (c) a New York Times (NYT) corpus (<a href="#c20">Jones &amp; Mewhort, 2004</a>). The TASA corpus was composed of 10,500 documents, with each document having a mean length of 289 words. The Wikipedia corpus was composed of 9,755 documents, with a mean document length of 391 words. The NYT corpus is composed of 9,100 documents with a mean length of 250 words. These are smaller versions of the full corpora, and the reduced size was necessary due to the computational complexity of this measure. Lexical decision times (LDTs) and naming times (NTs) were obtained from the English Lexicon Project (<a href="#c6">Balota et al., 2002</a>). Measures were computed for 17,984, 22,673, and 14,609 words for the TASA, WIKI, and NYT corpora, respectively.</p><a id="cep-66-2-115-ID0EBDCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Results</h4><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><a href="#fig1">Figure 1</a> illustrates a median split of words with high/low semantic distinctiveness (mean document dissimilarity) by high/low document count for the LDTs and the NYT corpus (the pattern was consistent across all corpora and with NTs as well). A main effect was observed for document count, with words occurring in more documents having faster LDTs than words occurring in fewer, <em>F</em>(1, 4,232) = 177.43, <em>p</em> &lt; .001. Further, a main effect was observed for semantic distinctiveness, with more semantically distinct words having faster LDTs than less distinct words, <em>F</em>(1, 4,232) = 143.78, <em>p</em> &lt; .001. Substituting word frequency for document count produces the same result (frequency and document count are highly correlated). Of particular interest is the finding that semantic distinctiveness and document count interacted, <em>F</em>(1, 4,232) = 74.26, <em>p</em> &lt; .001. As document count increases, words that occur in a greater number of semantically distinct documents see a greater benefit on their LDTs from the additional contextual occurrences.<br /><br /><a id="fig1"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/c391c76a8567688e5e12d6b569348427/571cd052/pdh/cep/cep-66-2-115-fig1a.gif" alt="cep-66-2-115-fig1a.gif" title="Figure 1. Factorial combination of semantic distinctiveness by document count on lexical decision times. Repetitions of a word in documents produces greater latency savings if the documents are low in semantic redundancy." /><em>Figure 1. Factorial combination of semantic distinctiveness by document count on lexical decision times. Repetitions of a word in documents produces greater latency savings if the documents are low in semantic redundancy.</em></span></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">However, <a href="#fig1">Figure 1</a> only demonstrates that words that occur on average in more unique documents have a reaction time (RT) savings in identification. <a href="#fig2">Figure 2</a> uses the SD_Count variable (a count of the documents in which the word occurs weighted by their semantic distinctiveness) to analyse the proportion of variance explained by SD_Count over word frequency (the zero point) and document count. The top panel shows LDT and the bottom panel NT. As the figure indicates, counting contexts relative their semantic uniqueness gives a much better account of the human data across all corpora for both LDT and NT. Because the variables are strongly correlated with each other, we emulate <a href="#c1">Adelman et al.'s (2006)</a> original regression analyses and examine variance predicted while systematically partialing out covariates. <a href="#tbl1">Table 1</a> shows the unique variance predicted by each variable for LDT and <a href="#tbl2">Table 2</a> for NT<a id="b-fn2"> </a><sup><a href="#fn2" /></sup>. As the tables indicate, the SD_Count variable gives a significantly better prediction of both LDT and NT than document count, <em>p</em> &lt; .001 in all cases, and subsumes the effect of frequency just as effectively as does document count.<br /><br /><a id="fig2"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/225afd95224d3f05a972e2a9f9b38267/571cd052/pdh/cep/cep-66-2-115-fig2a.gif" alt="cep-66-2-115-fig2a.gif" title="Figure 2. Increase in variance predicted over word frequency for lexical decision times (top panel) and naming times (bottom panel) for document count and SD_Count." /><em>Figure 2. Increase in variance predicted over word frequency for lexical decision times (top panel) and naming times (bottom panel) for document count and SD_Count.</em></span><br /><br /><a id="tbl1"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/798e9f9121d4d60174b206f2cc263491/571cd052/pdh/cep/cep-66-2-115-tbl1a.gif" alt="cep-66-2-115-tbl1a.gif" title="Lexical Decision Time Variance Predicted by SD_Count, Word Frequency, and Document Count" /><em>Lexical Decision Time Variance Predicted by SD_Count, Word Frequency, and Document Count</em></span><br /><br /><a id="tbl2"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/f716f1ff9c2c04760a510f088eca64cd/571cd052/pdh/cep/cep-66-2-115-tbl2a.gif" alt="cep-66-2-115-tbl2a.gif" title="Naming Time Variance Predicted by SD_Count, Word Frequency, and Document Count" /><em>Naming Time Variance Predicted by SD_Count, Word Frequency, and Document Count</em></span></p><a id="cep-66-2-115-ID0EADCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Discussion</h4><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">The corpus analysis suggests that when words with equivalent document counts are considered, those that occur in more semantically distinct contexts see a larger latency savings when compared to those that occur in redundant contexts. Our SD_Count variable follows the principle of likely need and corroborates the findings of <a href="#c1">Adelman et al. (2006)</a>, but clearly demonstrates that the lexical priority of a word depends on both the number and redundancy of the contexts in which it has been experienced. However, our regression analysis shares the same weaknesses with other correlational studies criticised in the introduction. Several confounding factors may still be the hidden causal variables (e.g., recency, concreteness, etc.). We next rule out these potential confounds by inducing the effect of diversity experimentally.</p><a id="cep-66-2-115-ID0ECCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><span class="medium-bold" xmlns:Translation="urn:EBSCO-Translation"><a data-auto="ep_link" href="#toc" id="hd_toc_36" title="Experiment 2: Testing Diversity in an Artificial Language">Experiment 2: Testing Diversity in an Artificial Language</a></span><br xmlns:Translation="urn:EBSCO-Translation" /><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Experiment 2 was designed to test the hypothesis that repetition of contextual occurrences produces greater latency savings for unique contexts than redundant contexts, as well as to compare the effect of contextual diversity on lexical decision times in a controlled paradigm. The CD effects used to support the principle of likely need have never been induced experimentally because in natural languages CD is confounded with many other sources of statistical information (<a href="#c28">McDonald &amp; Shillcock, 2001</a>). Thus, we used an artificial language paradigm to independently vary frequency/document count and semantic diversity, and to assess the relative contribution of each on identification latency.</p><a id="cep-66-2-115-ID0ECCCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Method</h4><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><strong>Participants</strong></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Thirty-two undergraduate students at Indiana University participated in the experiment for partial course credit. All had normal or corrected-to-normal vision.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><strong>Materials</strong></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Participants were trained in an artificial “alien” language referred to as Xaelon. The Xaelon lexicon consisted of a set of 12 one-syllable pronounceable nonwords selected from <a href="#c6">Balota et al.'s (2002)</a> database, equated for number of phonemes, number of letters, and orthographic neighborhood size. A set of 12 foils, to serve as negative examples during the lexical-decision task, was selected in the same way. The nonwords comprising the lexicon and the set of foils were selected so as to exhibit no significant differences in bigram count averages, bigram count sums (calculated by position as well as overall), or mean lexical decision latencies. To account for potential unforeseen differences in the processability of the lexicon compared with the foils, the set of nonwords that comprised the lexicon was swapped with the set of nonwords comprising the foils for half the participants.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">For each participant, a set of 450 training slides was created. Each training slide consisted of a three-word “sentence” in Xaelon above an image of a scene described by that sentence. Of the 12 words in the Xaelon lexicon, four were designated as <em>subject words</em>, four as <em>object words</em>, and four as <em>locatives</em>. Each subject word corresponded to a different unfamiliar image (“Fribbles”; <a href="#c44">Tarr, 2010</a>). Each object word corresponded to a different geometric shape constructed from geons, and each locative corresponded to a different position that the subject could be in relative to the object (above, below, to the left, or to the right of the object). Which words corresponded to which semantic designations were randomized for each participant.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Finally, of the 12 Xaelon words, four were randomly selected and crossed for a factorial combination of two levels of word frequency (hi/low) and two levels of semantic distinctiveness (hi/low). Note that in this experiment, a word's “document count” and its frequency are equivalent, as there are no repetitions within a context; hence we will just refer to repetitions as the frequency of Xaelon sentences in which the word occurs. Low-frequency words appeared 45 times each in the training slides, while high-frequency words appeared 180 times each. Further, whenever a low-diversity word appeared, it always appeared in the same semantic context (i.e., in the same sentence and with the same image), whereas each high-diversity word appeared in eight different semantic contexts (i.e., it could appear in any one of eight different sentences, each juxtaposed with its corresponding image). We selected nonwords and novel images so that participants could not simply translate the artificial language into English words for the subjects or objects, however, we did not attempt to create novel locatives.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><strong>Procedure</strong></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Participants were asked to imagine that they were explorers charged with the task of learning an alien language called Xaelon. Participants viewed 450 training trials, divided into 10 blocks of 45 images each. Training slides appeared in random order, and each slide was displayed for four seconds, with a 1-s intertrial interval. An example of a training trial is displayed in <a href="#fig3">Figure 3</a>. Following the training trials, participants were confronted with a surprise pseudolexical decision task (PLDT) in which they were told that they would be presented with several stimuli, some of which were words from the language that they had just learned, and some which were not. They were asked to press one key if the stimulus was part of the language they had just learned, and another key if it was not. Instructions stressed both speed and accuracy. Participants then completed 288 test trials, divided into 18 blocks of 16 trials each. Each trial consisted of a fixation cross for 500 ms, a blank screen for 200 ms, and finally either a foil or Xaelon word, which remained on the screen until the participant pressed one of the response keys. Exactly 12 examples of each Xaelon word and 12 examples of each foil were presented to participants during the lexical-decision task.<br /><br /><a id="fig3"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/0d9445090433324a85374c18c028c6c9/571cd052/pdh/cep/cep-66-2-115-fig3a.gif" alt="cep-66-2-115-fig3a.gif" title="Figure 3. Example of a training slide seen by participants while learning the “alien” language Xaelon." /><em>Figure 3. Example of a training slide seen by participants while learning the “alien” language Xaelon.</em></span></p><a id="cep-66-2-115-ID0EBCCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Results</h4><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Participants performed quite well at the PLDT task, with a mean accuracy of .88 (<em>SE</em> = .02) across all target and foil trials. We set a stringent accuracy criterion of 85%, which trimmed seven participants. The mean accuracy of the above-threshold participants was .94 (<em>SE</em> = .01). Latencies greater than 2.5 standard deviations from a participant's mean were removed; this resulted in 2.7% of latencies to be trimmed from the analysis. Response latencies did not differ as a function of part-of-speech (subject, locative, object), <em>F</em>(2, 23) = 0.11, <em>ns</em>.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><a href="#fig4">Figure 4</a> plots mean response latency as a function of frequency and semantic diversity. A repeated-measures ANOVA indicated no significant main effects for either frequency or semantic diversity, but a significant frequency-by-diversity interaction <em>F</em>(1, 24) = 4.37, <em>p</em> &lt; .05. Post hoc analyses (Bonferroni correction) revealed that the difference between the levels of diversity at low frequency was nonsignificant, <em>t</em>(24) = −1.54, <em>ns</em>, however, the difference between the levels of diversity at high frequency was statistically reliable, <em>t</em>(24) = 2.11, <em>p</em> &lt; .05. Further, the change in PLDT latency across the levels of frequency for low diversity was statistically flat, <em>t</em>(24) = −1.67, <em>ns</em>. However, the decrease in PLDT latency over frequency for high diversity was statistically significant, <em>t</em>(24) = 2.06, <em>p</em> &lt; .05. These results demonstrate that increasing the repetitions of a pseudoword from 45 to 180 produced no facilitation in PLDT if the contexts in which the word occurred were unchanged. However, processing savings were observed if the increase in frequency was accompanied by a change in contexts across learning.<br /><br /><a id="fig4"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/f6dbe7f3949abefad85b173de8cd4412/571cd052/pdh/cep/cep-66-2-115-fig4a.gif" alt="cep-66-2-115-fig4a.gif" title="Figure 4. Pseudo-lexical-decision latency in the Xaelon task as a function of token repetition frequency and semantic diversity. Item repetitions produced no detectable latency savings unless the repetitions were accompanied by a change in context." /><em>Figure 4. Pseudo-lexical-decision latency in the Xaelon task as a function of token repetition frequency and semantic diversity. Item repetitions produced no detectable latency savings unless the repetitions were accompanied by a change in context.</em></span></p><a id="cep-66-2-115-ID0EACCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Discussion</h4><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Consistent with the principle of likely need (and Experiment 1), Experiment 2 suggests that lexical access is facilitated for words appearing in a large number of contexts that are high in semantic distinctiveness. However, appearing in a large number of redundant contexts produced equivalent response latencies to a much lower number of repetitions in the redundant context. This finding parallels the results of our corpus analysis in Experiment 1, in which repetition of the word produced greater processing savings if the repetition was in a more semantically distinct context rather than if the repetition occurred in redundant contexts.</p><a id="cep-66-2-115-ID0EBCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><span class="medium-bold" xmlns:Translation="urn:EBSCO-Translation"><a data-auto="ep_link" href="#toc" id="hd_toc_60" title="A Computational Model of Semantic Diversity">A Computational Model of Semantic Diversity</a></span><br xmlns:Translation="urn:EBSCO-Translation" /><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Experiments 1 and 2 demonstrate that semantic diversity is an important source of information used by humans to organise the lexical priority of words. Both experiments point to an encoding operation by which the word is encoded most strongly if the current episodic context provides novel information about the word not already contained in memory. This conceptual framework is consistent with rational models (<a href="#c10">Chater &amp; Oaksford, 1997</a>) as well as <a href="#c36">Rosch's (1978)</a> notion of cognitive economy. It is also consistent with expectancy-congruency effects (e.g., <a href="#c18">Hirshman, 1988</a>; <a href="#c32">Ranganath &amp; Rainer, 2003</a>; <a href="#c37">Schmidt, 1991</a>)—unexpected or distinctive events are more memorable than expected occurrences. All of these well-established effects seem to have overlap with our semantic distinctiveness findings. However, we still lack a mechanistic explanation of how a process might give rise to the structure seen in studies of lexical access.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Contemporary computational models of lexical semantic similarity are also based on frequency (e.g., <a href="#c24">Landauer &amp; Dumais' 1997</a> Latent Semantic Analysis). Rather than single-token frequency, however, they depend on the co-occurrence frequency of words across a linguistic corpus. For example, the word <em>milk</em> may frequently co-occur in the same contexts as <em>drink</em> and <em>cookie</em>. As a result, it can be inferred that these words are semantically related. For reviews of the various models, see <a href="#c29">McRae and Jones (in press)</a> or <a href="#c35">Riordan and Jones (2011)</a>. Typically, these models first weight the word inversely proportional to its document entropy across the corpus. However, the weighting scheme is blind to the semantic content of those documents, which Experiments 1 and 2 suggest is an assumption incompatible with human encoding.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Co-occurrence models of lexical similarity also have the potential to account for lexical access. Typically, co-occurrence models learn from a word-by-document frequency matrix representation of a text corpus, and represent a word's meaning as a vector over semantic components. If the vectors for two words have a correlated pattern over components, they are similar. However, each word's vector also contains information about the word's individual frequency of occurrence as well (the magnitude of the vector), and this information is often discarded as a nuisance (<a href="#c13">Durda &amp; Buchanan, 2008</a>; <a href="#c40">Shaoul &amp; Westbury, 2006</a>). For example, if <a href="#c30">Murray and Forster's (2004)</a> model of lexical access based on rank frequency is correct, this information is already contained in the vector magnitude of a co-occurrence model—it just happens to be discarded when computing semantic similarity. Any vector contains both phase (direction) and magnitude (length). If a vector representation for a word is thought of abstractly as a “brain state” when the word is processed, then semantic similarity is the similarity of brain state phase patterns between two words, and lexical access is determined by the magnitude (intensity) of the brain state when the word is processed in isolation.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">We build our semantic distinctiveness model on a word-by-context matrix, in which each word is represented as a distribution over documents (either by frequency, document count, or weighted by semantic distinctiveness). This architecture allows a single model to be used to simultaneously account for single-word identification latency as well as paired-word semantic similarity data.</p><a id="cep-66-2-115-ID0EDBCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">The Semantic Distinctiveness Model</h4><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">The Semantic Distinctiveness Model (SDM) is based on other co-occurrence models of semantic memory, using a word-by-context matrix representation of a text corpus. However, the vector representation of a word is “grown” as the word is experienced in contexts. Each time a word is experienced in the corpus, the model compares the prediction of the word's current memory representation to the information in the current context. If the information in the current context is highly consistent with the current contents of memory, the context is encoded at a weaker magnitude. However, if the information in the context is novel compared to the current contents of memory, it is encoded at a much stronger magnitude. This process creates a representation consistent with our SD_Count variable. The same model framework may be used with a pure frequency or document count, allowing model comparisons from within the same formal framework.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">When a word is experienced in a document, each word in the document is retrieved from memory and its environmental context is represented as the sum of the vectors of the other words in the document (cf., in a list memory task, the item's context is the other items occurring in the list with it):
<a id="eq3"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/4b5af4a2a2b6081aabf686d58ec78678/571cd052/pdh/cep/cep-66-2-115-eq3a.gif" alt="cep-66-2-115-eq3a.gif" title=" " /><em> </em></span> where <em>n</em> is the number of words in the document, and <em>T</em><em>i</em> is the corresponding memory vector for a given word in the document (again, with function words removed).</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Next, the model assesses how similar the current contextual representation of the word is to its current memorial representation. The cosine (normalized dot product) is computed between the target word's contextual representation and its memory representation. If the cosine is relatively high, the current context is redundant with information already stored in memory; hence the current context is encoded at a lower weight. However, if the cosine is relatively low, the current context is more unique from information already stored in the word's memory vector; hence, the current context is encoded at a greater weight. The cosine is transferred through an inverse exponential density function (following <a href="#c41">Shepard's, 1987</a> universal law of similarity scaling) to reflect the current document's semantic distinctiveness (<em>SD</em>):
<a id="eq4"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/3c39b0e65be97bbd529ecf38551641e6/571cd052/pdh/cep/cep-66-2-115-eq4a.gif" alt="cep-66-2-115-eq4a.gif" title=" " /><em> </em></span> where λ is a fixed parameter with a positive value representing the slope of the similarity gradient. This <em>SD</em> value is then added into the memory matrix for the target word (row) in the specific context (column) of the matrix. A document count model can be considered to be nested within this model, with a λ fixed at 0.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">When a word is first encountered its memory vector will be empty, hence, the <em>SD</em> value will always be 1.0 for the first occurrence of a word, and it will be encoded at maximal strength. The second time a word is experienced, the similarity of this context is compared to the word's current lexical representation (which only contains the first context). If this is a repetition of the first document, the new context will be encoded at minimal strength. If, however, it is a context that is unique from the first, the new context will be encoded at maximal strength. In this fashion the word-by-document matrix has columns added to it each time a new document is learned, with the encoding strength for a document (for a particular word) dependent on the goodness-of-fit between what has been learned and what is being experienced.</p><a id="cep-66-2-115-ID0ECBCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Simulation 1: SDM Simulation of Experiment 1</h4><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">The SDM model was trained on the same corpora used in Experiment 1. The lambda parameter was fixed at 5.5 to encode <em>SD</em> for all following simulations, as preliminary work has suggested that fit to the human identification latency reaches asymptote at this value (similar to the septile in our SD_Count; see <a href="#c19">Johns &amp; Jones, 2008</a>). The analogue of the regression analysis conducted in Experiment 1 was conducted to assess the variance predicted in LDT and NT comparing a word's representation between versions of the model designed to attend to word frequency, document count, and <em>SD</em>.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">For each vector representation, a word's relative access was computed as the sum of its vector elements (magnitude). As in Experiment 1, log transforms of all variables were used, although the pattern reported is consistent across power and rank transforms as well. The other difference from Experiment 1 is that the models were trained on the full versions of the corpora: 37,600 documents from TASA, with an average length of 121 words per document, 40,000 documents from the Wikipedia corpus, with an average document length of 279 and 17,399 documents from The New York Times corpus, with an average document length of 250 words. LDT and NT data were again obtained from <a href="#c6">Balota et al.'s (2002)</a> database. In the analysis, latencies from 29,799, 35,518, and 20,744 words were used for the TASA, WIKI, and NYT corpora, respectively.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><a href="#tbl3">Tables 3</a> and <a href="#tbl4">4</a> show the increase in <em>R</em><sup>2</sup> for each version of the model for LDT and NT, respectively, while controlling for the variance accounted for by the other versions of the model (analogous to <a href="#tbl1">Tables 1</a> and <a href="#tbl2">2</a>). Similar to the corpus counts, the SDM model accounted for significantly more variance in both LDT and NT than models based on either frequency or document count. As was suggested by the corpus analysis, a mechanism that adjusts encoding strength relative to the amount of new information in a context not already contained in memory provides a better account of human single-word identification latency data.<br /><br /><a id="tbl3"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/14aa4cc0f2356a82684e0110d269de6d/571cd052/pdh/cep/cep-66-2-115-tbl3a.gif" alt="cep-66-2-115-tbl3a.gif" title="Lexical Decision Time Variance Predicted by SDM, Word Frequency, and Document Count Models" /><em>Lexical Decision Time Variance Predicted by SDM, Word Frequency, and Document Count Models</em></span><br /><br /><a id="tbl4"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/10186f1119ac00b6b9c8885d99290c7b/571cd052/pdh/cep/cep-66-2-115-tbl4a.gif" alt="cep-66-2-115-tbl4a.gif" title="Naming Time Variance Predicted by SDM, Word Frequency, and Document Count Models" /><em>Naming Time Variance Predicted by SDM, Word Frequency, and Document Count Models</em></span></p><a id="cep-66-2-115-ID0EBBCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Simulation 2: SDM Simulation of Experiment 2</h4><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">SDM was next used to simulate the artificial language data from Experiment 2. Xaelon sentences were considered to be distinct contexts, and a word-by-context matrix was constructed using <a href="#eq3">equations 3</a> and <a href="#eq4">4</a>. The exact sentences presented to the subjects in Experiment 2 were presented to the model, and predictions for PLDT were computed as the magnitude of a word's memory vector.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">The resulting behaviour of SDM is plotted in <a href="#fig5">Figure 5</a>.<a id="b-fn3"> </a><sup><a href="#fn3" /></sup> Note that neither a frequency nor document count version of the model could simulate this result by design of the stimuli—each would produce parallel effects rather than an interaction. SDM naturally produces a pattern similar to the behavioural data from Experiment 2: Words that were repeated in multiple distinct contexts produce a greater representational intensity than words that are repeated in redundant contexts. As with the human subjects, the simulation indicates that contextual repetitions of the word only benefit processing if the repetitions are accompanied by a change in semantic context. While this pattern could not be produced by a model that encoded frequency or contextual occurrences, it is a natural consequence of a mechanism that encodes words relative to their information overlap with what has already been stored.<br /><br /><a id="fig5"> </a><span class="centered"><img class="rs_skip" src="http://largecontent.ebsco-content.com/embimages/c832a2305432120e40362d9970b42a64/571cd052/pdh/cep/cep-66-2-115-fig5a.gif" alt="cep-66-2-115-fig5a.gif" title="Figure 5. SDM simulation of Experiment 2 (artificial language)." /><em>Figure 5. SDM simulation of Experiment 2 (artificial language).</em></span></p><a id="cep-66-2-115-ID0EABCAA" xmlns:Translation="urn:EBSCO-Translation"> </a><h4 xmlns:Translation="urn:EBSCO-Translation">Simulation 3: A Test of Semantic Similarity</h4><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Co-occurrence models of semantic similarity commonly use a word-by-document frequency matrix to determine lexical relatedness. However, the frequency assumption made by these co-occurrence models may also be wrong, and a simple document count or weighted semantic distinctiveness count may be the correct initial matrix representation to mimic human semantic similarities as well as lexical access. While many co-occurrence models apply a dimensional reduction mechanism (e.g., <a href="#c24">Landauer &amp; Dumais, 1997</a>), recent work suggests that raw co-occurrence counts from the original matrix may give a better approximations of human semantic similarity if simulated at a sufficiently large scale (<a href="#c26">Louwerse &amp; Connell, 2011</a>; <a href="#c9">Bullinaria &amp; Levy, 2007</a>; <a href="#c33">Recchia &amp; Jones, 2009</a>). Hence, we compare the cosines between specific word vectors learned by the raw frequency, document count, and <em>SD</em> versions of SDM here to explore how they map onto semantic similarities.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">For each model, predictions were made for the 45,786 word-pair similarities from <a href="#c27">Maki, McKinley, and Thompson (2004)</a>, computed from WordNet. Maki et al. have presented comparisons to human judgments of semantic similarity suggesting that the WordNet JCN metric is a very close correspondence to human similarity judgments. For each word pair, under each model representation (frequency, document count, <em>SD</em>), the predicted semantic similarity was simply the cosine of their respective vector representations. The <em>SD</em> version of the model gave a significantly better prediction of the WordNet semantic similarities (<em>r</em> = .172) than either frequency or document count (<em>r</em> = .126 and <em>r</em> = .161, respectively), and also outperformed <a href="#c24">Landauer and Dumais' (1997)</a> LSA model, which produced <em>r</em> = .158.<a id="b-fn4"> </a><sup><a href="#fn4" /></sup></p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">While semantic similarity among words is not the focus of this article, this simulation does demonstrate that the incorrect assumption of frequency in lexical access may have also been falsely applied to co-occurrence models of semantic similarity, and semantic diversity may be the correct source of information underlying both. Semantic diversity can be learned by a simple mechanism within a context co-occurrence framework and, as a byproduct, it also seems to produce a better organized semantic space. In this manner, we can represent both lexical access and lexical similarity within the same model, allowing insights into how single- and paired-word tasks are related to the same memorial structure.</p><a id="cep-66-2-115-ID0EACAA" xmlns:Translation="urn:EBSCO-Translation"> </a><span class="medium-bold" xmlns:Translation="urn:EBSCO-Translation"><a data-auto="ep_link" href="#toc" id="hd_toc_87" title="General Discussion">General Discussion</a></span><br xmlns:Translation="urn:EBSCO-Translation" /><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Experiments 1 and 2 provide converging evidence from both a megastudy (the corpus analysis) and a controlled microstudy (the experimentally induced artificial language). Both the mega and micro seem to point to the same pattern of behavioural data: Repetition of a word produces greater processing savings if the repetition is accompanied by a change in semantic context. Our findings corroborate recent evidence from others (e.g., <a href="#c1">Adelman et al., 2006</a>; <a href="#c31">Pexman et al., 2008</a>) suggesting that CD is potentially a more important variable than is frequency in word recognition and memory access. But it also makes a clear case for the importance of semantic context. The interaction of repetition and semantic redundancy found in both experiments is difficult to account for with most existing models of word identification.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Our semantic distinctiveness model implements this pattern by using an expectancy-congruency mechanism to build a word-by-context co-occurrence matrix: The encoding strength for a word in a given context is relative to the information overlap between the context and the current memorial representation of the word. This mechanism is very similar in principle to models that adjust attention across learning to dimensions that are more diagnostic (e.g., <a href="#c23">Kruschke, 1992</a>). In addition, newer models of lexical access, such as <a href="#c46">Wagenmakers et al.'s (2004)</a> REM-LD model, are sensitive to contextual variability and seem promising candidates to explain these findings.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Finally, our SDM implementation of a word-by-context matrix not only outperformed matrices based on frequency and document count, but the resulting matrix seemed to produce better semantic organisation as well, a free lunch we were not explicitly trying to create. This pattern bolsters the importance of semantic distinctiveness over frequency or document count, and points to an important connection between models of lexical access (based on single-word statistics) and lexical similarity (based on co-occurrence statistics).</p><a id="cep-66-2-115-ID0EAA" xmlns:Translation="urn:EBSCO-Translation"> </a><span class="medium-bold" xmlns:Translation="urn:EBSCO-Translation"><a data-auto="ep_link" href="#toc" id="hd_toc_92" title="Footnotes">Footnotes</a></span><a id="fn1" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><sup><a href="#b-fn1">1</a></sup> Alternatively, repetition may affect the number of instances of a word in memory, increasing availability, as in popular multiple-trace models (e.g., <a href="#c17">Hintzman, 1986</a>).</p><a id="fn2" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><sup><a href="#b-fn2">2</a></sup> Following <a href="#c1">Adelman et al. (2006)</a>, we use log transformations of each predictor in the regression. The ordinal pattern summarized in <a href="#tbl1">Tables 1</a> and <a href="#tbl2">2</a> is the same with either power or rank transformations.</p><a id="fn3" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><sup><a href="#b-fn3">3</a></sup> Note that the scale on the ordinate has been changed to negative magnitude to be consistent with <a href="#fig4">Figure 4</a>. Vector magnitude (word intensity) is negatively correlated with identification latency.</p><a id="fn4" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><sup><a href="#b-fn4">4</a></sup> Note that with 45,786 pairs, all numeric differences between correlation coefficients are significant.</p><a id="cep-66-2-115-ID0E0QB0ABAA" xmlns:Translation="urn:EBSCO-Translation"> </a><span class="medium-bold" xmlns:Translation="urn:EBSCO-Translation"><a data-auto="ep_link" href="#toc" id="hd_toc_102" title="References">References</a></span><a id="c1" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Adelman, J. S., Brown, G. D. A., &amp; Quesada, J. F. (2006). Contextual diversity, not word frequency, determines word-naming and lexical decision time. <em>Psychological Science</em>, <em>17</em>, 814–823. doi:10.1111/j.1467-9280.2006.01787.x</p><a id="c2" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Adelman, J. S., &amp; Brown, G. D. A. (2008). Modeling lexical decision: The form of frequency and diversity effects. <em>Psychological Review</em>, <em>115</em>, 214–227. doi:10.1037/0033-295X.115.1.214</p><a id="c3" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Anderson, J. R., &amp; Milson, R. (1989). Human memory: An adaptive perspective. <em>Psychological Review</em>, <em>96</em>, 703–719. doi:10.1037/0033-295X.96.4.703</p><a id="c4" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Anderson, J. R., &amp; Schooler, L. J. (1991). Reflections of the environment in memory. <em>Psychological Science</em>, <em>2</em>, 396–408. doi:10.1111/j.1467-9280.1991.tb00174.x</p><a id="c5" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Apfelbaum, K. S., &amp; McMurray, B. (2011). Using variability to guide dimensional weighting: Associative mechanisms in early word learning. <em>Cognitive Science</em>, <em>35</em>, 1105–1138. doi:10.1111/j.1551-6709.2011.01181.x</p><a id="c6" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Balota, D. A., Cortese, M. J., Hutchinson, K. A., Neely, J. H., Nelson, D., Simpson, G. B., &amp; Treiman, R. (2002). <em>The English Lexicon Project</em>. Retrieved September 30, 2007 from <a href="http://elexicon.wustl.edu/" target="_blank">http://elexicon.wustl.edu/</a></p><a id="c7" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Balota, D. A., &amp; Spieler, D. H. (1999). Word frequency, repetition, and lexicality effects in word recognition tasks: Beyond measures of central tendency. <em>Journal of Experimental Psychology: General</em>, <em>128</em>, 32–55. doi:10.1037/0096-3445.128.1.32</p><a id="c8" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Broadbent, D. E. (1967). Word-frequency effect and response bias. <em>Psychological Review</em>, <em>74</em>, 1–15. doi:10.1037/h0024206</p><a id="c9" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Bullinaria, J. A., &amp; Levy, J. P. (2007). Extracting semantic representations from word co-occurrence statistics: A computational study. <em>Behavior Research Methods</em>, <em>39</em>, 510–526. doi:10.3758/BF03193020</p><a id="c10" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Chater, N., &amp; Oaksford, M. (1997). Ten years of the rational analysis of memory. <em>Trends in Cognitive Science</em>, <em>3</em>, 57–65. doi:10.1016/S1364-6613(98)01273-X</p><a id="c11" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Coltheart, M., Rastle, K., Perry, C., Langdon, R., &amp; Ziegler, J. (2001). DRC: A dual route cascaded model of visual word recognition and reading aloud. <em>Psychological Review</em>, <em>108</em>, 204–256. doi:10.1037/0033-295X.108.1.204</p><a id="c12" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Dennis, S., &amp; Humphreys, M. S. (2001). A context noise model of episodic word recognition. <em>Psychological Review</em>, <em>108</em>, 452–478. doi:10.1037/0033-295X.108.2.452</p><a id="c13" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Durda, K., &amp; Buchanan, L. (2008). Windsor improved norms of distance and similarity of representations of semantics. <em>Behavior Research Methods</em>, <em>40</em>, 705–712. doi:10.3758/BRM.40.3.705</p><a id="c14" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Forster, K. I., &amp; Chambers, S. M. (1973). Lexical access and naming time. <em>Journal of Verbal Learning and Verbal Behavior</em>, <em>12</em>, 627−635.</p><a id="c15" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Galbraith, R. C., &amp; Underwood, B. J. (1973). Perceived frequency of concrete and abstract words. <em>Memory &amp; Cognition</em>, <em>1</em>, 56–60. doi:10.3758/BF03198068</p><a id="c16" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Goldinger, S. D., &amp; Azuma, T. (2004). Episodic memory reflected in printed word naming. <em>Psychonomic Bulletin &amp; Review</em>, <em>11</em>, 716–722. doi:10.3758/BF03196625</p><a id="c17" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Hintzman, D. L. (1986). “Schema Abstraction” in a multiple-trace memory model. <em>Psychological Review</em>, <em>93</em>, 411–428. doi:10.1037/0033-295X.93.4.411</p><a id="c18" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Hirshman, E. (1988). The expectation-violation effect: Paradoxical effects of semantic relatedness. <em>Journal of Memory and Language</em>, <em>27</em>, 40–58. doi:10.1016/0749-596X(88)90047-2</p><a id="c19" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Johns, B. T., &amp; Jones, M. N. (2008). Predicting word-naming and lexical decision times from a semantic space model. In V.Sloutsky, B.Love, &amp; K.McRae (Eds.), <em>Proceedings of the 30th Cognitive Science Society Meeting</em>, 279–284.</p><a id="c20" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Jones, M. N., &amp; Mewhort, D. J. K. (2004). Case-sensitive letter and bigram frequency counts from large-scale English corpora. <em>Behavior Research Methods, Instruments, and Computers</em>, <em>36</em>, 388–396. doi:10.3758/BF03195586</p><a id="c21" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Katz, S. M. (1996). Distribution of content words and phrases in text and language modeling. <em>Natural Language Engineering</em>, <em>2</em>, 15–59. doi:10.1017/S1351324996001246</p><a id="c22" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Krueger, L. E. (1975). Familiarity effects in visual information processing. <em>Psychological Bulletin</em>, <em>82</em>, 949–974. doi:10.1037/0033-2909.82.6.949</p><a id="c23" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Kruschke, J. K. (1992). ALCOVE: An exemplar-based connectionist model of category learning. <em>Psychological Review</em>, <em>99</em>, 22–44. doi:10.1037/0033-295X.99.1.22</p><a id="c24" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Landauer, T. K., &amp; Dumais, S. T. (1997). A solution to Plato's problem: The latent semantic analysis theory of the acquisition, induction, and representation of knowledge. <em>Psychological Review</em>, <em>104</em>, 211–240. doi:10.1037/0033-295X.104.2.211</p><a id="c25" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Lohnas, L. J., Polyn, S. M., &amp; Kahana, M. J. (2011). Contextual variability in free recall. <em>Journal of Memory and Language</em>, <em>64</em>, 249–255. doi:10.1016/j.jml.2010.11.003</p><a id="c26" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Louwerse, M. M., &amp; Connell, L. (2011). Symbol interdependency in symbolic and embodied cognition. <em>Cognitive Science</em>, <em>35</em>, 381–398. doi:10.1111/j.1551-6709.2010.01157.x</p><a id="c27" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Maki, W. S., McKinley, L. N., &amp; Thompson, A. G. (2004). Semantic distance norms computed from an electronic dictionary (WordNet). <em>Behavior Research Methods, Instruments, &amp; Computers</em>, <em>36</em>, 421–431. doi:10.3758/BF03195590</p><a id="c28" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">McDonald, S. A., &amp; Shillcock, R. C. (2001). Rethinking the word frequency effect: The neglected role of distributional information in lexical processing. <em>Language and Speech</em>, <em>44</em>, 295–323. doi:10.1177/00238309010440030101</p><a id="c29" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">McRae, K., &amp; Jones, M. N. (in press). Semantic memory. In D.Reisberg (Ed.), <em>The Oxford handbook of cognitive psychology</em> (pp.).</p><a id="c30" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Murray, W. S., &amp; Forster, K. I. (2004). Serial Mechanisms in lexical access: The rank hypothesis. <em>Psychological Review</em>, <em>111</em>, 721–756. doi:10.1037/0033-295X.111.3.721</p><a id="c48" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Nelson, A. B., &amp; Shiffrin, R. M. (2006). Modeling the effects of induced frequency. In R.Sun and N.Miyake (Eds.), <em>Proceedings of the 28th Conference of the Cognitive Science Society</em> (p. 2568).</p><a id="c31" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Pexman, P., Hargreaves, I., Siakaluk, P., Bodner, G., &amp; Pope, J. (2008). There are many ways to be rich: Effects of three measures of semantic richness on visual word recognition. <em>Psychonomic Bulletin &amp; Review</em>, <em>15</em>, 161–167. doi:10.3758/PBR.15.1.161</p><a id="c32" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Ranganath, C., &amp; Rainer, G. (2003). Neural mechanisms for detecting and remembering novel events. <em>Nature Reviews Neuroscience</em>, <em>4</em>, 193–202. doi:10.1038/nrn1052</p><a id="c33" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Recchia, G., &amp; Jones, M. N. (2009). More data trumps smarter algorithms: Comparing pointwise mutual information with latent semantic analysis. <em>Behavior Research Methods</em>, <em>41</em>, 647–656. doi:10.3758/BRM.41.3.647</p><a id="c34" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Redington, M., Chater, N., &amp; Finch, S. (1998). Distributional information: A powerful cue for acquiring syntactic information. <em>Cognitive Science</em>, <em>22</em>, 425–469. doi:10.1207/s15516709cog2204_2</p><a id="c35" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Riordan, B., &amp; Jones, M. N. (2011). Redundancy in linguistic and perceptual experience: Comparing distributional and feature-based models of semantic representation. <em>Topics in Cognitive Science</em>, <em>3</em>, 303–345. doi:10.1111/j.1756-8765.2010.01111.x</p><a id="c36" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Rosch, E. (1978). Principles of categorization. In E.Rosch &amp; B. B.Lloyd (Eds.), <em>Cognition and categorization</em> (pp. 27–48). Hillsdale, NJ: Erlbaum.</p><a id="c37" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Schmidt, S. R. (1991). Can we have a distinctive theory of memory?<em>Memory &amp; Cognition</em>, <em>19</em>, 523–542. doi:10.3758/BF03197149</p><a id="c38" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Schwanenflugel, P. J., &amp; Shoben, E. J. (1983). Differential context effects in the comprehension of abstract and concrete verbal materials. <em>Journal of Experimental Psychology: Learning, Memory, and Cognition</em>, <em>9</em>, 82–102. doi:10.1037/0278-7393.9.1.82</p><a id="c39" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Seidenberg, M. S., &amp; McClelland, J. L. (1989). A distributed, developmental model of word recognition and naming. <em>Psychological Review</em>, <em>111</em>, 721–756.</p><a id="c40" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Shaoul, C., &amp; Westbury, C. (2006). Word frequency effects in high dimensional co-occurrence models: A new approach. <em>Behavior Research Methods</em>, <em>38</em>, 190–195. doi:10.3758/BF03192768</p><a id="c41" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Shepard, R. N. (1987). Toward a universal law of generalization for psychological science. <em>Science</em>, <em>237</em>, 1317–1323. doi:10.1126/science.3629243</p><a id="c42" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Smith, L. B., &amp; Yu, C. (2008). Infants rapidly learn word-referent mappings via cross-situational statistics. <em>Cognition</em>, <em>106</em>, 333–338. doi:10.1016/j.cognition.2007.06.010</p><a id="c43" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Steyvers, M., &amp; Malmberg, K. J. (2003). The effect of normative context variability on recognition memory. <em>Journal of Experimental Psychology: Learning, Memory, and Cognition</em>, <em>29</em>, 760–766. doi:10.1037/0278-7393.29.5.760</p><a id="c44" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Tarr, M. J. (2010). <em>“Fribble” images courtesy of Michael J. Tarr</em>, Center for the Neural Basis of Cognition and Department of Psychology, Carnegie Mellon University, Pittsburgh, PA. Retrieved from <a href="http://www.tarrlab.org/" target="_blank">http://www.tarrlab.org/</a></p><a id="c45" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Verkoeijen, P. P. J. L., Rikers, R. M. J. P., &amp; Schmidt, H. G. (2004). Detrimental influence of contextual change on spacing effects in free recall. <em>Journal of Experimental Psychology: Learning, Memory, &amp; Cognition</em>, <em>30</em>, 796–800. doi:10.1037/0278-7393.30.4.796</p><a id="c46" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Wagenmakers, E. J., Steyvers, M., Raaijmakers, J. G. W., Shiffrin, R. M., van Rijn, H., &amp; Zeelenberg, R. (2004). A model for evidence accumulation in the lexical decision task. <em>Cognitive Psychology</em>, <em>48</em>, 332–367. doi:10.1016/j.cogpsych.2003.08.001</p><a id="c47" xmlns:Translation="urn:EBSCO-Translation"> </a><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation">Wickens, D. D. (1987). The dual meanings of context: Implications for research, theory, and applications. In D. S.Gorfein &amp; R. R.Hoffman (Eds.), <em>Memory and learning: The Ebbinghaus Centennial Conference</em> (pp. 135–152). Hillsdale, NJ: Erlbaum.</p><p class="body-paragraph" data-auto="body_paragraph" xmlns:Translation="urn:EBSCO-Translation"><em>Submitted: </em>August 23, 2011<em> Accepted: </em>November 28, 2011</p><hr noshade="noshade" /><p class="body-paragraph" data-auto="copyright_info">This publication is protected by US and international copyright laws and its content may not be copied without the copyright holders express written permission except for the print or download capabilities of the retrieval software used for access. This content is intended solely for the use of the individual user.<br xmlns:ExtendedMarkupController="urn:ExtendedMarkupController" /><br xmlns:ExtendedMarkupController="urn:ExtendedMarkupController" /><strong xmlns:ExtendedMarkupController="urn:ExtendedMarkupController">Source:&nbsp;</strong>Canadian Journal of Experimental Psychology. Vol. 66. (2), Jun, 2012 pp. 115-124)<br xmlns:ExtendedMarkupController="urn:ExtendedMarkupController" /><strong xmlns:ExtendedMarkupController="urn:ExtendedMarkupController">Accession Number:&nbsp;</strong>2012-15094-006<br xmlns:ExtendedMarkupController="urn:ExtendedMarkupController" /><strong xmlns:ExtendedMarkupController="urn:ExtendedMarkupController">Digital Object Identifier:&nbsp;</strong>10.1037/a0026727</p></section></div>
		

		<div class="widget-loading loading"></div>
	
		<!-- WorldCat Widgets-->
		

	<!-- Full text will be rendered in this placeholder if citation is being displayed with
	full text. -->
	
	
	

	<div class="content-footer" >
	 

	</div>
	
	<div class="rs-placeholder" id="ctl00_ctl00_MainContentArea_MainContentArea_speaker_box" style="display:none;" data-parent="textToSpeechPlaceholder" data-readid="TextToSpeech" data-speed="MEDIUM" data-voice="ScanSoft_Jill_Full_22kHz" data-server="http://app.rs.ebscohost.com/cgi-bin/rsent?customerid=5845" data-download="true" data-isdetail="true"> </div>



						</div>
					</div>
					<div id="column1" class="collapsible" >
	<a class="collapsible-toggle" href="#" ></a>
	<div class="collapsible-content">
		
	


<h3 class="vis-hidden">View:</h3>
<ul class="format-control" >
		<li id="ctl00_ctl00_Column1_Column1_formatButtonsTop_formatButtonRepeater_ctl01_listItem" class="format-item">
			 
			<!-- Making assumption that we don't want spaces between MARC link and parenthesis. -->
			
			
			<a id="ctl00_ctl00_Column1_Column1_formatButtonsTop_formatButtonRepeater_ctl01_linkButton" title="Detailed Record" class="record-type format-citation" href="javascript:__doPostBack(&#39;ctl00$ctl00$Column1$Column1$formatButtonsTop$formatButtonRepeater$ctl01$linkButton&#39;,&#39;&#39;)">Detailed Record</a>
			
			
			
		</li>
	
		<li id="ctl00_ctl00_Column1_Column1_formatButtonsTop_formatButtonRepeater_ctl02_listItem" class="format-item active">
			 
			<!-- Making assumption that we don't want spaces between MARC link and parenthesis. -->
			
			
			
			<span id="ctl00_ctl00_Column1_Column1_formatButtonsTop_formatButtonRepeater_ctl02_label" title="HTML Full Text" class="record-type html-ftwg">HTML Full Text</span>
			
			
		</li>
	
		<li id="ctl00_ctl00_Column1_Column1_formatButtonsTop_formatButtonRepeater_ctl03_listItem" class="format-item">
			 
			<!-- Making assumption that we don't want spaces between MARC link and parenthesis. -->
			
			<span id="ctl00_ctl00_Column1_Column1_formatButtonsTop_formatButtonRepeater_ctl03_hddnInstructionMessage" class="hidden">This PDF document opens in a frame, to view the document outside of a frame, please change your Adobe Reader settings. To do this, open Adobe Reader, go to Help Menu and select Accessibility Setup Assistant option then select Use Recommend Settings and Skip Setup. You only need to do this once with the current computer you are using.</span>
			<a id="ctl00_ctl00_Column1_Column1_formatButtonsTop_formatButtonRepeater_ctl03_linkButton" title="PDF Full Text" class="record-type pdf-ft" href="javascript:__doPostBack(&#39;ctl00$ctl00$Column1$Column1$formatButtonsTop$formatButtonRepeater$ctl03$linkButton&#39;,&#39;&#39;)">PDF Full Text</a>
			
			
			<span id="ctl00_ctl00_Column1_Column1_formatButtonsTop_formatButtonRepeater_ctl03_suffix" class="format-note">(358.6KB)</span>
		</li>
	</ul>
	

	
	<div id="citedExternalSources"  style="display:none;">
		
	</div>
	

	
	
	
	
	</div>
</div>
					<div role="complementary" id="column2" class="collapsible" >
	<a class="collapsible-toggle" href="#" ></a>
	<div class="collapsible-content" >
		
	<hr class="vis-none" />
<h2 title="Tools" accesskey="5" tabindex="0" class="article-tools-header" id="ArticleTools"  >Tools</h2>
<ul class="article-tools delivery-control" >
		<li class="article-tool" >
			<a   href="#" title="Print" class="print-link"    data-panel='{"Id":"print","Url":"delivery/printpanel","Js":"ep/controller/control/citationdeliverypanel.js"}' >Print</a>
		</li>
		<li class="article-tool" >
			<a   href="#" title="E-mail" class="email-link"    data-panel='{"Id":"email","Url":"delivery/emailpanel","Js":"ep/controller/control/citationdeliverypanel.js"}' >E-mail</a>
		</li>
		<li class="article-tool" >
			<a   href="#" title="Save" class="save-link"    data-panel='{"Id":"save","Url":"delivery/savepanel","Js":"ep/controller/control/citationdeliverypanel.js"}' >Save</a>
		</li>
		<li class="article-tool" >
			<a   href="#" title="Cite" class="cite-link"    data-panel='{"Id":"cite","Url":"delivery/citepanel","Js":"ep/controller/control/citepanel.js"}' >Cite</a>
		</li>
		<li class="article-tool" >
			<a   href="#" title="Export" class="export-link"    data-panel='{"Id":"export","Url":"/ehost/delivery/exportpanel?sid=3f1a0278-d466-4092-b2fa-b3ec663b4f99@sessionmgr4004\u0026vid=0\u0026form=False","Js":"ep/controller/control/exportpanel.js"}' >Export</a>
		</li>
		<li class="article-tool" >
			<a   href="#" title="Permalink" class="permalink-link"    data-panel='{"Id":"permalink","Url":"delivery/permalinkpanel","Js":"ep/controller/control/plinkpanel.js"}' >Permalink</a>
		</li>
		<li class="article-tool" >
			<a   href="#" title="Share" class="bookmark-link"    data-panel='{"Id":"bookmark","Url":"addthis/addthispanel","Js":"ep/controller/control/bookmarkpanel.js"}' >Share</a>
		</li>

</ul>

	</div>
</div>
				
					
				
				<div class="extra1" role="presentation">&nbsp;</div>
			</div>
			<div class="footer-wrapper" >
				
	

				<div class="push-sticky-footer"></div>
			</div>
		</div>
		
	

				
		


	</form>
	
</body>
</html>
