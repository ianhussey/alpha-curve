---
title: "An excess of Cronbach's Î± values at rule-of-thumb cut-offs"
subtitle: "Analyses of I/O dataset"
author: "Ian Hussey"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: hide
    highlight: haddock
    theme: flatly
    toc: yes
    toc_float: yes
---

Late in the analysis process we realised there was overlap between the two datasets: two journals (Journal of Applied Psychology and Journal of Occupational Health Psychology) were present in both datasets, although metaBUS contains a wider range of years. As such, we also present analyses that excluded DOIs from the I/O dataset that had already been used in the psych dataset. This resulted in 9090 alpha estimates (10.1%) being excluded from the I/O dataset.

```{r setup, include=FALSE}

knitr::opts_chunk$set(message = FALSE, 
                      warning = FALSE)

```

# Dependencies

```{r}

library(tidyverse)
library(scales)
library(ggtext)
library(broom)
library(extraDistr)
library(patchwork)
library(coin)
library(knitr)
library(kableExtra)

dir.create("plots")

# disable scientific notation
options(scipen=999)

# set seed
set.seed(42)

```

# Get data

```{r}

#data_processed <- read_rds("../../data/processed/IO/data_processed_io.rds")
data_processed <- read_rds("../../data/processed/IO/data_processed_io_no_overlap_with_psych.rds")

data_processed_binned <- data_processed %>%
  # round the alpha values to 2 decimal places, using standard rounding method
  # recode alpha values of 0 or 1 to .01 or .99, in order to be able to fit beta regression
  # nb none present in dataset
  mutate(alpha = janitor::round_half_up(alpha, 2),
         alpha = case_when(alpha == 0 ~ 0.01,
                           alpha == 1 ~ 0.99,
                           TRUE ~ alpha)) 

```

# Descriptives

```{r}

data_metabus <- read_rds("../../data/processed/IO/data_processed_io.rds")

n_doi <- nrow(data_metabus)

cat("n articles =", 
      n_doi, "\n")


n_estimates_after_exclusions <- data_processed |>
  count(name = "n_estimates") |>
  pull(n_estimates)

cat("n alpha estimates after exclusions =", 
      n_estimates_after_exclusions, "\n")

```

# Kernel smoothing

## Model and plot

```{r}

# create tibble containing all alpha bins, for a later join
all_alphas <- tibble(alpha = as.character(seq(0.01, 0.99, 0.01)),
                     freq = 0,
                     missing = TRUE)

data_frequency_temp <- data_processed_binned %>% 
  mutate(n_total = n()) %>% 
  group_by(alpha) %>% 
  summarise(freq = sum(!is.na(alpha))/first(n_total)) %>%
  # convert alpha to character to facilitate a join later
  mutate(alpha = as.character(alpha))

# kernel density estimation
data_density <- 
  density(data_processed_binned %>%
            pull(alpha),
          n      = length(seq(0.01, 0.99, 0.01)),
          from   = 0.01,
          to     = 0.99, 
          kernel = "gaussian",
          bw     = 0.01) # "nrd0" in the psychology dataset returns 0.01

data_frequency <-
  # join the previous two tibbles to create a complete list of alpha bins
  data_frequency_temp %>%
  bind_rows(all_alphas %>% anti_join(data_frequency_temp, by = "alpha")) %>%
  arrange(alpha, desc(freq)) %>%
  distinct(alpha, .keep_all = TRUE) %>%
  mutate(density_kernel = data_density$y,
         missing = ifelse(is.na(missing), FALSE, missing)) %>%
  mutate(residual_kernel = freq*100 - density_kernel,
         alpha = as.numeric(alpha),
         missing = ifelse(is.na(missing), FALSE, missing))

data_diff <- data_frequency %>%
  mutate(frequency = freq*100,
         frequency_without_residual_kernel = if_else(residual_kernel > 0, frequency - residual_kernel, frequency),
         pos_resid_kernel = residual_kernel > 0,
         residual_kernel = if_else(residual_kernel > 0, residual_kernel, -1 * residual_kernel)) %>%
  dplyr::select(alpha,
                #fitted,
                density_kernel,
                pos_resid_kernel,
                frequency_without_residual_kernel,
                residual_kernel) %>%
  pivot_longer(names_to = "type",
               values_to = "residual_kernel",
               c(frequency_without_residual_kernel, residual_kernel))


total_n <- nrow(data_processed_binned)/100

plot_data_io <- data_processed_binned |>
  mutate(n_total = n()) |>
  group_by(alpha) |>
  summarise(count = sum(!is.na(alpha))) |>
  mutate(type = case_when(alpha %in% c(.70, .80, .90) ~ "Common reliability threshold",
                          TRUE ~ "Non-threshold"),
         type = fct_relevel(type, "Non-threshold", "Common reliability threshold")) |>
  right_join(data_frequency, by = "alpha") |>
  mutate(excess = if_else(residual_kernel > 0, "excess", "deficit")) %>% 
  drop_na()

c_excess <- "#2A788EFF"
c_data <- "lightgrey"
c_line <- "black"
c_lpbg <- "white"
c_grid <- "#F0F0F0AA"

p1_kernel_io <-
  ggplot(plot_data_io, aes(alpha, y = count, fill = excess)) +
  geom_vline(xintercept = c(0.7, 0.8, 0.9), linetype = "dotted") +
  geom_col() +
  geom_col(aes(y = count - if_else(residual_kernel > 0, residual_kernel * total_n, 0)), fill = c_data) +
  geom_step(aes(y = density_kernel * total_n, group = 1), direction = "mid", size = 0.2, color = c_line) +
  scale_fill_manual(values = c(c_data, c_excess),
                    name = element_blank()) +
  scale_x_continuous(breaks = seq(from = 0, to = 1, by = 0.1),
                     labels = label_number(accuracy = 0.1),
                     minor_breaks = NULL) +
  theme_bw() +
  labs(x = "", 
       y = "Count") +
  theme(axis.title.x = element_markdown()) +
  ylim(0, 4200) +
  theme(legend.position = "none",
        legend.background = element_blank(),
        panel.grid = element_line(color = c_grid))

p2_kernel_io <- 
  ggplot(plot_data_io, aes(alpha, residual_kernel * total_n, fill = excess)) + 
  geom_vline(xintercept = c(0.7, 0.8, 0.9), linetype = "dotted") +
  geom_col(aes(color = excess), size = 0.2) +
  scale_color_manual(values = c(c_line, "#FFFFFF00")) +
  geom_segment(x = 0.005, xend = 0.995, y = 0,yend=0, size = 0.2, color = c_line) +
  scale_fill_manual(values = c("white", c_excess),
                    guide = guide_legend(reverse = TRUE)) +
  scale_x_continuous(breaks = seq(from = 0, to = 1, by = 0.1),
                     labels = label_number(accuracy = 0.1),
                     minor_breaks = NULL) +
  scale_y_continuous(breaks = seq(from = -200, to = 200, by = 50),
                     labels = label_number(accuracy = 1),
                     minor_breaks = NULL) +
  theme_bw() +
  labs(x = "Cronbach's &alpha;",
       y = "Residual") +
  theme(legend.title = element_blank(),
        axis.title.x = element_markdown(),
        legend.position = "none",
        panel.background = element_rect(fill = c_lpbg),
        panel.grid = element_line(color = c_grid)) # c(0.23, 0.77), legend.background = element_blank()

p_kernel_io <- 
  p1_kernel_io + 
  p2_kernel_io + 
  plot_layout(nrow = 2, heights = c(1.5, 1))

p_kernel_io

write_rds(p_kernel_io, "plots/p_kernel_io_no_overlap.rds")

ggsave("plots/p_kernel_io_no_overlap.pdf",
       plot = p_kernel_io,
       device = cairo_pdf,
       width = 6,
       height = 4.5,
       units = "in")

```

## Quantify excess 

```{r}

data_inflation_kernel <- data_frequency %>%
  filter(missing == FALSE) %>%
  dplyr::select(alpha, 
                observed = freq, 
                predicted = density_kernel, 
                residual = residual_kernel) %>%
  mutate(predicted = predicted/100,
         residual = residual/100,
         inflation = (observed/predicted - 1),
         inflation_for_reporting = janitor::round_half_up(inflation*100, digits = 0),
         group_3_cutoffs = as.factor(case_when(alpha %in% c(.70, .80, .90) ~ "cutoff",
                                               TRUE ~ "non cutoff")),
         group_1_cutoff = as.factor(case_when(alpha %in% c(.70) ~ "cutoff",
                                               TRUE ~ "non cutoff")))

```

- There were `r data_inflation_kernel$inflation_for_reporting[data_inflation_kernel$alpha == 0.70]`% more observations of Cronbach's $\alpha$ = .70 than predicted.
- There were `r data_inflation_kernel$inflation_for_reporting[data_inflation_kernel$alpha == 0.80]`% more observations of Cronbach's $\alpha$ = .80 than predicted.
- There were `r data_inflation_kernel$inflation_for_reporting[data_inflation_kernel$alpha == 0.90]`% more observations of Cronbach's $\alpha$ = .90 than predicted.

## Permutation tests

.70, .80, and .90 against the rest:

```{r}

independence_test(residual ~ group_3_cutoffs,
                  data = data_inflation_kernel,
                  distribution = "exact",
                  alternative = "greater")

```

.70 against the rest:

```{r}

independence_test(residual ~ group_1_cutoff,
                  data = data_inflation_kernel,
                  distribution = "exact",
                  alternative = "greater")

```

# Session info

```{r}

sessionInfo()

```
