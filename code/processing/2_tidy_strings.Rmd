---
title: "Data extraction"
author: "Ian Hussey"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: hide
    highlight: haddock
    theme: flatly
    toc: yes
    toc_float: yes
---

# Overview

The previous script, "1_extract_strings.Rmd" finds instances of (variants of) "Cronbach's alpha" and saves the 50 characters after this matches to a csv file. This extraction is quite slow as it has to iterate over larger txt files. This file does the second stage of data processing, by extracting Cronbach's alpha estimates from these 50 character strings and excluding instances that are less likely to be valid alpha estimates.

As in "1_extract_strings.Rmd", the goal here is to prioritize specificity over sensitivity. Likely many alpha estimates will not be extracted (e.g., those in tables), but we want high confidence that the values that are extracted are valid Cronbach's alpha estimates.   

```{r setup, include=FALSE}

knitr::opts_chunk$set(message = FALSE, 
                      warning = FALSE)

```

# Dependencies

```{r}

library(tidyverse)

```

# Regex tests

Test some of the regex that is used below. NB leading spaces in strings don't print correctly.

Repeat estimates only return the first estimate.

```{r}

regex_tests <- 
  tibble(string = c("in the current sample was 0.7", "= 0.7", "= .7", "=.7", " 0.7", " .7", "alpha of.70", "alphas of .70 and .80", "section 2.7", "doi 0.0.7"),
         required_result = c(.7, .7, .7, .7, .7, .7, .7, .7, NA, NA)) |>
  mutate(result = as.numeric(str_extract(required_result, "([a-zA-Z]|[0]|\\s|=)\\.[0-9]*")),
         test_passed = ifelse(result == required_result | (is.na(result) & is.na(required_result)), TRUE, FALSE))

regex_tests

```

# Load data

```{r}

data_combined_results_files <- 
  read_csv("../../data/processed/data_combined_results_files.csv")

```

# Extract Cronbach's alphas from extracted strings

```{r}

data_article_length <- data_combined_results_files |>
  filter(key == "total_length") |>
  dplyr::select(doi, total_length_of_article = value)

data_processed <- data_combined_results_files |>
  # check that there are not strings between "cronbach" and the estimate that suggest this is something other than a cronbach's alpha estimate 
  # e.g, reporting of a cutoff value ("cronbach's alpha of > .70"; this could get rid of real values too but more important that we exclude rules of thumb being reported)
  # e.g., a p value ("p < .05"). several of these were found.
  mutate(alpha_locate = str_locate(value, "([0]|\\s|=)\\.[0-9]*")[,"start"],
         substring = str_sub(value, start = 1, end = alpha_locate),
         comparison_present = str_detect(substring, "([<>≥≤]|over|exceeding|above|greater|higher|more|lower|below|less|at least|between)"),
         comparison_present_after_estimate = str_detect(value, "(or above|or more|or greater|or below|or less|or lower)"),
         cutoff_present = str_detect(substring, "cut(\\s|-)off|criteria"),
         range_present = str_detect(substring, "(range|ranging|between|from)"),
         p_present = str_detect(substring, "p\\s*[=<>≥≤]"),
         doi_present = str_detect(tolower(substring), "(doi:|doi.org)"),
         # something about references to this journal keep getting confused for alpha values
         psychometrika_present = str_detect(tolower(substring), "psychometrika")) |>
  # extract the estimate
  mutate(alpha = str_extract(value, "([a-zA-Z]|[0]|\\s|=)\\.[0-9]*"),
         alpha = str_remove(alpha, "="),
         alpha = ifelse(alpha == "." |
                          comparison_present == TRUE |
                          comparison_present_after_estimate == TRUE |
                          cutoff_present == TRUE |
                          range_present == TRUE |
                          p_present == TRUE |
                          doi_present == TRUE |
                          psychometrika_present == TRUE, 
                        NA, 
                        as.numeric(alpha))) |>
  dplyr::select(doi, exemplar, text = value, alpha) |>
  full_join(data_article_length, by = "doi") |>
  mutate(text = ifelse(text == total_length_of_article, NA, text)) |>
  # drop duplicate combinations of doi and exemplar
  distinct(doi, exemplar, .keep_all = TRUE)

```

# Write to disk

```{r}

dir.create("../../data/processed")

write_csv(data_processed, "../../data/processed/data_processed.csv")

```

# Session info

```{r}

sessionInfo()

```
